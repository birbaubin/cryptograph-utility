{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading polblogs dataset...\n",
      "Skipping node 1395 due to zero loss or negligible gradients.\n",
      "Skipping node 35 due to zero loss or negligible gradients.\n",
      "Skipping node 754 due to zero loss or negligible gradients.\n",
      "Skipping node 1169 due to zero loss or negligible gradients.\n",
      "Skipping node 362 due to zero loss or negligible gradients.\n",
      "Skipping node 1183 due to zero loss or negligible gradients.\n",
      "Skipping node 597 due to zero loss or negligible gradients.\n",
      "Skipping node 255 due to zero loss or negligible gradients.\n",
      "Skipping node 1036 due to zero loss or negligible gradients.\n",
      "Skipping node 1008 due to zero loss or negligible gradients.\n",
      "Skipping node 1046 due to zero loss or negligible gradients.\n",
      "Skipping node 146 due to zero loss or negligible gradients.\n",
      "Skipping node 1486 due to zero loss or negligible gradients.\n",
      "Skipping node 1057 due to zero loss or negligible gradients.\n",
      "Skipping node 1403 due to zero loss or negligible gradients.\n",
      "Skipping node 85 due to zero loss or negligible gradients.\n",
      "Skipping node 502 due to zero loss or negligible gradients.\n",
      "Skipping node 192 due to zero loss or negligible gradients.\n",
      "Skipping node 1050 due to zero loss or negligible gradients.\n",
      "Skipping node 1306 due to zero loss or negligible gradients.\n",
      "Skipping node 1485 due to zero loss or negligible gradients.\n",
      "Skipping node 309 due to zero loss or negligible gradients.\n",
      "Skipping node 140 due to zero loss or negligible gradients.\n",
      "Skipping node 263 due to zero loss or negligible gradients.\n",
      "Skipping node 1294 due to zero loss or negligible gradients.\n",
      "Skipping node 425 due to zero loss or negligible gradients.\n",
      "Skipping node 340 due to zero loss or negligible gradients.\n",
      "Skipping node 1471 due to zero loss or negligible gradients.\n",
      "Skipping node 1458 due to zero loss or negligible gradients.\n",
      "Skipping node 536 due to zero loss or negligible gradients.\n",
      "Skipping node 1430 due to zero loss or negligible gradients.\n",
      "Skipping node 881 due to zero loss or negligible gradients.\n",
      "Skipping node 88 due to zero loss or negligible gradients.\n",
      "Skipping node 6 due to zero loss or negligible gradients.\n",
      "Skipping node 834 due to zero loss or negligible gradients.\n",
      "Skipping node 271 due to zero loss or negligible gradients.\n",
      "Skipping node 1393 due to zero loss or negligible gradients.\n",
      "Skipping node 856 due to zero loss or negligible gradients.\n",
      "Skipping node 1069 due to zero loss or negligible gradients.\n",
      "Skipping node 1342 due to zero loss or negligible gradients.\n",
      "Skipping node 351 due to zero loss or negligible gradients.\n",
      "Skipping node 798 due to zero loss or negligible gradients.\n",
      "Skipping node 806 due to zero loss or negligible gradients.\n",
      "Skipping node 987 due to zero loss or negligible gradients.\n",
      "Skipping node 1388 due to zero loss or negligible gradients.\n",
      "Skipping node 864 due to zero loss or negligible gradients.\n",
      "Skipping node 1152 due to zero loss or negligible gradients.\n",
      "Skipping node 1443 due to zero loss or negligible gradients.\n",
      "Skipping node 101 due to zero loss or negligible gradients.\n",
      "Skipping node 1162 due to zero loss or negligible gradients.\n",
      "Skipping node 67 due to zero loss or negligible gradients.\n",
      "Skipping node 1420 due to zero loss or negligible gradients.\n",
      "Skipping node 595 due to zero loss or negligible gradients.\n",
      "Skipping node 1334 due to zero loss or negligible gradients.\n",
      "Skipping node 883 due to zero loss or negligible gradients.\n",
      "Skipping node 382 due to zero loss or negligible gradients.\n",
      "Skipping node 132 due to zero loss or negligible gradients.\n",
      "Skipping node 742 due to zero loss or negligible gradients.\n",
      "Skipping node 1412 due to zero loss or negligible gradients.\n",
      "Skipping node 25 due to zero loss or negligible gradients.\n",
      "Skipping node 1309 due to zero loss or negligible gradients.\n",
      "Skipping node 1016 due to zero loss or negligible gradients.\n",
      "Skipping node 1251 due to zero loss or negligible gradients.\n",
      "Skipping node 201 due to zero loss or negligible gradients.\n",
      "Skipping node 1359 due to zero loss or negligible gradients.\n",
      "Skipping node 878 due to zero loss or negligible gradients.\n",
      "Skipping node 614 due to zero loss or negligible gradients.\n",
      "Skipping node 715 due to zero loss or negligible gradients.\n",
      "Skipping node 258 due to zero loss or negligible gradients.\n",
      "Skipping node 837 due to zero loss or negligible gradients.\n",
      "Skipping node 21 due to zero loss or negligible gradients.\n",
      "Skipping node 560 due to zero loss or negligible gradients.\n",
      "Skipping node 693 due to zero loss or negligible gradients.\n",
      "Skipping node 1354 due to zero loss or negligible gradients.\n",
      "Skipping node 1090 due to zero loss or negligible gradients.\n",
      "Skipping node 1454 due to zero loss or negligible gradients.\n",
      "Skipping node 1171 due to zero loss or negligible gradients.\n",
      "Skipping node 721 due to zero loss or negligible gradients.\n",
      "Skipping node 1370 due to zero loss or negligible gradients.\n",
      "Skipping node 418 due to zero loss or negligible gradients.\n",
      "Skipping node 1353 due to zero loss or negligible gradients.\n",
      "Skipping node 763 due to zero loss or negligible gradients.\n",
      "Skipping node 38 due to zero loss or negligible gradients.\n",
      "Skipping node 746 due to zero loss or negligible gradients.\n",
      "Skipping node 1144 due to zero loss or negligible gradients.\n",
      "Skipping node 938 due to zero loss or negligible gradients.\n",
      "Skipping node 276 due to zero loss or negligible gradients.\n",
      "Skipping node 828 due to zero loss or negligible gradients.\n",
      "Skipping node 1101 due to zero loss or negligible gradients.\n",
      "Skipping node 854 due to zero loss or negligible gradients.\n",
      "Skipping node 1212 due to zero loss or negligible gradients.\n",
      "Skipping node 991 due to zero loss or negligible gradients.\n",
      "Skipping node 725 due to zero loss or negligible gradients.\n",
      "Skipping node 30 due to zero loss or negligible gradients.\n",
      "Skipping node 1102 due to zero loss or negligible gradients.\n",
      "Skipping node 1167 due to zero loss or negligible gradients.\n",
      "Skipping node 673 due to zero loss or negligible gradients.\n",
      "Skipping node 613 due to zero loss or negligible gradients.\n",
      "Skipping node 343 due to zero loss or negligible gradients.\n",
      "Skipping node 1080 due to zero loss or negligible gradients.\n",
      "Skipping node 39 due to zero loss or negligible gradients.\n",
      "Skipping node 524 due to zero loss or negligible gradients.\n",
      "Skipping node 388 due to zero loss or negligible gradients.\n",
      "Skipping node 1017 due to zero loss or negligible gradients.\n",
      "Skipping node 549 due to zero loss or negligible gradients.\n",
      "Skipping node 1157 due to zero loss or negligible gradients.\n",
      "Skipping node 890 due to zero loss or negligible gradients.\n",
      "Skipping node 1321 due to zero loss or negligible gradients.\n",
      "Skipping node 683 due to zero loss or negligible gradients.\n",
      "Skipping node 117 due to zero loss or negligible gradients.\n",
      "Skipping node 274 due to zero loss or negligible gradients.\n",
      "Skipping node 1428 due to zero loss or negligible gradients.\n",
      "Skipping node 540 due to zero loss or negligible gradients.\n",
      "Skipping node 692 due to zero loss or negligible gradients.\n",
      "Skipping node 1044 due to zero loss or negligible gradients.\n",
      "Skipping node 294 due to zero loss or negligible gradients.\n",
      "Skipping node 480 due to zero loss or negligible gradients.\n",
      "Skipping node 247 due to zero loss or negligible gradients.\n",
      "Skipping node 402 due to zero loss or negligible gradients.\n",
      "Skipping node 874 due to zero loss or negligible gradients.\n",
      "Skipping node 608 due to zero loss or negligible gradients.\n",
      "Skipping node 1151 due to zero loss or negligible gradients.\n",
      "Skipping node 415 due to zero loss or negligible gradients.\n",
      "Skipping node 1266 due to zero loss or negligible gradients.\n",
      "Skipping node 210 due to zero loss or negligible gradients.\n",
      "Skipping node 359 due to zero loss or negligible gradients.\n",
      "Skipping node 1047 due to zero loss or negligible gradients.\n",
      "Skipping node 407 due to zero loss or negligible gradients.\n",
      "Skipping node 1112 due to zero loss or negligible gradients.\n",
      "Skipping node 1287 due to zero loss or negligible gradients.\n",
      "Skipping node 1329 due to zero loss or negligible gradients.\n",
      "Skipping node 964 due to zero loss or negligible gradients.\n",
      "Skipping node 640 due to zero loss or negligible gradients.\n",
      "Skipping node 1390 due to zero loss or negligible gradients.\n",
      "Skipping node 232 due to zero loss or negligible gradients.\n",
      "Skipping node 1029 due to zero loss or negligible gradients.\n",
      "Skipping node 1252 due to zero loss or negligible gradients.\n",
      "Skipping node 1200 due to zero loss or negligible gradients.\n",
      "Skipping node 409 due to zero loss or negligible gradients.\n",
      "Skipping node 1096 due to zero loss or negligible gradients.\n",
      "Skipping node 336 due to zero loss or negligible gradients.\n",
      "Skipping node 1027 due to zero loss or negligible gradients.\n",
      "Skipping node 396 due to zero loss or negligible gradients.\n",
      "Skipping node 1327 due to zero loss or negligible gradients.\n",
      "Skipping node 1441 due to zero loss or negligible gradients.\n",
      "Skipping node 562 due to zero loss or negligible gradients.\n",
      "Skipping node 188 due to zero loss or negligible gradients.\n",
      "Skipping node 444 due to zero loss or negligible gradients.\n",
      "Skipping node 1138 due to zero loss or negligible gradients.\n",
      "Skipping node 238 due to zero loss or negligible gradients.\n",
      "Skipping node 1043 due to zero loss or negligible gradients.\n",
      "Skipping node 537 due to zero loss or negligible gradients.\n",
      "Skipping node 783 due to zero loss or negligible gradients.\n",
      "Skipping node 995 due to zero loss or negligible gradients.\n",
      "Skipping node 1175 due to zero loss or negligible gradients.\n",
      "Skipping node 89 due to zero loss or negligible gradients.\n",
      "Skipping node 410 due to zero loss or negligible gradients.\n",
      "Skipping node 832 due to zero loss or negligible gradients.\n",
      "Skipping node 7 due to zero loss or negligible gradients.\n",
      "Skipping node 1466 due to zero loss or negligible gradients.\n",
      "Skipping node 573 due to zero loss or negligible gradients.\n",
      "Skipping node 439 due to zero loss or negligible gradients.\n",
      "Skipping node 83 due to zero loss or negligible gradients.\n",
      "Skipping node 863 due to zero loss or negligible gradients.\n",
      "Skipping node 451 due to zero loss or negligible gradients.\n",
      "Skipping node 370 due to zero loss or negligible gradients.\n",
      "Skipping node 757 due to zero loss or negligible gradients.\n",
      "Skipping node 801 due to zero loss or negligible gradients.\n",
      "Skipping node 645 due to zero loss or negligible gradients.\n",
      "Skipping node 357 due to zero loss or negligible gradients.\n",
      "Skipping node 65 due to zero loss or negligible gradients.\n",
      "Skipping node 1429 due to zero loss or negligible gradients.\n",
      "Skipping node 924 due to zero loss or negligible gradients.\n",
      "Skipping node 1302 due to zero loss or negligible gradients.\n",
      "Skipping node 647 due to zero loss or negligible gradients.\n",
      "Skipping node 913 due to zero loss or negligible gradients.\n",
      "Skipping node 807 due to zero loss or negligible gradients.\n",
      "Skipping node 1002 due to zero loss or negligible gradients.\n",
      "Skipping node 360 due to zero loss or negligible gradients.\n",
      "Skipping node 993 due to zero loss or negligible gradients.\n",
      "Skipping node 1204 due to zero loss or negligible gradients.\n",
      "Skipping node 731 due to zero loss or negligible gradients.\n",
      "Skipping node 157 due to zero loss or negligible gradients.\n",
      "Skipping node 648 due to zero loss or negligible gradients.\n",
      "Skipping node 1007 due to zero loss or negligible gradients.\n",
      "Skipping node 577 due to zero loss or negligible gradients.\n",
      "Skipping node 364 due to zero loss or negligible gradients.\n",
      "Skipping node 1129 due to zero loss or negligible gradients.\n",
      "Skipping node 1143 due to zero loss or negligible gradients.\n",
      "Skipping node 1391 due to zero loss or negligible gradients.\n",
      "Skipping node 824 due to zero loss or negligible gradients.\n",
      "Skipping node 541 due to zero loss or negligible gradients.\n",
      "Skipping node 634 due to zero loss or negligible gradients.\n",
      "Skipping node 777 due to zero loss or negligible gradients.\n",
      "Skipping node 646 due to zero loss or negligible gradients.\n",
      "Skipping node 816 due to zero loss or negligible gradients.\n",
      "Skipping node 554 due to zero loss or negligible gradients.\n",
      "Skipping node 1437 due to zero loss or negligible gradients.\n",
      "Skipping node 1164 due to zero loss or negligible gradients.\n",
      "Skipping node 1289 due to zero loss or negligible gradients.\n",
      "Skipping node 559 due to zero loss or negligible gradients.\n",
      "Skipping node 237 due to zero loss or negligible gradients.\n",
      "Skipping node 106 due to zero loss or negligible gradients.\n",
      "Skipping node 1389 due to zero loss or negligible gradients.\n",
      "Skipping node 566 due to zero loss or negligible gradients.\n",
      "Skipping node 955 due to zero loss or negligible gradients.\n",
      "Skipping node 377 due to zero loss or negligible gradients.\n",
      "Skipping node 776 due to zero loss or negligible gradients.\n",
      "Skipping node 170 due to zero loss or negligible gradients.\n",
      "Skipping node 1235 due to zero loss or negligible gradients.\n",
      "Skipping node 1380 due to zero loss or negligible gradients.\n",
      "Skipping node 74 due to zero loss or negligible gradients.\n",
      "Skipping node 690 due to zero loss or negligible gradients.\n",
      "Skipping node 126 due to zero loss or negligible gradients.\n",
      "Skipping node 1386 due to zero loss or negligible gradients.\n",
      "Skipping node 216 due to zero loss or negligible gradients.\n",
      "Skipping node 373 due to zero loss or negligible gradients.\n",
      "Skipping node 944 due to zero loss or negligible gradients.\n",
      "Skipping node 95 due to zero loss or negligible gradients.\n",
      "Skipping node 675 due to zero loss or negligible gradients.\n",
      "Skipping node 574 due to zero loss or negligible gradients.\n",
      "Skipping node 445 due to zero loss or negligible gradients.\n",
      "Skipping node 14 due to zero loss or negligible gradients.\n",
      "Skipping node 1059 due to zero loss or negligible gradients.\n",
      "Skipping node 792 due to zero loss or negligible gradients.\n",
      "Skipping node 1472 due to zero loss or negligible gradients.\n",
      "Skipping node 1023 due to zero loss or negligible gradients.\n",
      "Skipping node 266 due to zero loss or negligible gradients.\n",
      "Skipping node 1347 due to zero loss or negligible gradients.\n",
      "Skipping node 578 due to zero loss or negligible gradients.\n",
      "Skipping node 1324 due to zero loss or negligible gradients.\n",
      "Skipping node 679 due to zero loss or negligible gradients.\n",
      "Skipping node 760 due to zero loss or negligible gradients.\n",
      "Skipping node 884 due to zero loss or negligible gradients.\n",
      "Skipping node 711 due to zero loss or negligible gradients.\n",
      "Skipping node 936 due to zero loss or negligible gradients.\n",
      "Skipping node 350 due to zero loss or negligible gradients.\n",
      "Skipping node 543 due to zero loss or negligible gradients.\n",
      "Skipping node 861 due to zero loss or negligible gradients.\n",
      "Skipping node 1314 due to zero loss or negligible gradients.\n",
      "Skipping node 1424 due to zero loss or negligible gradients.\n",
      "Skipping node 252 due to zero loss or negligible gradients.\n",
      "Skipping node 45 due to zero loss or negligible gradients.\n",
      "Skipping node 1103 due to zero loss or negligible gradients.\n",
      "Skipping node 470 due to zero loss or negligible gradients.\n",
      "Skipping node 1125 due to zero loss or negligible gradients.\n",
      "Skipping node 1216 due to zero loss or negligible gradients.\n",
      "Skipping node 1286 due to zero loss or negligible gradients.\n",
      "Skipping node 815 due to zero loss or negligible gradients.\n",
      "Skipping node 1383 due to zero loss or negligible gradients.\n",
      "Skipping node 770 due to zero loss or negligible gradients.\n",
      "Skipping node 66 due to zero loss or negligible gradients.\n",
      "Skipping node 272 due to zero loss or negligible gradients.\n",
      "Skipping node 1397 due to zero loss or negligible gradients.\n",
      "Skipping node 1269 due to zero loss or negligible gradients.\n",
      "Skipping node 420 due to zero loss or negligible gradients.\n",
      "Skipping node 1121 due to zero loss or negligible gradients.\n",
      "Skipping node 36 due to zero loss or negligible gradients.\n",
      "Skipping node 892 due to zero loss or negligible gradients.\n",
      "Skipping node 907 due to zero loss or negligible gradients.\n",
      "Skipping node 8 due to zero loss or negligible gradients.\n",
      "Skipping node 1281 due to zero loss or negligible gradients.\n",
      "Skipping node 896 due to zero loss or negligible gradients.\n",
      "Skipping node 77 due to zero loss or negligible gradients.\n",
      "Skipping node 1315 due to zero loss or negligible gradients.\n",
      "Skipping node 535 due to zero loss or negligible gradients.\n",
      "Skipping node 929 due to zero loss or negligible gradients.\n",
      "Skipping node 1478 due to zero loss or negligible gradients.\n",
      "Skipping node 954 due to zero loss or negligible gradients.\n",
      "Skipping node 191 due to zero loss or negligible gradients.\n",
      "Skipping node 867 due to zero loss or negligible gradients.\n",
      "Skipping node 1480 due to zero loss or negligible gradients.\n",
      "Skipping node 245 due to zero loss or negligible gradients.\n",
      "Skipping node 1414 due to zero loss or negligible gradients.\n",
      "Skipping node 686 due to zero loss or negligible gradients.\n",
      "Skipping node 827 due to zero loss or negligible gradients.\n",
      "Skipping node 1105 due to zero loss or negligible gradients.\n",
      "Skipping node 781 due to zero loss or negligible gradients.\n",
      "Skipping node 442 due to zero loss or negligible gradients.\n",
      "Skipping node 465 due to zero loss or negligible gradients.\n",
      "Skipping node 149 due to zero loss or negligible gradients.\n",
      "Skipping node 703 due to zero loss or negligible gradients.\n",
      "Skipping node 139 due to zero loss or negligible gradients.\n",
      "Skipping node 466 due to zero loss or negligible gradients.\n",
      "Skipping node 740 due to zero loss or negligible gradients.\n",
      "Skipping node 199 due to zero loss or negligible gradients.\n",
      "Skipping node 297 due to zero loss or negligible gradients.\n",
      "Skipping node 1203 due to zero loss or negligible gradients.\n",
      "Skipping node 1039 due to zero loss or negligible gradients.\n",
      "Skipping node 180 due to zero loss or negligible gradients.\n",
      "Skipping node 124 due to zero loss or negligible gradients.\n",
      "Skipping node 1113 due to zero loss or negligible gradients.\n",
      "Skipping node 977 due to zero loss or negligible gradients.\n",
      "Skipping node 1170 due to zero loss or negligible gradients.\n",
      "Skipping node 1091 due to zero loss or negligible gradients.\n",
      "Skipping node 1063 due to zero loss or negligible gradients.\n",
      "Skipping node 555 due to zero loss or negligible gradients.\n",
      "Skipping node 1470 due to zero loss or negligible gradients.\n",
      "Skipping node 1318 due to zero loss or negligible gradients.\n",
      "Skipping node 818 due to zero loss or negligible gradients.\n",
      "Skipping node 176 due to zero loss or negligible gradients.\n",
      "Skipping node 1273 due to zero loss or negligible gradients.\n",
      "Skipping node 649 due to zero loss or negligible gradients.\n",
      "Skipping node 618 due to zero loss or negligible gradients.\n",
      "Skipping node 156 due to zero loss or negligible gradients.\n",
      "Skipping node 738 due to zero loss or negligible gradients.\n",
      "Skipping node 1071 due to zero loss or negligible gradients.\n",
      "Skipping node 1249 due to zero loss or negligible gradients.\n",
      "Skipping node 1078 due to zero loss or negligible gradients.\n",
      "Skipping node 1025 due to zero loss or negligible gradients.\n",
      "Skipping node 1085 due to zero loss or negligible gradients.\n",
      "Skipping node 1333 due to zero loss or negligible gradients.\n",
      "Skipping node 1062 due to zero loss or negligible gradients.\n",
      "Skipping node 236 due to zero loss or negligible gradients.\n",
      "Skipping node 1028 due to zero loss or negligible gradients.\n",
      "Skipping node 506 due to zero loss or negligible gradients.\n",
      "Skipping node 829 due to zero loss or negligible gradients.\n",
      "Skipping node 682 due to zero loss or negligible gradients.\n",
      "Skipping node 400 due to zero loss or negligible gradients.\n",
      "Skipping node 1176 due to zero loss or negligible gradients.\n",
      "Skipping node 810 due to zero loss or negligible gradients.\n",
      "Skipping node 1012 due to zero loss or negligible gradients.\n",
      "Skipping node 185 due to zero loss or negligible gradients.\n",
      "Skipping node 452 due to zero loss or negligible gradients.\n",
      "Skipping node 17 due to zero loss or negligible gradients.\n",
      "Skipping node 1237 due to zero loss or negligible gradients.\n",
      "Skipping node 848 due to zero loss or negligible gradients.\n",
      "Skipping node 76 due to zero loss or negligible gradients.\n",
      "Skipping node 482 due to zero loss or negligible gradients.\n",
      "Skipping node 1095 due to zero loss or negligible gradients.\n",
      "Skipping node 800 due to zero loss or negligible gradients.\n",
      "Skipping node 641 due to zero loss or negligible gradients.\n",
      "Skipping node 204 due to zero loss or negligible gradients.\n",
      "Skipping node 716 due to zero loss or negligible gradients.\n",
      "Skipping node 976 due to zero loss or negligible gradients.\n",
      "Skipping node 761 due to zero loss or negligible gradients.\n",
      "Skipping node 423 due to zero loss or negligible gradients.\n",
      "Skipping node 644 due to zero loss or negligible gradients.\n",
      "Skipping node 479 due to zero loss or negligible gradients.\n",
      "Skipping node 910 due to zero loss or negligible gradients.\n",
      "Skipping node 698 due to zero loss or negligible gradients.\n",
      "Skipping node 700 due to zero loss or negligible gradients.\n",
      "Skipping node 179 due to zero loss or negligible gradients.\n",
      "Skipping node 1349 due to zero loss or negligible gradients.\n",
      "Skipping node 663 due to zero loss or negligible gradients.\n",
      "Skipping node 870 due to zero loss or negligible gradients.\n",
      "Skipping node 656 due to zero loss or negligible gradients.\n",
      "Skipping node 702 due to zero loss or negligible gradients.\n",
      "Skipping node 328 due to zero loss or negligible gradients.\n",
      "Skipping node 306 due to zero loss or negligible gradients.\n",
      "Skipping node 802 due to zero loss or negligible gradients.\n",
      "Skipping node 243 due to zero loss or negligible gradients.\n",
      "Skipping node 1257 due to zero loss or negligible gradients.\n",
      "Skipping node 50 due to zero loss or negligible gradients.\n",
      "Skipping node 1013 due to zero loss or negligible gradients.\n",
      "Skipping node 486 due to zero loss or negligible gradients.\n",
      "Skipping node 940 due to zero loss or negligible gradients.\n",
      "Skipping node 1411 due to zero loss or negligible gradients.\n",
      "Skipping node 334 due to zero loss or negligible gradients.\n",
      "Skipping node 515 due to zero loss or negligible gradients.\n",
      "Skipping node 1483 due to zero loss or negligible gradients.\n",
      "Skipping node 583 due to zero loss or negligible gradients.\n",
      "Skipping node 1278 due to zero loss or negligible gradients.\n",
      "Skipping node 1210 due to zero loss or negligible gradients.\n",
      "Skipping node 1352 due to zero loss or negligible gradients.\n",
      "Skipping node 962 due to zero loss or negligible gradients.\n",
      "Skipping node 780 due to zero loss or negligible gradients.\n",
      "Skipping node 717 due to zero loss or negligible gradients.\n",
      "Skipping node 171 due to zero loss or negligible gradients.\n",
      "Skipping node 1335 due to zero loss or negligible gradients.\n",
      "Skipping node 552 due to zero loss or negligible gradients.\n",
      "Skipping node 438 due to zero loss or negligible gradients.\n",
      "Skipping node 755 due to zero loss or negligible gradients.\n",
      "Skipping node 889 due to zero loss or negligible gradients.\n",
      "Skipping node 143 due to zero loss or negligible gradients.\n",
      "Skipping node 1426 due to zero loss or negligible gradients.\n",
      "Skipping node 941 due to zero loss or negligible gradients.\n",
      "Skipping node 203 due to zero loss or negligible gradients.\n",
      "Skipping node 572 due to zero loss or negligible gradients.\n",
      "Skipping node 582 due to zero loss or negligible gradients.\n",
      "Skipping node 1250 due to zero loss or negligible gradients.\n",
      "Skipping node 875 due to zero loss or negligible gradients.\n",
      "Skipping node 1344 due to zero loss or negligible gradients.\n",
      "Skipping node 491 due to zero loss or negligible gradients.\n",
      "Skipping node 1195 due to zero loss or negligible gradients.\n",
      "Skipping node 1413 due to zero loss or negligible gradients.\n",
      "Skipping node 983 due to zero loss or negligible gradients.\n",
      "Skipping node 689 due to zero loss or negligible gradients.\n",
      "Skipping node 1134 due to zero loss or negligible gradients.\n",
      "Skipping node 753 due to zero loss or negligible gradients.\n",
      "Skipping node 208 due to zero loss or negligible gradients.\n",
      "Skipping node 159 due to zero loss or negligible gradients.\n",
      "Skipping node 669 due to zero loss or negligible gradients.\n",
      "Skipping node 1168 due to zero loss or negligible gradients.\n",
      "Skipping node 1331 due to zero loss or negligible gradients.\n",
      "Skipping node 278 due to zero loss or negligible gradients.\n",
      "Skipping node 1199 due to zero loss or negligible gradients.\n",
      "Skipping node 937 due to zero loss or negligible gradients.\n",
      "Skipping node 897 due to zero loss or negligible gradients.\n",
      "Skipping node 627 due to zero loss or negligible gradients.\n",
      "Skipping node 1431 due to zero loss or negligible gradients.\n",
      "Skipping node 1457 due to zero loss or negligible gradients.\n",
      "Skipping node 850 due to zero loss or negligible gradients.\n",
      "Skipping node 453 due to zero loss or negligible gradients.\n",
      "Skipping node 1297 due to zero loss or negligible gradients.\n",
      "Skipping node 226 due to zero loss or negligible gradients.\n",
      "Skipping node 1037 due to zero loss or negligible gradients.\n",
      "Skipping node 1064 due to zero loss or negligible gradients.\n",
      "Skipping node 1218 due to zero loss or negligible gradients.\n",
      "Skipping node 1319 due to zero loss or negligible gradients.\n",
      "Skipping node 1351 due to zero loss or negligible gradients.\n",
      "Skipping node 408 due to zero loss or negligible gradients.\n",
      "Skipping node 1086 due to zero loss or negligible gradients.\n",
      "Skipping node 688 due to zero loss or negligible gradients.\n",
      "Skipping node 299 due to zero loss or negligible gradients.\n",
      "Skipping node 426 due to zero loss or negligible gradients.\n",
      "Skipping node 102 due to zero loss or negligible gradients.\n",
      "Skipping node 630 due to zero loss or negligible gradients.\n",
      "Skipping node 116 due to zero loss or negligible gradients.\n",
      "Skipping node 561 due to zero loss or negligible gradients.\n",
      "Skipping node 282 due to zero loss or negligible gradients.\n",
      "Skipping node 1473 due to zero loss or negligible gradients.\n",
      "Skipping node 1075 due to zero loss or negligible gradients.\n",
      "Skipping node 766 due to zero loss or negligible gradients.\n",
      "Skipping node 478 due to zero loss or negligible gradients.\n",
      "Skipping node 1099 due to zero loss or negligible gradients.\n",
      "Skipping node 900 due to zero loss or negligible gradients.\n",
      "Skipping node 467 due to zero loss or negligible gradients.\n",
      "Skipping node 1003 due to zero loss or negligible gradients.\n",
      "Skipping node 1018 due to zero loss or negligible gradients.\n",
      "Skipping node 858 due to zero loss or negligible gradients.\n",
      "Skipping node 488 due to zero loss or negligible gradients.\n",
      "Skipping node 403 due to zero loss or negligible gradients.\n",
      "Skipping node 389 due to zero loss or negligible gradients.\n",
      "Skipping node 684 due to zero loss or negligible gradients.\n",
      "Skipping node 654 due to zero loss or negligible gradients.\n",
      "Skipping node 435 due to zero loss or negligible gradients.\n",
      "Skipping node 765 due to zero loss or negligible gradients.\n",
      "Skipping node 154 due to zero loss or negligible gradients.\n",
      "Skipping node 52 due to zero loss or negligible gradients.\n",
      "Skipping node 268 due to zero loss or negligible gradients.\n",
      "Skipping node 569 due to zero loss or negligible gradients.\n",
      "Skipping node 772 due to zero loss or negligible gradients.\n",
      "Skipping node 23 due to zero loss or negligible gradients.\n",
      "Skipping node 1446 due to zero loss or negligible gradients.\n",
      "Skipping node 1056 due to zero loss or negligible gradients.\n",
      "Skipping node 404 due to zero loss or negligible gradients.\n",
      "Skipping node 136 due to zero loss or negligible gradients.\n",
      "Skipping node 19 due to zero loss or negligible gradients.\n",
      "Skipping node 853 due to zero loss or negligible gradients.\n",
      "Skipping node 304 due to zero loss or negligible gradients.\n",
      "Skipping node 308 due to zero loss or negligible gradients.\n",
      "Skipping node 1243 due to zero loss or negligible gradients.\n",
      "Skipping node 1439 due to zero loss or negligible gradients.\n",
      "Skipping node 346 due to zero loss or negligible gradients.\n",
      "Skipping node 320 due to zero loss or negligible gradients.\n",
      "Skipping node 959 due to zero loss or negligible gradients.\n",
      "Skipping node 398 due to zero loss or negligible gradients.\n",
      "Skipping node 1394 due to zero loss or negligible gradients.\n",
      "Skipping node 421 due to zero loss or negligible gradients.\n",
      "Skipping node 1392 due to zero loss or negligible gradients.\n",
      "Skipping node 602 due to zero loss or negligible gradients.\n",
      "Skipping node 734 due to zero loss or negligible gradients.\n",
      "Skipping node 43 due to zero loss or negligible gradients.\n",
      "Skipping node 951 due to zero loss or negligible gradients.\n",
      "Skipping node 1285 due to zero loss or negligible gradients.\n",
      "Skipping node 510 due to zero loss or negligible gradients.\n",
      "Skipping node 366 due to zero loss or negligible gradients.\n",
      "Skipping node 131 due to zero loss or negligible gradients.\n",
      "Skipping node 174 due to zero loss or negligible gradients.\n",
      "Skipping node 911 due to zero loss or negligible gradients.\n",
      "Skipping node 1066 due to zero loss or negligible gradients.\n",
      "Skipping node 84 due to zero loss or negligible gradients.\n",
      "Skipping node 869 due to zero loss or negligible gradients.\n",
      "Skipping node 267 due to zero loss or negligible gradients.\n",
      "Skipping node 1182 due to zero loss or negligible gradients.\n",
      "Skipping node 1340 due to zero loss or negligible gradients.\n",
      "Skipping node 1022 due to zero loss or negligible gradients.\n",
      "Skipping node 13 due to zero loss or negligible gradients.\n",
      "Skipping node 517 due to zero loss or negligible gradients.\n",
      "Skipping node 624 due to zero loss or negligible gradients.\n",
      "Skipping node 982 due to zero loss or negligible gradients.\n",
      "Skipping node 1290 due to zero loss or negligible gradients.\n",
      "Skipping node 934 due to zero loss or negligible gradients.\n",
      "Skipping node 113 due to zero loss or negligible gradients.\n",
      "Skipping node 1332 due to zero loss or negligible gradients.\n",
      "Skipping node 1448 due to zero loss or negligible gradients.\n",
      "Skipping node 899 due to zero loss or negligible gradients.\n",
      "Skipping node 148 due to zero loss or negligible gradients.\n",
      "Skipping node 520 due to zero loss or negligible gradients.\n",
      "Skipping node 668 due to zero loss or negligible gradients.\n",
      "Skipping node 1425 due to zero loss or negligible gradients.\n",
      "Skipping node 128 due to zero loss or negligible gradients.\n",
      "Skipping node 1233 due to zero loss or negligible gradients.\n",
      "Skipping node 300 due to zero loss or negligible gradients.\n",
      "Skipping node 1079 due to zero loss or negligible gradients.\n",
      "Skipping node 830 due to zero loss or negligible gradients.\n",
      "Skipping node 1384 due to zero loss or negligible gradients.\n",
      "Skipping node 791 due to zero loss or negligible gradients.\n",
      "Skipping node 217 due to zero loss or negligible gradients.\n",
      "Skipping node 34 due to zero loss or negligible gradients.\n",
      "Skipping node 1360 due to zero loss or negligible gradients.\n",
      "Skipping node 1033 due to zero loss or negligible gradients.\n",
      "Skipping node 1440 due to zero loss or negligible gradients.\n",
      "Skipping node 178 due to zero loss or negligible gradients.\n",
      "Skipping node 42 due to zero loss or negligible gradients.\n",
      "Skipping node 513 due to zero loss or negligible gradients.\n",
      "Skipping node 1153 due to zero loss or negligible gradients.\n",
      "Skipping node 699 due to zero loss or negligible gradients.\n",
      "Skipping node 619 due to zero loss or negligible gradients.\n",
      "Skipping node 895 due to zero loss or negligible gradients.\n",
      "Skipping node 1435 due to zero loss or negligible gradients.\n",
      "Skipping node 464 due to zero loss or negligible gradients.\n",
      "Skipping node 558 due to zero loss or negligible gradients.\n",
      "Skipping node 1338 due to zero loss or negligible gradients.\n",
      "Skipping node 739 due to zero loss or negligible gradients.\n",
      "Skipping node 1417 due to zero loss or negligible gradients.\n",
      "Skipping node 912 due to zero loss or negligible gradients.\n",
      "Skipping node 387 due to zero loss or negligible gradients.\n",
      "Skipping node 41 due to zero loss or negligible gradients.\n",
      "Skipping node 1468 due to zero loss or negligible gradients.\n",
      "Skipping node 28 due to zero loss or negligible gradients.\n",
      "Skipping node 643 due to zero loss or negligible gradients.\n",
      "Skipping node 1444 due to zero loss or negligible gradients.\n",
      "Skipping node 833 due to zero loss or negligible gradients.\n",
      "Skipping node 1419 due to zero loss or negligible gradients.\n",
      "Skipping node 599 due to zero loss or negligible gradients.\n",
      "Skipping node 381 due to zero loss or negligible gradients.\n",
      "Skipping node 611 due to zero loss or negligible gradients.\n",
      "Skipping node 187 due to zero loss or negligible gradients.\n",
      "Skipping node 1280 due to zero loss or negligible gradients.\n",
      "Skipping node 1477 due to zero loss or negligible gradients.\n",
      "Skipping node 1404 due to zero loss or negligible gradients.\n",
      "Skipping node 112 due to zero loss or negligible gradients.\n",
      "Skipping node 814 due to zero loss or negligible gradients.\n",
      "Skipping node 1253 due to zero loss or negligible gradients.\n",
      "Skipping node 256 due to zero loss or negligible gradients.\n",
      "Skipping node 942 due to zero loss or negligible gradients.\n",
      "Skipping node 1221 due to zero loss or negligible gradients.\n",
      "Skipping node 1040 due to zero loss or negligible gradients.\n",
      "Skipping node 965 due to zero loss or negligible gradients.\n",
      "Skipping node 762 due to zero loss or negligible gradients.\n",
      "Skipping node 526 due to zero loss or negligible gradients.\n",
      "Skipping node 115 due to zero loss or negligible gradients.\n",
      "Skipping node 1469 due to zero loss or negligible gradients.\n",
      "Skipping node 1225 due to zero loss or negligible gradients.\n",
      "Skipping node 1133 due to zero loss or negligible gradients.\n",
      "Skipping node 920 due to zero loss or negligible gradients.\n",
      "Skipping node 588 due to zero loss or negligible gradients.\n",
      "Skipping node 1361 due to zero loss or negligible gradients.\n",
      "Skipping node 620 due to zero loss or negligible gradients.\n",
      "Skipping node 877 due to zero loss or negligible gradients.\n",
      "Skipping node 68 due to zero loss or negligible gradients.\n",
      "Skipping node 958 due to zero loss or negligible gradients.\n",
      "Skipping node 786 due to zero loss or negligible gradients.\n",
      "Skipping node 823 due to zero loss or negligible gradients.\n",
      "Skipping node 1166 due to zero loss or negligible gradients.\n",
      "Skipping node 601 due to zero loss or negligible gradients.\n",
      "Skipping node 1367 due to zero loss or negligible gradients.\n",
      "Skipping node 1179 due to zero loss or negligible gradients.\n",
      "Skipping node 1270 due to zero loss or negligible gradients.\n",
      "Skipping node 931 due to zero loss or negligible gradients.\n",
      "Skipping node 1295 due to zero loss or negligible gradients.\n",
      "Skipping node 1467 due to zero loss or negligible gradients.\n",
      "Skipping node 1004 due to zero loss or negligible gradients.\n",
      "Skipping node 1481 due to zero loss or negligible gradients.\n",
      "Skipping node 11 due to zero loss or negligible gradients.\n",
      "Skipping node 548 due to zero loss or negligible gradients.\n",
      "Skipping node 1197 due to zero loss or negligible gradients.\n",
      "Skipping node 223 due to zero loss or negligible gradients.\n",
      "Skipping node 206 due to zero loss or negligible gradients.\n",
      "Skipping node 391 due to zero loss or negligible gradients.\n",
      "Skipping node 787 due to zero loss or negligible gradients.\n",
      "Skipping node 1145 due to zero loss or negligible gradients.\n",
      "Skipping node 81 due to zero loss or negligible gradients.\n",
      "Skipping node 658 due to zero loss or negligible gradients.\n",
      "Skipping node 512 due to zero loss or negligible gradients.\n",
      "Skipping node 871 due to zero loss or negligible gradients.\n",
      "Skipping node 1400 due to zero loss or negligible gradients.\n",
      "Skipping node 748 due to zero loss or negligible gradients.\n",
      "Skipping node 978 due to zero loss or negligible gradients.\n",
      "Skipping node 1084 due to zero loss or negligible gradients.\n",
      "Skipping node 1316 due to zero loss or negligible gradients.\n",
      "Skipping node 82 due to zero loss or negligible gradients.\n",
      "Skipping node 64 due to zero loss or negligible gradients.\n",
      "Skipping node 542 due to zero loss or negligible gradients.\n",
      "Skipping node 177 due to zero loss or negligible gradients.\n",
      "Skipping node 553 due to zero loss or negligible gradients.\n",
      "Skipping node 197 due to zero loss or negligible gradients.\n",
      "Skipping node 908 due to zero loss or negligible gradients.\n",
      "Skipping node 448 due to zero loss or negligible gradients.\n",
      "Skipping node 401 due to zero loss or negligible gradients.\n",
      "Skipping node 72 due to zero loss or negligible gradients.\n",
      "Skipping node 1337 due to zero loss or negligible gradients.\n",
      "Skipping node 633 due to zero loss or negligible gradients.\n",
      "Skipping node 1123 due to zero loss or negligible gradients.\n",
      "Skipping node 1254 due to zero loss or negligible gradients.\n",
      "Skipping node 687 due to zero loss or negligible gradients.\n",
      "Skipping node 290 due to zero loss or negligible gradients.\n",
      "Skipping node 1074 due to zero loss or negligible gradients.\n",
      "Skipping node 1239 due to zero loss or negligible gradients.\n",
      "Skipping node 737 due to zero loss or negligible gradients.\n",
      "Skipping node 1366 due to zero loss or negligible gradients.\n",
      "Skipping node 1376 due to zero loss or negligible gradients.\n",
      "Skipping node 1261 due to zero loss or negligible gradients.\n",
      "Skipping node 504 due to zero loss or negligible gradients.\n",
      "Skipping node 1308 due to zero loss or negligible gradients.\n",
      "Skipping node 1193 due to zero loss or negligible gradients.\n",
      "Skipping node 676 due to zero loss or negligible gradients.\n",
      "Skipping node 273 due to zero loss or negligible gradients.\n",
      "Skipping node 846 due to zero loss or negligible gradients.\n",
      "Skipping node 211 due to zero loss or negligible gradients.\n",
      "Skipping node 621 due to zero loss or negligible gradients.\n",
      "Skipping node 244 due to zero loss or negligible gradients.\n",
      "Skipping node 1301 due to zero loss or negligible gradients.\n",
      "Skipping node 437 due to zero loss or negligible gradients.\n",
      "Skipping node 383 due to zero loss or negligible gradients.\n",
      "Skipping node 507 due to zero loss or negligible gradients.\n",
      "Skipping node 494 due to zero loss or negligible gradients.\n",
      "Skipping node 1445 due to zero loss or negligible gradients.\n",
      "Skipping node 1292 due to zero loss or negligible gradients.\n",
      "Skipping node 879 due to zero loss or negligible gradients.\n",
      "Skipping node 1398 due to zero loss or negligible gradients.\n",
      "Skipping node 1219 due to zero loss or negligible gradients.\n",
      "Skipping node 169 due to zero loss or negligible gradients.\n",
      "Skipping node 380 due to zero loss or negligible gradients.\n",
      "Skipping node 950 due to zero loss or negligible gradients.\n",
      "Skipping node 427 due to zero loss or negligible gradients.\n",
      "Skipping node 1148 due to zero loss or negligible gradients.\n",
      "Skipping node 330 due to zero loss or negligible gradients.\n",
      "Skipping node 891 due to zero loss or negligible gradients.\n",
      "Skipping node 809 due to zero loss or negligible gradients.\n",
      "Skipping node 1211 due to zero loss or negligible gradients.\n",
      "Skipping node 728 due to zero loss or negligible gradients.\n",
      "Skipping node 925 due to zero loss or negligible gradients.\n",
      "Skipping node 1126 due to zero loss or negligible gradients.\n",
      "Skipping node 764 due to zero loss or negligible gradients.\n",
      "Skipping node 317 due to zero loss or negligible gradients.\n",
      "Skipping node 141 due to zero loss or negligible gradients.\n",
      "Skipping node 1156 due to zero loss or negligible gradients.\n",
      "Skipping node 567 due to zero loss or negligible gradients.\n",
      "Skipping node 22 due to zero loss or negligible gradients.\n",
      "Skipping node 1208 due to zero loss or negligible gradients.\n",
      "Skipping node 167 due to zero loss or negligible gradients.\n",
      "Skipping node 1433 due to zero loss or negligible gradients.\n",
      "Skipping node 1355 due to zero loss or negligible gradients.\n",
      "Skipping node 612 due to zero loss or negligible gradients.\n",
      "Skipping node 248 due to zero loss or negligible gradients.\n",
      "Skipping node 33 due to zero loss or negligible gradients.\n",
      "Skipping node 1264 due to zero loss or negligible gradients.\n",
      "Skipping node 1226 due to zero loss or negligible gradients.\n",
      "Skipping node 935 due to zero loss or negligible gradients.\n",
      "Skipping node 1020 due to zero loss or negligible gradients.\n",
      "Skipping node 390 due to zero loss or negligible gradients.\n",
      "Skipping node 918 due to zero loss or negligible gradients.\n",
      "Skipping node 821 due to zero loss or negligible gradients.\n",
      "Skipping node 44 due to zero loss or negligible gradients.\n",
      "Skipping node 393 due to zero loss or negligible gradients.\n",
      "Skipping node 99 due to zero loss or negligible gradients.\n",
      "Skipping node 893 due to zero loss or negligible gradients.\n",
      "Skipping node 1487 due to zero loss or negligible gradients.\n",
      "Skipping node 1198 due to zero loss or negligible gradients.\n",
      "Skipping node 997 due to zero loss or negligible gradients.\n",
      "Skipping node 533 due to zero loss or negligible gradients.\n",
      "Skipping node 708 due to zero loss or negligible gradients.\n",
      "Skipping node 1111 due to zero loss or negligible gradients.\n",
      "Skipping node 1410 due to zero loss or negligible gradients.\n",
      "Skipping node 1147 due to zero loss or negligible gradients.\n",
      "Skipping node 29 due to zero loss or negligible gradients.\n",
      "Skipping node 796 due to zero loss or negligible gradients.\n",
      "Skipping node 16 due to zero loss or negligible gradients.\n",
      "Skipping node 849 due to zero loss or negligible gradients.\n",
      "Skipping node 949 due to zero loss or negligible gradients.\n",
      "Skipping node 1092 due to zero loss or negligible gradients.\n",
      "Skipping node 1416 due to zero loss or negligible gradients.\n",
      "Skipping node 1350 due to zero loss or negligible gradients.\n",
      "Skipping node 785 due to zero loss or negligible gradients.\n",
      "Skipping node 104 due to zero loss or negligible gradients.\n",
      "Skipping node 1124 due to zero loss or negligible gradients.\n",
      "Skipping node 150 due to zero loss or negligible gradients.\n",
      "Skipping node 295 due to zero loss or negligible gradients.\n",
      "Skipping node 1312 due to zero loss or negligible gradients.\n",
      "Skipping node 1190 due to zero loss or negligible gradients.\n",
      "Skipping node 756 due to zero loss or negligible gradients.\n",
      "Skipping node 94 due to zero loss or negligible gradients.\n",
      "Skipping node 1378 due to zero loss or negligible gradients.\n",
      "Skipping node 1209 due to zero loss or negligible gradients.\n",
      "Skipping node 120 due to zero loss or negligible gradients.\n",
      "Skipping node 1083 due to zero loss or negligible gradients.\n",
      "Skipping node 1434 due to zero loss or negligible gradients.\n",
      "Skipping node 1048 due to zero loss or negligible gradients.\n",
      "Skipping node 4 due to zero loss or negligible gradients.\n",
      "Skipping node 1262 due to zero loss or negligible gradients.\n",
      "Skipping node 712 due to zero loss or negligible gradients.\n",
      "Skipping node 1186 due to zero loss or negligible gradients.\n",
      "Skipping node 367 due to zero loss or negligible gradients.\n",
      "Skipping node 1051 due to zero loss or negligible gradients.\n",
      "Skipping node 58 due to zero loss or negligible gradients.\n",
      "Skipping node 1202 due to zero loss or negligible gradients.\n",
      "Skipping node 1087 due to zero loss or negligible gradients.\n",
      "Skipping node 87 due to zero loss or negligible gradients.\n",
      "Skipping node 395 due to zero loss or negligible gradients.\n",
      "Skipping node 31 due to zero loss or negligible gradients.\n",
      "Skipping node 930 due to zero loss or negligible gradients.\n",
      "Skipping node 974 due to zero loss or negligible gradients.\n",
      "Skipping node 277 due to zero loss or negligible gradients.\n",
      "Skipping node 71 due to zero loss or negligible gradients.\n",
      "Skipping node 344 due to zero loss or negligible gradients.\n",
      "Skipping node 55 due to zero loss or negligible gradients.\n",
      "Skipping node 378 due to zero loss or negligible gradients.\n",
      "Skipping node 417 due to zero loss or negligible gradients.\n",
      "Skipping node 221 due to zero loss or negligible gradients.\n",
      "Skipping node 1241 due to zero loss or negligible gradients.\n",
      "Skipping node 775 due to zero loss or negligible gradients.\n",
      "Skipping node 704 due to zero loss or negligible gradients.\n",
      "Skipping node 322 due to zero loss or negligible gradients.\n",
      "Skipping node 550 due to zero loss or negligible gradients.\n",
      "Skipping node 492 due to zero loss or negligible gradients.\n",
      "Skipping node 1009 due to zero loss or negligible gradients.\n",
      "Skipping node 292 due to zero loss or negligible gradients.\n",
      "Skipping node 129 due to zero loss or negligible gradients.\n",
      "Skipping node 1449 due to zero loss or negligible gradients.\n",
      "Skipping node 222 due to zero loss or negligible gradients.\n",
      "Skipping node 1263 due to zero loss or negligible gradients.\n",
      "Skipping node 363 due to zero loss or negligible gradients.\n",
      "Skipping node 1214 due to zero loss or negligible gradients.\n",
      "Skipping node 1161 due to zero loss or negligible gradients.\n",
      "Skipping node 1409 due to zero loss or negligible gradients.\n",
      "Skipping node 531 due to zero loss or negligible gradients.\n",
      "Skipping node 579 due to zero loss or negligible gradients.\n",
      "Skipping node 1460 due to zero loss or negligible gradients.\n",
      "Skipping node 1272 due to zero loss or negligible gradients.\n",
      "Skipping node 1320 due to zero loss or negligible gradients.\n",
      "Skipping node 291 due to zero loss or negligible gradients.\n",
      "Skipping node 981 due to zero loss or negligible gradients.\n",
      "Skipping node 419 due to zero loss or negligible gradients.\n",
      "Skipping node 331 due to zero loss or negligible gradients.\n",
      "Skipping node 1174 due to zero loss or negligible gradients.\n",
      "Skipping node 1453 due to zero loss or negligible gradients.\n",
      "Number of nodes with non-zero loss and non-zero gradients: 211\n",
      "Contents of max_nodes: []\n",
      "Contents of min_nodes: [(207, 0.0035292469763980594, 0.0002302858338225633, 15.325506)]\n",
      "Contents of moyen_nodes: []\n",
      "Contents of nodes_to_attack: [(207, 0.0035292469763980594, 0.0002302858338225633, 15.325506)]\n",
      "Nodes chosen for attack:\n",
      "Node: 207, Total Score: 0.0035, Loss: 0.0002, Gradient Sum: 15.3255\n",
      "Nodes chosen for attack: [207]\n",
      "Nodes that are clean: [1395, 35, 754, 1169, 362, 1183, 597, 255, 1036, 1008, 585, 1046, 1293, 146, 1486, 866, 229, 1057, 1403, 85, 1465, 333, 502, 192, 1050, 1026, 664, 1306, 1485, 309, 1452, 140, 365, 263, 1294, 723, 852, 425, 340, 1471, 397, 1458, 536, 1430, 881, 88, 1065, 6, 834, 271, 1451, 1393, 125, 194, 856, 1069, 1342, 351, 798, 1109, 806, 987, 697, 499, 1388, 864, 1152, 1443, 101, 127, 768, 1139, 1162, 379, 118, 901, 67, 528, 1420, 595, 1334, 883, 382, 132, 742, 1412, 473, 1034, 25, 691, 1309, 1016, 747, 1251, 681, 201, 657, 1359, 878, 614, 715, 258, 837, 21, 560, 693, 706, 1381, 1354, 1090, 1454, 1171, 49, 605, 721, 138, 1370, 418, 1353, 358, 763, 38, 746, 1144, 938, 276, 828, 1101, 854, 1150, 1212, 991, 318, 725, 843, 30, 1102, 394, 1167, 673, 133, 385, 613, 343, 275, 1080, 39, 524, 388, 916, 1017, 549, 727, 1157, 998, 1362, 890, 1321, 683, 484, 117, 1385, 274, 1428, 540, 231, 927, 692, 1044, 294, 480, 230, 247, 402, 874, 608, 1151, 1180, 415, 324, 1266, 210, 359, 338, 1047, 407, 581, 1055, 1112, 1287, 1329, 964, 640, 1390, 232, 1029, 1283, 665, 1252, 1200, 409, 1096, 336, 1027, 396, 1327, 233, 1441, 562, 188, 371, 444, 1138, 238, 1043, 537, 783, 995, 1175, 1119, 89, 410, 832, 7, 1466, 573, 967, 439, 969, 83, 863, 451, 1357, 370, 1188, 757, 801, 645, 357, 65, 1429, 924, 1302, 262, 647, 913, 264, 807, 1002, 628, 360, 993, 1204, 1117, 731, 157, 70, 648, 545, 1007, 239, 215, 412, 577, 364, 1129, 1143, 1391, 824, 541, 634, 777, 646, 816, 788, 554, 1437, 1164, 1289, 456, 559, 237, 106, 1389, 566, 955, 377, 776, 60, 170, 696, 876, 1235, 265, 1380, 74, 690, 126, 1386, 865, 216, 373, 944, 93, 95, 675, 574, 445, 14, 1059, 792, 1472, 1023, 266, 1347, 578, 1324, 679, 760, 884, 711, 936, 198, 301, 350, 543, 1108, 202, 53, 861, 79, 1106, 107, 1314, 1424, 252, 45, 1103, 470, 1125, 1216, 1286, 815, 1383, 770, 66, 272, 1397, 1269, 420, 1121, 36, 892, 907, 8, 1281, 196, 59, 896, 77, 284, 1315, 535, 195, 929, 1478, 954, 191, 867, 751, 1480, 1229, 329, 245, 1414, 686, 827, 1105, 781, 442, 465, 149, 703, 139, 466, 740, 199, 297, 1203, 1039, 180, 124, 225, 1113, 977, 1170, 1091, 1063, 555, 1470, 1318, 818, 176, 1273, 638, 649, 618, 156, 738, 173, 956, 1071, 1249, 1078, 1025, 1085, 1333, 260, 1062, 236, 1028, 506, 829, 682, 400, 1176, 810, 1012, 185, 452, 17, 1237, 848, 76, 482, 1095, 800, 463, 1093, 641, 204, 716, 976, 1461, 761, 423, 644, 479, 910, 698, 700, 179, 1349, 1032, 752, 663, 870, 656, 702, 522, 328, 306, 530, 152, 802, 243, 1257, 50, 1013, 253, 486, 940, 1411, 334, 515, 1483, 583, 1278, 1210, 1072, 1352, 962, 780, 717, 672, 171, 1335, 552, 24, 1142, 438, 755, 889, 143, 111, 1426, 941, 203, 572, 582, 1250, 875, 1344, 491, 1195, 1413, 983, 689, 1134, 489, 753, 208, 1110, 159, 669, 1168, 1331, 278, 462, 1199, 937, 897, 627, 1431, 1457, 850, 453, 1297, 226, 1037, 1098, 1064, 1218, 1319, 1351, 408, 1323, 1086, 688, 299, 426, 102, 758, 630, 227, 116, 561, 282, 1473, 1075, 1130, 766, 478, 1099, 175, 900, 467, 1003, 1018, 858, 488, 403, 389, 684, 654, 435, 765, 154, 52, 283, 268, 632, 569, 772, 23, 1446, 769, 1056, 404, 241, 136, 19, 853, 304, 308, 1243, 1439, 346, 320, 959, 398, 1192, 1394, 421, 1392, 259, 602, 734, 43, 951, 1285, 510, 366, 131, 174, 911, 1066, 323, 84, 1442, 869, 267, 235, 525, 1182, 1479, 1340, 1022, 250, 13, 517, 624, 982, 181, 1290, 844, 934, 113, 1332, 1448, 899, 148, 520, 668, 1425, 128, 1233, 300, 1079, 830, 1384, 1247, 1042, 791, 217, 34, 1360, 305, 1033, 1131, 1440, 178, 42, 513, 1153, 699, 619, 895, 1435, 1375, 464, 558, 163, 1338, 739, 1417, 1173, 912, 387, 41, 1468, 28, 643, 1444, 833, 1419, 599, 705, 381, 611, 187, 1280, 1477, 1404, 112, 814, 1253, 256, 942, 1221, 335, 1040, 1215, 965, 762, 526, 183, 269, 115, 1346, 1469, 1228, 1225, 1133, 920, 429, 588, 1361, 620, 877, 68, 958, 786, 823, 1166, 601, 1367, 1179, 922, 1270, 931, 1295, 1467, 1004, 1481, 119, 342, 11, 548, 1197, 1242, 223, 206, 391, 787, 1145, 81, 658, 512, 655, 871, 985, 1400, 748, 347, 978, 293, 1084, 1316, 82, 64, 1058, 542, 177, 553, 197, 908, 448, 455, 401, 72, 341, 1337, 633, 1123, 182, 1254, 687, 290, 1074, 1239, 737, 1366, 1376, 623, 1261, 62, 504, 551, 1308, 1193, 1326, 676, 273, 846, 1288, 211, 621, 244, 794, 1301, 437, 212, 383, 507, 494, 1445, 1292, 1399, 485, 879, 1398, 1219, 145, 169, 380, 950, 820, 427, 1148, 330, 891, 424, 809, 1211, 1049, 719, 808, 728, 925, 1126, 764, 317, 141, 1156, 1011, 368, 567, 1310, 22, 1208, 167, 319, 56, 1116, 1433, 1355, 580, 612, 248, 33, 1264, 1226, 935, 1020, 390, 918, 1097, 821, 1298, 44, 393, 1038, 784, 99, 893, 1487, 1198, 997, 533, 708, 709, 1111, 1410, 1147, 29, 796, 16, 849, 949, 564, 1092, 1416, 1350, 785, 104, 1124, 150, 610, 295, 1312, 472, 1190, 756, 94, 1378, 1209, 120, 1083, 213, 1476, 1434, 1048, 4, 1262, 712, 1186, 367, 1051, 58, 1202, 1087, 87, 395, 31, 930, 974, 1146, 277, 71, 344, 55, 378, 417, 221, 1241, 775, 704, 322, 550, 492, 1009, 121, 292, 129, 996, 695, 475, 1449, 222, 1263, 363, 1214, 1161, 1409, 168, 531, 246, 579, 1460, 1272, 1320, 134, 291, 981, 419, 331, 1174, 1453]\n",
      "Nodes chosen for attack: [207]\n",
      "Nodes that are clean: [1395, 35, 754, 1169, 362, 1183, 597, 255, 1036, 1008, 585, 1046, 1293, 146, 1486, 866, 229, 1057, 1403, 85, 1465, 333, 502, 192, 1050, 1026, 664, 1306, 1485, 309, 1452, 140, 365, 263, 1294, 723, 852, 425, 340, 1471, 397, 1458, 536, 1430, 881, 88, 1065, 6, 834, 271, 1451, 1393, 125, 194, 856, 1069, 1342, 351, 798, 1109, 806, 987, 697, 499, 1388, 864, 1152, 1443, 101, 127, 768, 1139, 1162, 379, 118, 901, 67, 528, 1420, 595, 1334, 883, 382, 132, 742, 1412, 473, 1034, 25, 691, 1309, 1016, 747, 1251, 681, 201, 657, 1359, 878, 614, 715, 258, 837, 21, 560, 693, 706, 1381, 1354, 1090, 1454, 1171, 49, 605, 721, 138, 1370, 418, 1353, 358, 763, 38, 746, 1144, 938, 276, 828, 1101, 854, 1150, 1212, 991, 318, 725, 843, 30, 1102, 394, 1167, 673, 133, 385, 613, 343, 275, 1080, 39, 524, 388, 916, 1017, 549, 727, 1157, 998, 1362, 890, 1321, 683, 484, 117, 1385, 274, 1428, 540, 231, 927, 692, 1044, 294, 480, 230, 247, 402, 874, 608, 1151, 1180, 415, 324, 1266, 210, 359, 338, 1047, 407, 581, 1055, 1112, 1287, 1329, 964, 640, 1390, 232, 1029, 1283, 665, 1252, 1200, 409, 1096, 336, 1027, 396, 1327, 233, 1441, 562, 188, 371, 444, 1138, 238, 1043, 537, 783, 995, 1175, 1119, 89, 410, 832, 7, 1466, 573, 967, 439, 969, 83, 863, 451, 1357, 370, 1188, 757, 801, 645, 357, 65, 1429, 924, 1302, 262, 647, 913, 264, 807, 1002, 628, 360, 993, 1204, 1117, 731, 157, 70, 648, 545, 1007, 239, 215, 412, 577, 364, 1129, 1143, 1391, 824, 541, 634, 777, 646, 816, 788, 554, 1437, 1164, 1289, 456, 559, 237, 106, 1389, 566, 955, 377, 776, 60, 170, 696, 876, 1235, 265, 1380, 74, 690, 126, 1386, 865, 216, 373, 944, 93, 95, 675, 574, 445, 14, 1059, 792, 1472, 1023, 266, 1347, 578, 1324, 679, 760, 884, 711, 936, 198, 301, 350, 543, 1108, 202, 53, 861, 79, 1106, 107, 1314, 1424, 252, 45, 1103, 470, 1125, 1216, 1286, 815, 1383, 770, 66, 272, 1397, 1269, 420, 1121, 36, 892, 907, 8, 1281, 196, 59, 896, 77, 284, 1315, 535, 195, 929, 1478, 954, 191, 867, 751, 1480, 1229, 329, 245, 1414, 686, 827, 1105, 781, 442, 465, 149, 703, 139, 466, 740, 199, 297, 1203, 1039, 180, 124, 225, 1113, 977, 1170, 1091, 1063, 555, 1470, 1318, 818, 176, 1273, 638, 649, 618, 156, 738, 173, 956, 1071, 1249, 1078, 1025, 1085, 1333, 260, 1062, 236, 1028, 506, 829, 682, 400, 1176, 810, 1012, 185, 452, 17, 1237, 848, 76, 482, 1095, 800, 463, 1093, 641, 204, 716, 976, 1461, 761, 423, 644, 479, 910, 698, 700, 179, 1349, 1032, 752, 663, 870, 656, 702, 522, 328, 306, 530, 152, 802, 243, 1257, 50, 1013, 253, 486, 940, 1411, 334, 515, 1483, 583, 1278, 1210, 1072, 1352, 962, 780, 717, 672, 171, 1335, 552, 24, 1142, 438, 755, 889, 143, 111, 1426, 941, 203, 572, 582, 1250, 875, 1344, 491, 1195, 1413, 983, 689, 1134, 489, 753, 208, 1110, 159, 669, 1168, 1331, 278, 462, 1199, 937, 897, 627, 1431, 1457, 850, 453, 1297, 226, 1037, 1098, 1064, 1218, 1319, 1351, 408, 1323, 1086, 688, 299, 426, 102, 758, 630, 227, 116, 561, 282, 1473, 1075, 1130, 766, 478, 1099, 175, 900, 467, 1003, 1018, 858, 488, 403, 389, 684, 654, 435, 765, 154, 52, 283, 268, 632, 569, 772, 23, 1446, 769, 1056, 404, 241, 136, 19, 853, 304, 308, 1243, 1439, 346, 320, 959, 398, 1192, 1394, 421, 1392, 259, 602, 734, 43, 951, 1285, 510, 366, 131, 174, 911, 1066, 323, 84, 1442, 869, 267, 235, 525, 1182, 1479, 1340, 1022, 250, 13, 517, 624, 982, 181, 1290, 844, 934, 113, 1332, 1448, 899, 148, 520, 668, 1425, 128, 1233, 300, 1079, 830, 1384, 1247, 1042, 791, 217, 34, 1360, 305, 1033, 1131, 1440, 178, 42, 513, 1153, 699, 619, 895, 1435, 1375, 464, 558, 163, 1338, 739, 1417, 1173, 912, 387, 41, 1468, 28, 643, 1444, 833, 1419, 599, 705, 381, 611, 187, 1280, 1477, 1404, 112, 814, 1253, 256, 942, 1221, 335, 1040, 1215, 965, 762, 526, 183, 269, 115, 1346, 1469, 1228, 1225, 1133, 920, 429, 588, 1361, 620, 877, 68, 958, 786, 823, 1166, 601, 1367, 1179, 922, 1270, 931, 1295, 1467, 1004, 1481, 119, 342, 11, 548, 1197, 1242, 223, 206, 391, 787, 1145, 81, 658, 512, 655, 871, 985, 1400, 748, 347, 978, 293, 1084, 1316, 82, 64, 1058, 542, 177, 553, 197, 908, 448, 455, 401, 72, 341, 1337, 633, 1123, 182, 1254, 687, 290, 1074, 1239, 737, 1366, 1376, 623, 1261, 62, 504, 551, 1308, 1193, 1326, 676, 273, 846, 1288, 211, 621, 244, 794, 1301, 437, 212, 383, 507, 494, 1445, 1292, 1399, 485, 879, 1398, 1219, 145, 169, 380, 950, 820, 427, 1148, 330, 891, 424, 809, 1211, 1049, 719, 808, 728, 925, 1126, 764, 317, 141, 1156, 1011, 368, 567, 1310, 22, 1208, 167, 319, 56, 1116, 1433, 1355, 580, 612, 248, 33, 1264, 1226, 935, 1020, 390, 918, 1097, 821, 1298, 44, 393, 1038, 784, 99, 893, 1487, 1198, 997, 533, 708, 709, 1111, 1410, 1147, 29, 796, 16, 849, 949, 564, 1092, 1416, 1350, 785, 104, 1124, 150, 610, 295, 1312, 472, 1190, 756, 94, 1378, 1209, 120, 1083, 213, 1476, 1434, 1048, 4, 1262, 712, 1186, 367, 1051, 58, 1202, 1087, 87, 395, 31, 930, 974, 1146, 277, 71, 344, 55, 378, 417, 221, 1241, 775, 704, 322, 550, 492, 1009, 121, 292, 129, 996, 695, 475, 1449, 222, 1263, 363, 1214, 1161, 1409, 168, 531, 246, 579, 1460, 1272, 1320, 134, 291, 981, 419, 331, 1174, 1453]\n",
      "Test set results: loss= 0.3643 accuracy= 0.8547\n",
      "Test set results: loss= 0.6464 accuracy= 1.0000\n",
      "Test set results: loss= 0.3640 accuracy= 0.8546\n",
      "Test accuracy on attack set:  1.0\n",
      "Test accuracy on clean set:  0.8545837723919916\n",
      " number of nodes to attack: 1\n",
      "budget: 1 alpha(gradient's importance): 1.0 beta( commun neighbor's importance): 1.0\n",
      "Target node is: 207 with label: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahsa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\sparse\\_index.py:102: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added edge: (207, 1477)\n",
      "Gradient: 0.012218998745083809\n",
      "Number of common neighbors: 1\n",
      "Score: 1.0122189987450838\n",
      "Total number of edges added: 1\n",
      "Total number of edges removed: 0\n",
      "Test set results: loss= 0.7027 accuracy= 0.0000\n",
      "Test set results: loss= 0.3859 accuracy= 0.8493\n",
      "Test accuracy on attack set after attack:  0.0\n",
      "Test accuracy on clean set after attack:  0.8493150684931506\n",
      "*************** Crypto'Graph defense ***************\n",
      "Dropping dissimilar edges using metric :  neighbors  on links\n",
      "removed 2215 edges in polblogs 1\n",
      "removed 1664 edges in polblogs 2\n",
      "*** polblogs 1 ***\n",
      "Test set results: loss= 0.8147 accuracy= 0.0000\n",
      "*** polblogs 2 ***\n",
      "Test set results: loss= 0.8072 accuracy= 0.0000\n",
      "*** polblogs 1 ***\n",
      "Test set results: loss= 0.4530 accuracy= 0.7355\n",
      "*** polblogs 2 ***\n",
      "Test set results: loss= 0.4586 accuracy= 0.7302\n",
      "Test accuracy on attack set after Crypto'Graph:  (0.0, 0.0)\n",
      "Test accuracy on clean set after Crypto'Graph:  (0.7355110642781876, 0.7302423603793466)\n",
      "Total number of removed edges by CG: 2215\n",
      "Inserted edges removed by CG: [(207, 1477)]\n",
      "Number of inserted edges removed by CG: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########  scenario 3: surrogate: adj, ACC1:adj, attaque:adj, ACC2: adj^,   Crypto'Graph: adj^, adj2  , ACC3: adj*\n",
    "# mahsa-V7 : improvements on V5.1 : changing to remove edges too. change: calculate_edge_scores_all_actions and perturb_edges_between_targets_all_actions\n",
    "# now it considers just ACC3 for adj1 after CG defense.\n",
    "# initial train on adj1  -DONE BEFORE ATTACK in the main execution\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from deeprobust.graph.defense import GCN\n",
    "from deeprobust.graph.utils import *\n",
    "from deeprobust.graph.data import Dataset\n",
    "from DistributedDefense import TwoPartyCNGCN\n",
    "from experiments import split_dataset\n",
    "from scipy.sparse import csr_matrix\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Train surrogate model\n",
    "def train_surrogate_model(features, adj, labels, idx_train, idx_val):\n",
    "    surrogate_model = GCN(nfeat=features.shape[1], nclass=labels.max().item() + 1, nhid=16, device=device, dropout=0.5)\n",
    "    surrogate_model = surrogate_model.to(device)\n",
    "    surrogate_model.fit(features, adj, labels, idx_train, idx_val=idx_val, patience=30, train_iters=200)\n",
    "    return surrogate_model\n",
    "\n",
    "# Compute gradients of loss w.r.t adjacency matrix\n",
    "def compute_gradients(surrogate_model, features, adj, labels, target_node):\n",
    "    surrogate_model.eval()\n",
    "    adj = torch.FloatTensor(adj.toarray()).to(device)\n",
    "    adj.requires_grad = True\n",
    "    features = torch.FloatTensor(features.toarray()).to(device)\n",
    "    labels = torch.LongTensor(labels).to(device)\n",
    "    output = surrogate_model(features, adj)\n",
    "    loss = F.nll_loss(output[[target_node]], labels[[target_node]])\n",
    "    loss.backward()\n",
    "    gradients = adj.grad.cpu().numpy()\n",
    "    return loss.item(), gradients\n",
    "\n",
    "\n",
    "def calc_nodes_to_attack(surrogate_model, features, adj, labels, idx_test, max_count=0, min_count=0, moyen_count=0, manual_nodes=None):\n",
    "    if manual_nodes is not None and len(manual_nodes) > 0:\n",
    "        print(\"Using manually selected nodes.\")\n",
    "        return [(node, 0, 0, 0) for node in manual_nodes]  # Dummy values for score, loss, gradient sum\n",
    "\n",
    "    non_zero_loss_and_gradients_nodes = []\n",
    "\n",
    "    for target_node in idx_test:\n",
    "        loss, gradients = compute_gradients(surrogate_model, features, adj, labels, target_node)\n",
    "\n",
    "        # Filter based on zero loss or negligible gradient values\n",
    "        gradient_sum = np.sum(np.abs(gradients))\n",
    "        if loss < 1e-5 or gradient_sum < 1e-12:  # Using a small threshold to account for numerical precision issues\n",
    "            print(f\"Skipping node {target_node} due to zero loss or negligible gradients.\")\n",
    "            continue\n",
    "\n",
    "        impact_score = loss * gradient_sum\n",
    "        non_zero_loss_and_gradients_nodes.append((target_node, impact_score, loss, gradient_sum))\n",
    "\n",
    "    print(f\"Number of nodes with non-zero loss and non-zero gradients: {len(non_zero_loss_and_gradients_nodes)}\")\n",
    "\n",
    "    # Sort nodes by impact score\n",
    "    # sorted_nodes = sorted(non_zero_loss_and_gradients_nodes, key=lambda x: x[1])\n",
    "    \n",
    "    sorted_nodes = sorted(non_zero_loss_and_gradients_nodes, key=lambda x: (x[1], x[2]))  # Sort by impact score, then by loss\n",
    "\n",
    "\n",
    "\n",
    "    # Selecting the top nodes with maximum loss and gradient\n",
    "    max_nodes = sorted_nodes[-max_count:] if max_count > 0 else []\n",
    "    # Selecting the bottom nodes with minimum loss and gradient\n",
    "    min_nodes = sorted_nodes[:min_count] if min_count > 0 else []\n",
    "    # Selecting nodes with median (moyen) loss and gradient\n",
    "    median_index = len(sorted_nodes) // 2\n",
    "    moyen_nodes = sorted_nodes[max(0, median_index - moyen_count//2): min(len(sorted_nodes), median_index + moyen_count//2)] if moyen_count > 0 else []\n",
    "\n",
    "    return max_nodes, min_nodes, moyen_nodes\n",
    "\n",
    "\n",
    "# Calculate common neighbors between two nodes\n",
    "def calculate_common_neighbors(adj, node1, node2):\n",
    "    neighbors1 = set(adj[node1].indices)\n",
    "    neighbors2 = set(adj[node2].indices)\n",
    "    common_neighbors = neighbors1 & neighbors2\n",
    "    return len(common_neighbors)\n",
    "\n",
    "\n",
    "### change to consider all actions (add/remove) for each target node\n",
    "def calculate_edge_scores_all_actions(adj, gradients, target_node, labels, alpha, beta):\n",
    "    scores = []\n",
    "    for i in range(adj.shape[0]):\n",
    "        if i == target_node:\n",
    "            continue\n",
    "        common_neighbors = calculate_common_neighbors(adj, target_node, i)\n",
    "        if adj[target_node, i] == 0:\n",
    "            # Edge does not exist, consider adding it\n",
    "            delta_loss = gradients[target_node, i]\n",
    "            score = delta_loss * alpha + common_neighbors * beta\n",
    "            action = 'add'\n",
    "        elif adj[target_node, i] == 1:\n",
    "            # Edge exists, consider removing it\n",
    "            delta_loss = -gradients[target_node, i]\n",
    "            score = delta_loss * alpha + common_neighbors * beta\n",
    "            action = 'remove'\n",
    "        else:\n",
    "            continue\n",
    "        scores.append((target_node, i, action, score, delta_loss, common_neighbors))\n",
    "    return scores\n",
    "\n",
    "def perturb_edges_between_targets(adj, surrogate_model, features, labels, idx_test_attack, budget, alpha, beta):\n",
    "    attacked_adj = adj.copy()\n",
    "    added_edges = []\n",
    "    removed_edges = []\n",
    "    edge_added_count = 0\n",
    "    edge_removed_count = 0\n",
    "\n",
    "    for target_node in idx_test_attack:\n",
    "        print(f\"Target node is: {target_node} with label: {labels[target_node]}\")\n",
    "        loss, gradients = compute_gradients(surrogate_model, features, attacked_adj, labels, target_node)\n",
    "        if not np.any(gradients):\n",
    "            continue\n",
    "        for _ in range(budget):\n",
    "            scores = calculate_edge_scores_all_actions(attacked_adj, gradients, target_node, labels, alpha, beta)\n",
    "            if not scores:\n",
    "                break\n",
    "            # Select the edge and action with the highest score\n",
    "            best_edge_info = max(scores, key=lambda x: x[3])  # x[3] is the score\n",
    "            edge = (best_edge_info[0], best_edge_info[1])\n",
    "            action = best_edge_info[2]\n",
    "            score = best_edge_info[3]\n",
    "            delta_loss = best_edge_info[4]\n",
    "            common_neighbors = best_edge_info[5]\n",
    "\n",
    "            if action == 'add':\n",
    "                # Perform add\n",
    "                attacked_adj[edge[0], edge[1]] = 1\n",
    "                attacked_adj[edge[1], edge[0]] = 1\n",
    "                added_edges.append(edge)\n",
    "                edge_added_count += 1\n",
    "                print(f\"Added edge: {edge}\")\n",
    "                print(f\"Gradient: {gradients[edge[0], edge[1]]}\")\n",
    "                print(f\"Number of common neighbors: {common_neighbors}\")\n",
    "                print(f\"Score: {score}\")\n",
    "            elif action == 'remove':\n",
    "                # Perform remove\n",
    "                attacked_adj[edge[0], edge[1]] = 0\n",
    "                attacked_adj[edge[1], edge[0]] = 0\n",
    "                removed_edges.append(edge)\n",
    "                edge_removed_count += 1\n",
    "                print(f\"Removed edge: {edge}\")\n",
    "                print(f\"Gradient: {gradients[edge[0], edge[1]]}\")\n",
    "                print(f\"Number of common neighbors: {common_neighbors}\")\n",
    "                print(f\"Score: {score}\")\n",
    "\n",
    "    print(f\"Total number of edges added: {edge_added_count}\") \n",
    "    print(f\"Total number of edges removed: {edge_removed_count}\") \n",
    "    return attacked_adj, added_edges, removed_edges\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main execution\n",
    "################################ Data loading #######################################\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load data\n",
    "dataset = \"polblogs\"\n",
    "data = Dataset(root='.', name=dataset, setting='gcn', seed=15)\n",
    "seed = 42\n",
    " \n",
    "set_seeds(seed)\n",
    "adj, features, labels = data.adj, data.features, data.labels\n",
    "idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
    "\n",
    "proportion_of_common_links = 0.5\n",
    "adj1, adj2 = split_dataset(adj, proportion_of_common_links) \n",
    "\n",
    "############  Train surrogate model and determine nodes to attack\n",
    "surrogate_model = train_surrogate_model(features, adj, labels, idx_train, idx_val)\n",
    "\n",
    "# k = 10 # budget in the meaning of Number of nodes to attack\n",
    "# nodes_to_attack = calc_nodes_to_attack(surrogate_model, features, adj, labels, idx_test)\n",
    "max_count = 0    # Number of nodes with maximum loss and gradients\n",
    "min_count = 1    # Number of nodes with minimum loss and gradients\n",
    "moyen_count = 0  # Number of nodes with median (moyen) loss and gradients\n",
    "manual_nodes = [275, 1130, 716, 1182, 213, 121]  # List of nodes to attack manually\n",
    "max_nodes, min_nodes, moyen_nodes = calc_nodes_to_attack(surrogate_model, features, adj, labels, idx_test, max_count, min_count, moyen_count)\n",
    "# max_nodes, min_nodes, moyen_nodes = calc_nodes_to_attack(surrogate_model, features, adj, labels, idx_test, manual_nodes=manual_nodes)\n",
    "\n",
    "print(\"Contents of max_nodes:\", max_nodes)\n",
    "print(\"Contents of min_nodes:\", min_nodes)\n",
    "print(\"Contents of moyen_nodes:\", moyen_nodes)\n",
    "\n",
    "# Combine into nodes_to_attack\n",
    "nodes_to_attack = max_nodes + min_nodes + moyen_nodes\n",
    "print(\"Contents of nodes_to_attack:\", nodes_to_attack)\n",
    "\n",
    "# Print out node information\n",
    "print(\"Nodes chosen for attack:\")\n",
    "for node_info in nodes_to_attack:\n",
    "    if isinstance(node_info, tuple):\n",
    "        node, score, loss, gradient_sum = node_info\n",
    "        print(f\"Node: {node}, Total Score: {score:.4f}, Loss: {loss:.4f}, Gradient Sum: {gradient_sum:.4f}\")\n",
    "    else:\n",
    "        print(f\"Unexpected format: {node_info} (type: {type(node_info)})\")\n",
    "\n",
    "\n",
    "# Extract only the node IDs from nodes_to_attack\n",
    "nodes_to_attack_ids = [node_info[0] for node_info in nodes_to_attack if isinstance(node_info, tuple)]\n",
    "idx_test_attack = nodes_to_attack_ids\n",
    "# Identify clean nodes\n",
    "idx_test_clean = [node for node in idx_test if node not in nodes_to_attack_ids]\n",
    "\n",
    "print(\"Nodes chosen for attack:\", nodes_to_attack_ids)\n",
    "print(\"Nodes that are clean:\", idx_test_clean)\n",
    "\n",
    "print(\"Nodes chosen for attack:\", idx_test_attack)\n",
    "print(\"Nodes that are clean:\", idx_test_clean)\n",
    "\n",
    "\n",
    "########### Train GCN model initially for evaluation before attack\n",
    "model = GCN(nfeat=features.shape[1], nclass=labels.max().item()+1,\n",
    "            nhid=16, device=device, dropout=0.5)\n",
    "model = model.to(device)\n",
    "model.fit(features, adj, labels, idx_train, idx_val=idx_val, patience=30, train_iters=200)\n",
    "model.eval()\n",
    "output = model.test(idx_test)\n",
    "\n",
    "accuracy_test_attack_1 = model.test(idx_test_attack) \n",
    "accuracy_test_clean_1 = model.test(idx_test_clean)\n",
    "print(\"Test accuracy on attack set: \", accuracy_test_attack_1)\n",
    "print(\"Test accuracy on clean set: \", accuracy_test_clean_1)\n",
    "\n",
    "############# perform attack Perturb edges\n",
    "budget = 1 # Number of edges to add or remove for each target node\n",
    "alpha = 1.0\n",
    "beta = 1.0\n",
    "print(f\" number of nodes to attack: {len(idx_test_attack)}\")\n",
    "print(f\"budget: {budget}\", f\"alpha(gradient's importance): {alpha}\", F\"beta( commun neighbor's importance): {beta}\")\n",
    "\n",
    "attacked_adj_1, added_edges, removed_edges = perturb_edges_between_targets(adj, surrogate_model, features, labels, idx_test_attack, budget, alpha, beta)\n",
    "\n",
    "# Model evaluation after attack\n",
    "model.fit(features, attacked_adj_1, labels, idx_train, idx_val=idx_val, patience=30, train_iters=200)\n",
    "model.eval()\n",
    "\n",
    "accuracy_test_attack_2 = model.test(idx_test_attack)\n",
    "accuracy_test_clean_2 = model.test(idx_test_clean)\n",
    "print(\"Test accuracy on attack set after attack: \", accuracy_test_attack_2)\n",
    "print(\"Test accuracy on clean set after attack: \", accuracy_test_clean_2)\n",
    "\n",
    "##################   Crypto'Graph defense\n",
    "print(\"*************** Crypto'Graph defense ***************\")\n",
    "\n",
    "threshold = 2\n",
    "metric = \"neighbors\"\n",
    "object = \"links\"\n",
    "\n",
    "model = TwoPartyCNGCN(dataset=dataset, nfeat=features.shape[1], nhid=16, nclass=labels.max().item() + 1, device=device)\n",
    "defense_duration, defense_duration, training_duration1, training_duration2, CG_defended_adj1, CG_defended_adj2 = model.fit(\n",
    "        attacked_adj_1.copy(), adj2.copy(), features, features, labels, idx_train, threshold, metric=metric, object=object,\n",
    "        train_iters=200, initialize=True, verbose=False, idx_val=idx_val)\n",
    "model.eval()\n",
    "\n",
    "accuracy_test_attack_3 = model.test(idx_test_attack)\n",
    "accuracy_test_clean_3 = model.test(idx_test_clean)\n",
    "print(\"Test accuracy on attack set after Crypto'Graph: \", accuracy_test_attack_3)\n",
    "print(\"Test accuracy on clean set after Crypto'Graph: \", accuracy_test_clean_3)\n",
    "\n",
    "# Use only the accuracy for the first part of the graph (adj1)\n",
    "accuracy_test_attack_3_adj1 = accuracy_test_attack_3[0]  # First part for attack set\n",
    "accuracy_test_clean_3_adj1 = accuracy_test_clean_3[0]  # First part for clean set\n",
    "\n",
    "\n",
    "# Function to find removed edges\n",
    "# Function to find removed edges  - should compare adj after attack and cg \n",
    "def find_removed_edges(attacked_adj_1, adj2, defended_adj1, defended_adj2):\n",
    "    removed_edges = []\n",
    "    attacked_adj_1 = attacked_adj_1.toarray()\n",
    "    adj2 = adj2.toarray()\n",
    "    defended_adj1 = defended_adj1.toarray()\n",
    "    defended_adj2 = defended_adj2.toarray()\n",
    "    combined_attacked_adj = np.maximum(attacked_adj_1, adj2)\n",
    "    combined_defended_adj = np.maximum(defended_adj1, defended_adj2)\n",
    "    for i in range(combined_attacked_adj.shape[0]):\n",
    "        for j in range(i + 1, combined_attacked_adj.shape[1]):\n",
    "            if combined_attacked_adj[i, j] == 1 and combined_defended_adj[i, j] == 0:\n",
    "                removed_edges.append((i, j))\n",
    "    return removed_edges\n",
    "\n",
    "# Find all removed edges\n",
    "removed_edges = find_removed_edges(attacked_adj_1, adj2, CG_defended_adj1, CG_defended_adj2)\n",
    "print(f\"Total number of removed edges by CG: {len(removed_edges)}\")\n",
    "\n",
    "\n",
    "# Check if any of the inserted edges during the attack were removed by CG\n",
    "removed_inserted_edges = [edge for edge in added_edges if edge in removed_edges]\n",
    "print(f\"Inserted edges removed by CG: {removed_inserted_edges}\")\n",
    "print(f\"Number of inserted edges removed by CG: {len(removed_inserted_edges)}\")\n",
    "\n",
    "################ Save and plot results\n",
    "accuracy_test_attack_1_std = np.std(accuracy_test_attack_1)\n",
    "accuracy_test_clean_1_std = np.std(accuracy_test_clean_1)\n",
    "accuracy_test_attack_2_std = np.std(accuracy_test_attack_2)\n",
    "accuracy_test_clean_2_std = np.std(accuracy_test_clean_2)\n",
    "accuracy_test_attack_3_std = np.std(accuracy_test_attack_3_adj1)\n",
    "accuracy_test_clean_3_std = np.std(accuracy_test_clean_3_adj1)\n",
    "\n",
    "with open('variables_std.pkl', 'wb') as f:\n",
    "    pickle.dump([accuracy_test_attack_1_std, accuracy_test_clean_1_std, \n",
    "                 accuracy_test_attack_2_std, accuracy_test_clean_2_std, \n",
    "                 accuracy_test_attack_3_std, accuracy_test_clean_3_std], f)\n",
    "\n",
    "accuracy_test_attack_1_avg = np.mean(accuracy_test_attack_1)\n",
    "accuracy_test_clean_1_avg = np.mean(accuracy_test_clean_1)\n",
    "accuracy_test_attack_2_avg = np.mean(accuracy_test_attack_2)\n",
    "accuracy_test_clean_2_avg = np.mean(accuracy_test_clean_2)\n",
    "accuracy_test_attack_3_avg = np.mean(accuracy_test_attack_3_adj1)\n",
    "accuracy_test_clean_3_avg = np.mean(accuracy_test_clean_3_adj1)\n",
    "\n",
    "with open('variables.pkl', 'wb') as f:\n",
    "    pickle.dump([accuracy_test_attack_1_avg, accuracy_test_clean_1_avg, \n",
    "                 accuracy_test_attack_2_avg, accuracy_test_clean_2_avg, \n",
    "                 accuracy_test_attack_3_avg, accuracy_test_clean_3_avg], f)\n",
    "\n",
    "labels = ['ACCs before attack', 'ACCs before attack', \n",
    "          'ACCs after attack', 'ACCs after attack', \n",
    "          'ACCs after CryptoGraph', 'ACCs after CryptoGraph']\n",
    "values = [accuracy_test_attack_1_avg, accuracy_test_clean_1_avg, \n",
    "          accuracy_test_attack_2_avg, accuracy_test_clean_2_avg, \n",
    "          accuracy_test_attack_3_avg, accuracy_test_clean_3_avg]\n",
    "std_devs = [accuracy_test_attack_1_std, accuracy_test_clean_1_std, \n",
    "            accuracy_test_attack_2_std, accuracy_test_clean_2_std, \n",
    "            accuracy_test_attack_3_std, accuracy_test_clean_3_std]\n",
    "x = np.arange(len(labels)//2)\n",
    "width = 0.35\n",
    "bars1 = plt.bar(x - width/2, values[::2], width, yerr=std_devs[::2], label='Attack set', color='red', capsize=5)\n",
    "bars2 = plt.bar(x + width/2, values[1::2], width, yerr=std_devs[1::2], label='Clean set', color='blue', capsize=5)\n",
    "plt.title('Average Accuracies')\n",
    "plt.xlabel('Levels of Attack and Defense')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.xticks(x, labels[::2])\n",
    "plt.legend()\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading polblogs dataset...\n",
      "Skipping node 1395 due to zero loss or negligible gradients.\n",
      "Skipping node 35 due to zero loss or negligible gradients.\n",
      "Skipping node 754 due to zero loss or negligible gradients.\n",
      "Skipping node 1169 due to zero loss or negligible gradients.\n",
      "Skipping node 362 due to zero loss or negligible gradients.\n",
      "Skipping node 1183 due to zero loss or negligible gradients.\n",
      "Skipping node 597 due to zero loss or negligible gradients.\n",
      "Skipping node 255 due to zero loss or negligible gradients.\n",
      "Skipping node 1036 due to zero loss or negligible gradients.\n",
      "Skipping node 1008 due to zero loss or negligible gradients.\n",
      "Skipping node 1046 due to zero loss or negligible gradients.\n",
      "Skipping node 146 due to zero loss or negligible gradients.\n",
      "Skipping node 1486 due to zero loss or negligible gradients.\n",
      "Skipping node 1057 due to zero loss or negligible gradients.\n",
      "Skipping node 1403 due to zero loss or negligible gradients.\n",
      "Skipping node 85 due to zero loss or negligible gradients.\n",
      "Skipping node 502 due to zero loss or negligible gradients.\n",
      "Skipping node 192 due to zero loss or negligible gradients.\n",
      "Skipping node 1050 due to zero loss or negligible gradients.\n",
      "Skipping node 1306 due to zero loss or negligible gradients.\n",
      "Skipping node 1485 due to zero loss or negligible gradients.\n",
      "Skipping node 309 due to zero loss or negligible gradients.\n",
      "Skipping node 140 due to zero loss or negligible gradients.\n",
      "Skipping node 263 due to zero loss or negligible gradients.\n",
      "Skipping node 1294 due to zero loss or negligible gradients.\n",
      "Skipping node 425 due to zero loss or negligible gradients.\n",
      "Skipping node 340 due to zero loss or negligible gradients.\n",
      "Skipping node 1471 due to zero loss or negligible gradients.\n",
      "Skipping node 1458 due to zero loss or negligible gradients.\n",
      "Skipping node 536 due to zero loss or negligible gradients.\n",
      "Skipping node 1430 due to zero loss or negligible gradients.\n",
      "Skipping node 881 due to zero loss or negligible gradients.\n",
      "Skipping node 88 due to zero loss or negligible gradients.\n",
      "Skipping node 6 due to zero loss or negligible gradients.\n",
      "Skipping node 834 due to zero loss or negligible gradients.\n",
      "Skipping node 271 due to zero loss or negligible gradients.\n",
      "Skipping node 1393 due to zero loss or negligible gradients.\n",
      "Skipping node 856 due to zero loss or negligible gradients.\n",
      "Skipping node 1069 due to zero loss or negligible gradients.\n",
      "Skipping node 1342 due to zero loss or negligible gradients.\n",
      "Skipping node 351 due to zero loss or negligible gradients.\n",
      "Skipping node 798 due to zero loss or negligible gradients.\n",
      "Skipping node 806 due to zero loss or negligible gradients.\n",
      "Skipping node 987 due to zero loss or negligible gradients.\n",
      "Skipping node 1388 due to zero loss or negligible gradients.\n",
      "Skipping node 864 due to zero loss or negligible gradients.\n",
      "Skipping node 1152 due to zero loss or negligible gradients.\n",
      "Skipping node 1443 due to zero loss or negligible gradients.\n",
      "Skipping node 101 due to zero loss or negligible gradients.\n",
      "Skipping node 1162 due to zero loss or negligible gradients.\n",
      "Skipping node 67 due to zero loss or negligible gradients.\n",
      "Skipping node 1420 due to zero loss or negligible gradients.\n",
      "Skipping node 595 due to zero loss or negligible gradients.\n",
      "Skipping node 1334 due to zero loss or negligible gradients.\n",
      "Skipping node 883 due to zero loss or negligible gradients.\n",
      "Skipping node 382 due to zero loss or negligible gradients.\n",
      "Skipping node 132 due to zero loss or negligible gradients.\n",
      "Skipping node 742 due to zero loss or negligible gradients.\n",
      "Skipping node 1412 due to zero loss or negligible gradients.\n",
      "Skipping node 25 due to zero loss or negligible gradients.\n",
      "Skipping node 1309 due to zero loss or negligible gradients.\n",
      "Skipping node 1016 due to zero loss or negligible gradients.\n",
      "Skipping node 1251 due to zero loss or negligible gradients.\n",
      "Skipping node 201 due to zero loss or negligible gradients.\n",
      "Skipping node 1359 due to zero loss or negligible gradients.\n",
      "Skipping node 878 due to zero loss or negligible gradients.\n",
      "Skipping node 614 due to zero loss or negligible gradients.\n",
      "Skipping node 715 due to zero loss or negligible gradients.\n",
      "Skipping node 258 due to zero loss or negligible gradients.\n",
      "Skipping node 837 due to zero loss or negligible gradients.\n",
      "Skipping node 21 due to zero loss or negligible gradients.\n",
      "Skipping node 560 due to zero loss or negligible gradients.\n",
      "Skipping node 693 due to zero loss or negligible gradients.\n",
      "Skipping node 1354 due to zero loss or negligible gradients.\n",
      "Skipping node 1090 due to zero loss or negligible gradients.\n",
      "Skipping node 1454 due to zero loss or negligible gradients.\n",
      "Skipping node 1171 due to zero loss or negligible gradients.\n",
      "Skipping node 721 due to zero loss or negligible gradients.\n",
      "Skipping node 1370 due to zero loss or negligible gradients.\n",
      "Skipping node 418 due to zero loss or negligible gradients.\n",
      "Skipping node 1353 due to zero loss or negligible gradients.\n",
      "Skipping node 763 due to zero loss or negligible gradients.\n",
      "Skipping node 38 due to zero loss or negligible gradients.\n",
      "Skipping node 746 due to zero loss or negligible gradients.\n",
      "Skipping node 1144 due to zero loss or negligible gradients.\n",
      "Skipping node 938 due to zero loss or negligible gradients.\n",
      "Skipping node 276 due to zero loss or negligible gradients.\n",
      "Skipping node 828 due to zero loss or negligible gradients.\n",
      "Skipping node 1101 due to zero loss or negligible gradients.\n",
      "Skipping node 854 due to zero loss or negligible gradients.\n",
      "Skipping node 1212 due to zero loss or negligible gradients.\n",
      "Skipping node 991 due to zero loss or negligible gradients.\n",
      "Skipping node 725 due to zero loss or negligible gradients.\n",
      "Skipping node 30 due to zero loss or negligible gradients.\n",
      "Skipping node 1102 due to zero loss or negligible gradients.\n",
      "Skipping node 1167 due to zero loss or negligible gradients.\n",
      "Skipping node 673 due to zero loss or negligible gradients.\n",
      "Skipping node 613 due to zero loss or negligible gradients.\n",
      "Skipping node 343 due to zero loss or negligible gradients.\n",
      "Skipping node 1080 due to zero loss or negligible gradients.\n",
      "Skipping node 39 due to zero loss or negligible gradients.\n",
      "Skipping node 524 due to zero loss or negligible gradients.\n",
      "Skipping node 388 due to zero loss or negligible gradients.\n",
      "Skipping node 1017 due to zero loss or negligible gradients.\n",
      "Skipping node 549 due to zero loss or negligible gradients.\n",
      "Skipping node 1157 due to zero loss or negligible gradients.\n",
      "Skipping node 890 due to zero loss or negligible gradients.\n",
      "Skipping node 1321 due to zero loss or negligible gradients.\n",
      "Skipping node 683 due to zero loss or negligible gradients.\n",
      "Skipping node 117 due to zero loss or negligible gradients.\n",
      "Skipping node 274 due to zero loss or negligible gradients.\n",
      "Skipping node 1428 due to zero loss or negligible gradients.\n",
      "Skipping node 540 due to zero loss or negligible gradients.\n",
      "Skipping node 692 due to zero loss or negligible gradients.\n",
      "Skipping node 1044 due to zero loss or negligible gradients.\n",
      "Skipping node 294 due to zero loss or negligible gradients.\n",
      "Skipping node 480 due to zero loss or negligible gradients.\n",
      "Skipping node 247 due to zero loss or negligible gradients.\n",
      "Skipping node 402 due to zero loss or negligible gradients.\n",
      "Skipping node 874 due to zero loss or negligible gradients.\n",
      "Skipping node 608 due to zero loss or negligible gradients.\n",
      "Skipping node 1151 due to zero loss or negligible gradients.\n",
      "Skipping node 415 due to zero loss or negligible gradients.\n",
      "Skipping node 1266 due to zero loss or negligible gradients.\n",
      "Skipping node 210 due to zero loss or negligible gradients.\n",
      "Skipping node 359 due to zero loss or negligible gradients.\n",
      "Skipping node 1047 due to zero loss or negligible gradients.\n",
      "Skipping node 407 due to zero loss or negligible gradients.\n",
      "Skipping node 1112 due to zero loss or negligible gradients.\n",
      "Skipping node 1287 due to zero loss or negligible gradients.\n",
      "Skipping node 1329 due to zero loss or negligible gradients.\n",
      "Skipping node 964 due to zero loss or negligible gradients.\n",
      "Skipping node 640 due to zero loss or negligible gradients.\n",
      "Skipping node 1390 due to zero loss or negligible gradients.\n",
      "Skipping node 232 due to zero loss or negligible gradients.\n",
      "Skipping node 1029 due to zero loss or negligible gradients.\n",
      "Skipping node 1252 due to zero loss or negligible gradients.\n",
      "Skipping node 1200 due to zero loss or negligible gradients.\n",
      "Skipping node 409 due to zero loss or negligible gradients.\n",
      "Skipping node 1096 due to zero loss or negligible gradients.\n",
      "Skipping node 336 due to zero loss or negligible gradients.\n",
      "Skipping node 1027 due to zero loss or negligible gradients.\n",
      "Skipping node 396 due to zero loss or negligible gradients.\n",
      "Skipping node 1327 due to zero loss or negligible gradients.\n",
      "Skipping node 1441 due to zero loss or negligible gradients.\n",
      "Skipping node 562 due to zero loss or negligible gradients.\n",
      "Skipping node 188 due to zero loss or negligible gradients.\n",
      "Skipping node 444 due to zero loss or negligible gradients.\n",
      "Skipping node 1138 due to zero loss or negligible gradients.\n",
      "Skipping node 238 due to zero loss or negligible gradients.\n",
      "Skipping node 1043 due to zero loss or negligible gradients.\n",
      "Skipping node 537 due to zero loss or negligible gradients.\n",
      "Skipping node 783 due to zero loss or negligible gradients.\n",
      "Skipping node 995 due to zero loss or negligible gradients.\n",
      "Skipping node 1175 due to zero loss or negligible gradients.\n",
      "Skipping node 89 due to zero loss or negligible gradients.\n",
      "Skipping node 410 due to zero loss or negligible gradients.\n",
      "Skipping node 832 due to zero loss or negligible gradients.\n",
      "Skipping node 7 due to zero loss or negligible gradients.\n",
      "Skipping node 1466 due to zero loss or negligible gradients.\n",
      "Skipping node 573 due to zero loss or negligible gradients.\n",
      "Skipping node 439 due to zero loss or negligible gradients.\n",
      "Skipping node 83 due to zero loss or negligible gradients.\n",
      "Skipping node 863 due to zero loss or negligible gradients.\n",
      "Skipping node 451 due to zero loss or negligible gradients.\n",
      "Skipping node 370 due to zero loss or negligible gradients.\n",
      "Skipping node 757 due to zero loss or negligible gradients.\n",
      "Skipping node 801 due to zero loss or negligible gradients.\n",
      "Skipping node 645 due to zero loss or negligible gradients.\n",
      "Skipping node 357 due to zero loss or negligible gradients.\n",
      "Skipping node 65 due to zero loss or negligible gradients.\n",
      "Skipping node 1429 due to zero loss or negligible gradients.\n",
      "Skipping node 924 due to zero loss or negligible gradients.\n",
      "Skipping node 1302 due to zero loss or negligible gradients.\n",
      "Skipping node 647 due to zero loss or negligible gradients.\n",
      "Skipping node 913 due to zero loss or negligible gradients.\n",
      "Skipping node 807 due to zero loss or negligible gradients.\n",
      "Skipping node 1002 due to zero loss or negligible gradients.\n",
      "Skipping node 360 due to zero loss or negligible gradients.\n",
      "Skipping node 993 due to zero loss or negligible gradients.\n",
      "Skipping node 1204 due to zero loss or negligible gradients.\n",
      "Skipping node 731 due to zero loss or negligible gradients.\n",
      "Skipping node 157 due to zero loss or negligible gradients.\n",
      "Skipping node 648 due to zero loss or negligible gradients.\n",
      "Skipping node 1007 due to zero loss or negligible gradients.\n",
      "Skipping node 577 due to zero loss or negligible gradients.\n",
      "Skipping node 364 due to zero loss or negligible gradients.\n",
      "Skipping node 1129 due to zero loss or negligible gradients.\n",
      "Skipping node 1143 due to zero loss or negligible gradients.\n",
      "Skipping node 1391 due to zero loss or negligible gradients.\n",
      "Skipping node 824 due to zero loss or negligible gradients.\n",
      "Skipping node 541 due to zero loss or negligible gradients.\n",
      "Skipping node 634 due to zero loss or negligible gradients.\n",
      "Skipping node 777 due to zero loss or negligible gradients.\n",
      "Skipping node 646 due to zero loss or negligible gradients.\n",
      "Skipping node 816 due to zero loss or negligible gradients.\n",
      "Skipping node 554 due to zero loss or negligible gradients.\n",
      "Skipping node 1437 due to zero loss or negligible gradients.\n",
      "Skipping node 1164 due to zero loss or negligible gradients.\n",
      "Skipping node 1289 due to zero loss or negligible gradients.\n",
      "Skipping node 559 due to zero loss or negligible gradients.\n",
      "Skipping node 237 due to zero loss or negligible gradients.\n",
      "Skipping node 106 due to zero loss or negligible gradients.\n",
      "Skipping node 1389 due to zero loss or negligible gradients.\n",
      "Skipping node 566 due to zero loss or negligible gradients.\n",
      "Skipping node 955 due to zero loss or negligible gradients.\n",
      "Skipping node 377 due to zero loss or negligible gradients.\n",
      "Skipping node 776 due to zero loss or negligible gradients.\n",
      "Skipping node 170 due to zero loss or negligible gradients.\n",
      "Skipping node 1235 due to zero loss or negligible gradients.\n",
      "Skipping node 1380 due to zero loss or negligible gradients.\n",
      "Skipping node 74 due to zero loss or negligible gradients.\n",
      "Skipping node 690 due to zero loss or negligible gradients.\n",
      "Skipping node 126 due to zero loss or negligible gradients.\n",
      "Skipping node 1386 due to zero loss or negligible gradients.\n",
      "Skipping node 216 due to zero loss or negligible gradients.\n",
      "Skipping node 373 due to zero loss or negligible gradients.\n",
      "Skipping node 944 due to zero loss or negligible gradients.\n",
      "Skipping node 95 due to zero loss or negligible gradients.\n",
      "Skipping node 675 due to zero loss or negligible gradients.\n",
      "Skipping node 574 due to zero loss or negligible gradients.\n",
      "Skipping node 445 due to zero loss or negligible gradients.\n",
      "Skipping node 14 due to zero loss or negligible gradients.\n",
      "Skipping node 1059 due to zero loss or negligible gradients.\n",
      "Skipping node 792 due to zero loss or negligible gradients.\n",
      "Skipping node 1472 due to zero loss or negligible gradients.\n",
      "Skipping node 1023 due to zero loss or negligible gradients.\n",
      "Skipping node 266 due to zero loss or negligible gradients.\n",
      "Skipping node 1347 due to zero loss or negligible gradients.\n",
      "Skipping node 578 due to zero loss or negligible gradients.\n",
      "Skipping node 1324 due to zero loss or negligible gradients.\n",
      "Skipping node 679 due to zero loss or negligible gradients.\n",
      "Skipping node 760 due to zero loss or negligible gradients.\n",
      "Skipping node 884 due to zero loss or negligible gradients.\n",
      "Skipping node 711 due to zero loss or negligible gradients.\n",
      "Skipping node 936 due to zero loss or negligible gradients.\n",
      "Skipping node 350 due to zero loss or negligible gradients.\n",
      "Skipping node 543 due to zero loss or negligible gradients.\n",
      "Skipping node 861 due to zero loss or negligible gradients.\n",
      "Skipping node 1314 due to zero loss or negligible gradients.\n",
      "Skipping node 1424 due to zero loss or negligible gradients.\n",
      "Skipping node 252 due to zero loss or negligible gradients.\n",
      "Skipping node 45 due to zero loss or negligible gradients.\n",
      "Skipping node 1103 due to zero loss or negligible gradients.\n",
      "Skipping node 470 due to zero loss or negligible gradients.\n",
      "Skipping node 1125 due to zero loss or negligible gradients.\n",
      "Skipping node 1216 due to zero loss or negligible gradients.\n",
      "Skipping node 1286 due to zero loss or negligible gradients.\n",
      "Skipping node 815 due to zero loss or negligible gradients.\n",
      "Skipping node 1383 due to zero loss or negligible gradients.\n",
      "Skipping node 770 due to zero loss or negligible gradients.\n",
      "Skipping node 66 due to zero loss or negligible gradients.\n",
      "Skipping node 272 due to zero loss or negligible gradients.\n",
      "Skipping node 1397 due to zero loss or negligible gradients.\n",
      "Skipping node 1269 due to zero loss or negligible gradients.\n",
      "Skipping node 420 due to zero loss or negligible gradients.\n",
      "Skipping node 1121 due to zero loss or negligible gradients.\n",
      "Skipping node 36 due to zero loss or negligible gradients.\n",
      "Skipping node 892 due to zero loss or negligible gradients.\n",
      "Skipping node 907 due to zero loss or negligible gradients.\n",
      "Skipping node 8 due to zero loss or negligible gradients.\n",
      "Skipping node 1281 due to zero loss or negligible gradients.\n",
      "Skipping node 896 due to zero loss or negligible gradients.\n",
      "Skipping node 77 due to zero loss or negligible gradients.\n",
      "Skipping node 1315 due to zero loss or negligible gradients.\n",
      "Skipping node 535 due to zero loss or negligible gradients.\n",
      "Skipping node 929 due to zero loss or negligible gradients.\n",
      "Skipping node 1478 due to zero loss or negligible gradients.\n",
      "Skipping node 954 due to zero loss or negligible gradients.\n",
      "Skipping node 191 due to zero loss or negligible gradients.\n",
      "Skipping node 867 due to zero loss or negligible gradients.\n",
      "Skipping node 1480 due to zero loss or negligible gradients.\n",
      "Skipping node 245 due to zero loss or negligible gradients.\n",
      "Skipping node 1414 due to zero loss or negligible gradients.\n",
      "Skipping node 686 due to zero loss or negligible gradients.\n",
      "Skipping node 827 due to zero loss or negligible gradients.\n",
      "Skipping node 1105 due to zero loss or negligible gradients.\n",
      "Skipping node 781 due to zero loss or negligible gradients.\n",
      "Skipping node 442 due to zero loss or negligible gradients.\n",
      "Skipping node 465 due to zero loss or negligible gradients.\n",
      "Skipping node 149 due to zero loss or negligible gradients.\n",
      "Skipping node 703 due to zero loss or negligible gradients.\n",
      "Skipping node 139 due to zero loss or negligible gradients.\n",
      "Skipping node 466 due to zero loss or negligible gradients.\n",
      "Skipping node 740 due to zero loss or negligible gradients.\n",
      "Skipping node 199 due to zero loss or negligible gradients.\n",
      "Skipping node 297 due to zero loss or negligible gradients.\n",
      "Skipping node 1203 due to zero loss or negligible gradients.\n",
      "Skipping node 1039 due to zero loss or negligible gradients.\n",
      "Skipping node 180 due to zero loss or negligible gradients.\n",
      "Skipping node 124 due to zero loss or negligible gradients.\n",
      "Skipping node 1113 due to zero loss or negligible gradients.\n",
      "Skipping node 977 due to zero loss or negligible gradients.\n",
      "Skipping node 1170 due to zero loss or negligible gradients.\n",
      "Skipping node 1091 due to zero loss or negligible gradients.\n",
      "Skipping node 1063 due to zero loss or negligible gradients.\n",
      "Skipping node 555 due to zero loss or negligible gradients.\n",
      "Skipping node 1470 due to zero loss or negligible gradients.\n",
      "Skipping node 1318 due to zero loss or negligible gradients.\n",
      "Skipping node 818 due to zero loss or negligible gradients.\n",
      "Skipping node 176 due to zero loss or negligible gradients.\n",
      "Skipping node 1273 due to zero loss or negligible gradients.\n",
      "Skipping node 649 due to zero loss or negligible gradients.\n",
      "Skipping node 618 due to zero loss or negligible gradients.\n",
      "Skipping node 156 due to zero loss or negligible gradients.\n",
      "Skipping node 738 due to zero loss or negligible gradients.\n",
      "Skipping node 1071 due to zero loss or negligible gradients.\n",
      "Skipping node 1249 due to zero loss or negligible gradients.\n",
      "Skipping node 1078 due to zero loss or negligible gradients.\n",
      "Skipping node 1025 due to zero loss or negligible gradients.\n",
      "Skipping node 1085 due to zero loss or negligible gradients.\n",
      "Skipping node 1333 due to zero loss or negligible gradients.\n",
      "Skipping node 1062 due to zero loss or negligible gradients.\n",
      "Skipping node 236 due to zero loss or negligible gradients.\n",
      "Skipping node 1028 due to zero loss or negligible gradients.\n",
      "Skipping node 506 due to zero loss or negligible gradients.\n",
      "Skipping node 829 due to zero loss or negligible gradients.\n",
      "Skipping node 682 due to zero loss or negligible gradients.\n",
      "Skipping node 400 due to zero loss or negligible gradients.\n",
      "Skipping node 1176 due to zero loss or negligible gradients.\n",
      "Skipping node 810 due to zero loss or negligible gradients.\n",
      "Skipping node 1012 due to zero loss or negligible gradients.\n",
      "Skipping node 185 due to zero loss or negligible gradients.\n",
      "Skipping node 452 due to zero loss or negligible gradients.\n",
      "Skipping node 17 due to zero loss or negligible gradients.\n",
      "Skipping node 1237 due to zero loss or negligible gradients.\n",
      "Skipping node 848 due to zero loss or negligible gradients.\n",
      "Skipping node 76 due to zero loss or negligible gradients.\n",
      "Skipping node 482 due to zero loss or negligible gradients.\n",
      "Skipping node 1095 due to zero loss or negligible gradients.\n",
      "Skipping node 800 due to zero loss or negligible gradients.\n",
      "Skipping node 641 due to zero loss or negligible gradients.\n",
      "Skipping node 204 due to zero loss or negligible gradients.\n",
      "Skipping node 716 due to zero loss or negligible gradients.\n",
      "Skipping node 976 due to zero loss or negligible gradients.\n",
      "Skipping node 761 due to zero loss or negligible gradients.\n",
      "Skipping node 423 due to zero loss or negligible gradients.\n",
      "Skipping node 644 due to zero loss or negligible gradients.\n",
      "Skipping node 479 due to zero loss or negligible gradients.\n",
      "Skipping node 910 due to zero loss or negligible gradients.\n",
      "Skipping node 698 due to zero loss or negligible gradients.\n",
      "Skipping node 700 due to zero loss or negligible gradients.\n",
      "Skipping node 179 due to zero loss or negligible gradients.\n",
      "Skipping node 1349 due to zero loss or negligible gradients.\n",
      "Skipping node 663 due to zero loss or negligible gradients.\n",
      "Skipping node 870 due to zero loss or negligible gradients.\n",
      "Skipping node 656 due to zero loss or negligible gradients.\n",
      "Skipping node 702 due to zero loss or negligible gradients.\n",
      "Skipping node 328 due to zero loss or negligible gradients.\n",
      "Skipping node 306 due to zero loss or negligible gradients.\n",
      "Skipping node 802 due to zero loss or negligible gradients.\n",
      "Skipping node 243 due to zero loss or negligible gradients.\n",
      "Skipping node 1257 due to zero loss or negligible gradients.\n",
      "Skipping node 50 due to zero loss or negligible gradients.\n",
      "Skipping node 1013 due to zero loss or negligible gradients.\n",
      "Skipping node 486 due to zero loss or negligible gradients.\n",
      "Skipping node 940 due to zero loss or negligible gradients.\n",
      "Skipping node 1411 due to zero loss or negligible gradients.\n",
      "Skipping node 334 due to zero loss or negligible gradients.\n",
      "Skipping node 515 due to zero loss or negligible gradients.\n",
      "Skipping node 1483 due to zero loss or negligible gradients.\n",
      "Skipping node 583 due to zero loss or negligible gradients.\n",
      "Skipping node 1278 due to zero loss or negligible gradients.\n",
      "Skipping node 1210 due to zero loss or negligible gradients.\n",
      "Skipping node 1352 due to zero loss or negligible gradients.\n",
      "Skipping node 962 due to zero loss or negligible gradients.\n",
      "Skipping node 780 due to zero loss or negligible gradients.\n",
      "Skipping node 717 due to zero loss or negligible gradients.\n",
      "Skipping node 171 due to zero loss or negligible gradients.\n",
      "Skipping node 1335 due to zero loss or negligible gradients.\n",
      "Skipping node 552 due to zero loss or negligible gradients.\n",
      "Skipping node 438 due to zero loss or negligible gradients.\n",
      "Skipping node 755 due to zero loss or negligible gradients.\n",
      "Skipping node 889 due to zero loss or negligible gradients.\n",
      "Skipping node 143 due to zero loss or negligible gradients.\n",
      "Skipping node 1426 due to zero loss or negligible gradients.\n",
      "Skipping node 941 due to zero loss or negligible gradients.\n",
      "Skipping node 203 due to zero loss or negligible gradients.\n",
      "Skipping node 572 due to zero loss or negligible gradients.\n",
      "Skipping node 582 due to zero loss or negligible gradients.\n",
      "Skipping node 1250 due to zero loss or negligible gradients.\n",
      "Skipping node 875 due to zero loss or negligible gradients.\n",
      "Skipping node 1344 due to zero loss or negligible gradients.\n",
      "Skipping node 491 due to zero loss or negligible gradients.\n",
      "Skipping node 1195 due to zero loss or negligible gradients.\n",
      "Skipping node 1413 due to zero loss or negligible gradients.\n",
      "Skipping node 983 due to zero loss or negligible gradients.\n",
      "Skipping node 689 due to zero loss or negligible gradients.\n",
      "Skipping node 1134 due to zero loss or negligible gradients.\n",
      "Skipping node 753 due to zero loss or negligible gradients.\n",
      "Skipping node 208 due to zero loss or negligible gradients.\n",
      "Skipping node 159 due to zero loss or negligible gradients.\n",
      "Skipping node 669 due to zero loss or negligible gradients.\n",
      "Skipping node 1168 due to zero loss or negligible gradients.\n",
      "Skipping node 1331 due to zero loss or negligible gradients.\n",
      "Skipping node 278 due to zero loss or negligible gradients.\n",
      "Skipping node 1199 due to zero loss or negligible gradients.\n",
      "Skipping node 937 due to zero loss or negligible gradients.\n",
      "Skipping node 897 due to zero loss or negligible gradients.\n",
      "Skipping node 627 due to zero loss or negligible gradients.\n",
      "Skipping node 1431 due to zero loss or negligible gradients.\n",
      "Skipping node 1457 due to zero loss or negligible gradients.\n",
      "Skipping node 850 due to zero loss or negligible gradients.\n",
      "Skipping node 453 due to zero loss or negligible gradients.\n",
      "Skipping node 1297 due to zero loss or negligible gradients.\n",
      "Skipping node 226 due to zero loss or negligible gradients.\n",
      "Skipping node 1037 due to zero loss or negligible gradients.\n",
      "Skipping node 1064 due to zero loss or negligible gradients.\n",
      "Skipping node 1218 due to zero loss or negligible gradients.\n",
      "Skipping node 1319 due to zero loss or negligible gradients.\n",
      "Skipping node 1351 due to zero loss or negligible gradients.\n",
      "Skipping node 408 due to zero loss or negligible gradients.\n",
      "Skipping node 1086 due to zero loss or negligible gradients.\n",
      "Skipping node 688 due to zero loss or negligible gradients.\n",
      "Skipping node 299 due to zero loss or negligible gradients.\n",
      "Skipping node 426 due to zero loss or negligible gradients.\n",
      "Skipping node 102 due to zero loss or negligible gradients.\n",
      "Skipping node 630 due to zero loss or negligible gradients.\n",
      "Skipping node 116 due to zero loss or negligible gradients.\n",
      "Skipping node 561 due to zero loss or negligible gradients.\n",
      "Skipping node 282 due to zero loss or negligible gradients.\n",
      "Skipping node 1473 due to zero loss or negligible gradients.\n",
      "Skipping node 1075 due to zero loss or negligible gradients.\n",
      "Skipping node 766 due to zero loss or negligible gradients.\n",
      "Skipping node 478 due to zero loss or negligible gradients.\n",
      "Skipping node 1099 due to zero loss or negligible gradients.\n",
      "Skipping node 900 due to zero loss or negligible gradients.\n",
      "Skipping node 467 due to zero loss or negligible gradients.\n",
      "Skipping node 1003 due to zero loss or negligible gradients.\n",
      "Skipping node 1018 due to zero loss or negligible gradients.\n",
      "Skipping node 858 due to zero loss or negligible gradients.\n",
      "Skipping node 488 due to zero loss or negligible gradients.\n",
      "Skipping node 403 due to zero loss or negligible gradients.\n",
      "Skipping node 389 due to zero loss or negligible gradients.\n",
      "Skipping node 684 due to zero loss or negligible gradients.\n",
      "Skipping node 654 due to zero loss or negligible gradients.\n",
      "Skipping node 435 due to zero loss or negligible gradients.\n",
      "Skipping node 765 due to zero loss or negligible gradients.\n",
      "Skipping node 154 due to zero loss or negligible gradients.\n",
      "Skipping node 52 due to zero loss or negligible gradients.\n",
      "Skipping node 268 due to zero loss or negligible gradients.\n",
      "Skipping node 569 due to zero loss or negligible gradients.\n",
      "Skipping node 772 due to zero loss or negligible gradients.\n",
      "Skipping node 23 due to zero loss or negligible gradients.\n",
      "Skipping node 1446 due to zero loss or negligible gradients.\n",
      "Skipping node 1056 due to zero loss or negligible gradients.\n",
      "Skipping node 404 due to zero loss or negligible gradients.\n",
      "Skipping node 136 due to zero loss or negligible gradients.\n",
      "Skipping node 19 due to zero loss or negligible gradients.\n",
      "Skipping node 853 due to zero loss or negligible gradients.\n",
      "Skipping node 304 due to zero loss or negligible gradients.\n",
      "Skipping node 308 due to zero loss or negligible gradients.\n",
      "Skipping node 1243 due to zero loss or negligible gradients.\n",
      "Skipping node 1439 due to zero loss or negligible gradients.\n",
      "Skipping node 346 due to zero loss or negligible gradients.\n",
      "Skipping node 320 due to zero loss or negligible gradients.\n",
      "Skipping node 959 due to zero loss or negligible gradients.\n",
      "Skipping node 398 due to zero loss or negligible gradients.\n",
      "Skipping node 1394 due to zero loss or negligible gradients.\n",
      "Skipping node 421 due to zero loss or negligible gradients.\n",
      "Skipping node 1392 due to zero loss or negligible gradients.\n",
      "Skipping node 602 due to zero loss or negligible gradients.\n",
      "Skipping node 734 due to zero loss or negligible gradients.\n",
      "Skipping node 43 due to zero loss or negligible gradients.\n",
      "Skipping node 951 due to zero loss or negligible gradients.\n",
      "Skipping node 1285 due to zero loss or negligible gradients.\n",
      "Skipping node 510 due to zero loss or negligible gradients.\n",
      "Skipping node 366 due to zero loss or negligible gradients.\n",
      "Skipping node 131 due to zero loss or negligible gradients.\n",
      "Skipping node 174 due to zero loss or negligible gradients.\n",
      "Skipping node 911 due to zero loss or negligible gradients.\n",
      "Skipping node 1066 due to zero loss or negligible gradients.\n",
      "Skipping node 84 due to zero loss or negligible gradients.\n",
      "Skipping node 869 due to zero loss or negligible gradients.\n",
      "Skipping node 267 due to zero loss or negligible gradients.\n",
      "Skipping node 1182 due to zero loss or negligible gradients.\n",
      "Skipping node 1340 due to zero loss or negligible gradients.\n",
      "Skipping node 1022 due to zero loss or negligible gradients.\n",
      "Skipping node 13 due to zero loss or negligible gradients.\n",
      "Skipping node 517 due to zero loss or negligible gradients.\n",
      "Skipping node 624 due to zero loss or negligible gradients.\n",
      "Skipping node 982 due to zero loss or negligible gradients.\n",
      "Skipping node 1290 due to zero loss or negligible gradients.\n",
      "Skipping node 934 due to zero loss or negligible gradients.\n",
      "Skipping node 113 due to zero loss or negligible gradients.\n",
      "Skipping node 1332 due to zero loss or negligible gradients.\n",
      "Skipping node 1448 due to zero loss or negligible gradients.\n",
      "Skipping node 899 due to zero loss or negligible gradients.\n",
      "Skipping node 148 due to zero loss or negligible gradients.\n",
      "Skipping node 520 due to zero loss or negligible gradients.\n",
      "Skipping node 668 due to zero loss or negligible gradients.\n",
      "Skipping node 1425 due to zero loss or negligible gradients.\n",
      "Skipping node 128 due to zero loss or negligible gradients.\n",
      "Skipping node 1233 due to zero loss or negligible gradients.\n",
      "Skipping node 300 due to zero loss or negligible gradients.\n",
      "Skipping node 1079 due to zero loss or negligible gradients.\n",
      "Skipping node 830 due to zero loss or negligible gradients.\n",
      "Skipping node 1384 due to zero loss or negligible gradients.\n",
      "Skipping node 791 due to zero loss or negligible gradients.\n",
      "Skipping node 217 due to zero loss or negligible gradients.\n",
      "Skipping node 34 due to zero loss or negligible gradients.\n",
      "Skipping node 1360 due to zero loss or negligible gradients.\n",
      "Skipping node 1033 due to zero loss or negligible gradients.\n",
      "Skipping node 1440 due to zero loss or negligible gradients.\n",
      "Skipping node 178 due to zero loss or negligible gradients.\n",
      "Skipping node 42 due to zero loss or negligible gradients.\n",
      "Skipping node 513 due to zero loss or negligible gradients.\n",
      "Skipping node 1153 due to zero loss or negligible gradients.\n",
      "Skipping node 699 due to zero loss or negligible gradients.\n",
      "Skipping node 619 due to zero loss or negligible gradients.\n",
      "Skipping node 895 due to zero loss or negligible gradients.\n",
      "Skipping node 1435 due to zero loss or negligible gradients.\n",
      "Skipping node 464 due to zero loss or negligible gradients.\n",
      "Skipping node 558 due to zero loss or negligible gradients.\n",
      "Skipping node 1338 due to zero loss or negligible gradients.\n",
      "Skipping node 739 due to zero loss or negligible gradients.\n",
      "Skipping node 1417 due to zero loss or negligible gradients.\n",
      "Skipping node 912 due to zero loss or negligible gradients.\n",
      "Skipping node 387 due to zero loss or negligible gradients.\n",
      "Skipping node 41 due to zero loss or negligible gradients.\n",
      "Skipping node 1468 due to zero loss or negligible gradients.\n",
      "Skipping node 28 due to zero loss or negligible gradients.\n",
      "Skipping node 643 due to zero loss or negligible gradients.\n",
      "Skipping node 1444 due to zero loss or negligible gradients.\n",
      "Skipping node 833 due to zero loss or negligible gradients.\n",
      "Skipping node 1419 due to zero loss or negligible gradients.\n",
      "Skipping node 599 due to zero loss or negligible gradients.\n",
      "Skipping node 381 due to zero loss or negligible gradients.\n",
      "Skipping node 611 due to zero loss or negligible gradients.\n",
      "Skipping node 187 due to zero loss or negligible gradients.\n",
      "Skipping node 1280 due to zero loss or negligible gradients.\n",
      "Skipping node 1477 due to zero loss or negligible gradients.\n",
      "Skipping node 1404 due to zero loss or negligible gradients.\n",
      "Skipping node 112 due to zero loss or negligible gradients.\n",
      "Skipping node 814 due to zero loss or negligible gradients.\n",
      "Skipping node 1253 due to zero loss or negligible gradients.\n",
      "Skipping node 256 due to zero loss or negligible gradients.\n",
      "Skipping node 942 due to zero loss or negligible gradients.\n",
      "Skipping node 1221 due to zero loss or negligible gradients.\n",
      "Skipping node 1040 due to zero loss or negligible gradients.\n",
      "Skipping node 965 due to zero loss or negligible gradients.\n",
      "Skipping node 762 due to zero loss or negligible gradients.\n",
      "Skipping node 526 due to zero loss or negligible gradients.\n",
      "Skipping node 115 due to zero loss or negligible gradients.\n",
      "Skipping node 1469 due to zero loss or negligible gradients.\n",
      "Skipping node 1225 due to zero loss or negligible gradients.\n",
      "Skipping node 1133 due to zero loss or negligible gradients.\n",
      "Skipping node 920 due to zero loss or negligible gradients.\n",
      "Skipping node 588 due to zero loss or negligible gradients.\n",
      "Skipping node 1361 due to zero loss or negligible gradients.\n",
      "Skipping node 620 due to zero loss or negligible gradients.\n",
      "Skipping node 877 due to zero loss or negligible gradients.\n",
      "Skipping node 68 due to zero loss or negligible gradients.\n",
      "Skipping node 958 due to zero loss or negligible gradients.\n",
      "Skipping node 786 due to zero loss or negligible gradients.\n",
      "Skipping node 823 due to zero loss or negligible gradients.\n",
      "Skipping node 1166 due to zero loss or negligible gradients.\n",
      "Skipping node 601 due to zero loss or negligible gradients.\n",
      "Skipping node 1367 due to zero loss or negligible gradients.\n",
      "Skipping node 1179 due to zero loss or negligible gradients.\n",
      "Skipping node 1270 due to zero loss or negligible gradients.\n",
      "Skipping node 931 due to zero loss or negligible gradients.\n",
      "Skipping node 1295 due to zero loss or negligible gradients.\n",
      "Skipping node 1467 due to zero loss or negligible gradients.\n",
      "Skipping node 1004 due to zero loss or negligible gradients.\n",
      "Skipping node 1481 due to zero loss or negligible gradients.\n",
      "Skipping node 11 due to zero loss or negligible gradients.\n",
      "Skipping node 548 due to zero loss or negligible gradients.\n",
      "Skipping node 1197 due to zero loss or negligible gradients.\n",
      "Skipping node 223 due to zero loss or negligible gradients.\n",
      "Skipping node 206 due to zero loss or negligible gradients.\n",
      "Skipping node 391 due to zero loss or negligible gradients.\n",
      "Skipping node 787 due to zero loss or negligible gradients.\n",
      "Skipping node 1145 due to zero loss or negligible gradients.\n",
      "Skipping node 81 due to zero loss or negligible gradients.\n",
      "Skipping node 658 due to zero loss or negligible gradients.\n",
      "Skipping node 512 due to zero loss or negligible gradients.\n",
      "Skipping node 871 due to zero loss or negligible gradients.\n",
      "Skipping node 1400 due to zero loss or negligible gradients.\n",
      "Skipping node 748 due to zero loss or negligible gradients.\n",
      "Skipping node 978 due to zero loss or negligible gradients.\n",
      "Skipping node 1084 due to zero loss or negligible gradients.\n",
      "Skipping node 1316 due to zero loss or negligible gradients.\n",
      "Skipping node 82 due to zero loss or negligible gradients.\n",
      "Skipping node 64 due to zero loss or negligible gradients.\n",
      "Skipping node 542 due to zero loss or negligible gradients.\n",
      "Skipping node 177 due to zero loss or negligible gradients.\n",
      "Skipping node 553 due to zero loss or negligible gradients.\n",
      "Skipping node 197 due to zero loss or negligible gradients.\n",
      "Skipping node 908 due to zero loss or negligible gradients.\n",
      "Skipping node 448 due to zero loss or negligible gradients.\n",
      "Skipping node 401 due to zero loss or negligible gradients.\n",
      "Skipping node 72 due to zero loss or negligible gradients.\n",
      "Skipping node 1337 due to zero loss or negligible gradients.\n",
      "Skipping node 633 due to zero loss or negligible gradients.\n",
      "Skipping node 1123 due to zero loss or negligible gradients.\n",
      "Skipping node 1254 due to zero loss or negligible gradients.\n",
      "Skipping node 687 due to zero loss or negligible gradients.\n",
      "Skipping node 290 due to zero loss or negligible gradients.\n",
      "Skipping node 1074 due to zero loss or negligible gradients.\n",
      "Skipping node 1239 due to zero loss or negligible gradients.\n",
      "Skipping node 737 due to zero loss or negligible gradients.\n",
      "Skipping node 1366 due to zero loss or negligible gradients.\n",
      "Skipping node 1376 due to zero loss or negligible gradients.\n",
      "Skipping node 1261 due to zero loss or negligible gradients.\n",
      "Skipping node 504 due to zero loss or negligible gradients.\n",
      "Skipping node 1308 due to zero loss or negligible gradients.\n",
      "Skipping node 1193 due to zero loss or negligible gradients.\n",
      "Skipping node 676 due to zero loss or negligible gradients.\n",
      "Skipping node 273 due to zero loss or negligible gradients.\n",
      "Skipping node 846 due to zero loss or negligible gradients.\n",
      "Skipping node 211 due to zero loss or negligible gradients.\n",
      "Skipping node 621 due to zero loss or negligible gradients.\n",
      "Skipping node 244 due to zero loss or negligible gradients.\n",
      "Skipping node 1301 due to zero loss or negligible gradients.\n",
      "Skipping node 437 due to zero loss or negligible gradients.\n",
      "Skipping node 383 due to zero loss or negligible gradients.\n",
      "Skipping node 507 due to zero loss or negligible gradients.\n",
      "Skipping node 494 due to zero loss or negligible gradients.\n",
      "Skipping node 1445 due to zero loss or negligible gradients.\n",
      "Skipping node 1292 due to zero loss or negligible gradients.\n",
      "Skipping node 879 due to zero loss or negligible gradients.\n",
      "Skipping node 1398 due to zero loss or negligible gradients.\n",
      "Skipping node 1219 due to zero loss or negligible gradients.\n",
      "Skipping node 169 due to zero loss or negligible gradients.\n",
      "Skipping node 380 due to zero loss or negligible gradients.\n",
      "Skipping node 950 due to zero loss or negligible gradients.\n",
      "Skipping node 427 due to zero loss or negligible gradients.\n",
      "Skipping node 1148 due to zero loss or negligible gradients.\n",
      "Skipping node 330 due to zero loss or negligible gradients.\n",
      "Skipping node 891 due to zero loss or negligible gradients.\n",
      "Skipping node 809 due to zero loss or negligible gradients.\n",
      "Skipping node 1211 due to zero loss or negligible gradients.\n",
      "Skipping node 728 due to zero loss or negligible gradients.\n",
      "Skipping node 925 due to zero loss or negligible gradients.\n",
      "Skipping node 1126 due to zero loss or negligible gradients.\n",
      "Skipping node 764 due to zero loss or negligible gradients.\n",
      "Skipping node 317 due to zero loss or negligible gradients.\n",
      "Skipping node 141 due to zero loss or negligible gradients.\n",
      "Skipping node 1156 due to zero loss or negligible gradients.\n",
      "Skipping node 567 due to zero loss or negligible gradients.\n",
      "Skipping node 22 due to zero loss or negligible gradients.\n",
      "Skipping node 1208 due to zero loss or negligible gradients.\n",
      "Skipping node 167 due to zero loss or negligible gradients.\n",
      "Skipping node 1433 due to zero loss or negligible gradients.\n",
      "Skipping node 1355 due to zero loss or negligible gradients.\n",
      "Skipping node 612 due to zero loss or negligible gradients.\n",
      "Skipping node 248 due to zero loss or negligible gradients.\n",
      "Skipping node 33 due to zero loss or negligible gradients.\n",
      "Skipping node 1264 due to zero loss or negligible gradients.\n",
      "Skipping node 1226 due to zero loss or negligible gradients.\n",
      "Skipping node 935 due to zero loss or negligible gradients.\n",
      "Skipping node 1020 due to zero loss or negligible gradients.\n",
      "Skipping node 390 due to zero loss or negligible gradients.\n",
      "Skipping node 918 due to zero loss or negligible gradients.\n",
      "Skipping node 821 due to zero loss or negligible gradients.\n",
      "Skipping node 44 due to zero loss or negligible gradients.\n",
      "Skipping node 393 due to zero loss or negligible gradients.\n",
      "Skipping node 99 due to zero loss or negligible gradients.\n",
      "Skipping node 893 due to zero loss or negligible gradients.\n",
      "Skipping node 1487 due to zero loss or negligible gradients.\n",
      "Skipping node 1198 due to zero loss or negligible gradients.\n",
      "Skipping node 997 due to zero loss or negligible gradients.\n",
      "Skipping node 533 due to zero loss or negligible gradients.\n",
      "Skipping node 708 due to zero loss or negligible gradients.\n",
      "Skipping node 1111 due to zero loss or negligible gradients.\n",
      "Skipping node 1410 due to zero loss or negligible gradients.\n",
      "Skipping node 1147 due to zero loss or negligible gradients.\n",
      "Skipping node 29 due to zero loss or negligible gradients.\n",
      "Skipping node 796 due to zero loss or negligible gradients.\n",
      "Skipping node 16 due to zero loss or negligible gradients.\n",
      "Skipping node 849 due to zero loss or negligible gradients.\n",
      "Skipping node 949 due to zero loss or negligible gradients.\n",
      "Skipping node 1092 due to zero loss or negligible gradients.\n",
      "Skipping node 1416 due to zero loss or negligible gradients.\n",
      "Skipping node 1350 due to zero loss or negligible gradients.\n",
      "Skipping node 785 due to zero loss or negligible gradients.\n",
      "Skipping node 104 due to zero loss or negligible gradients.\n",
      "Skipping node 1124 due to zero loss or negligible gradients.\n",
      "Skipping node 150 due to zero loss or negligible gradients.\n",
      "Skipping node 295 due to zero loss or negligible gradients.\n",
      "Skipping node 1312 due to zero loss or negligible gradients.\n",
      "Skipping node 1190 due to zero loss or negligible gradients.\n",
      "Skipping node 756 due to zero loss or negligible gradients.\n",
      "Skipping node 94 due to zero loss or negligible gradients.\n",
      "Skipping node 1378 due to zero loss or negligible gradients.\n",
      "Skipping node 1209 due to zero loss or negligible gradients.\n",
      "Skipping node 120 due to zero loss or negligible gradients.\n",
      "Skipping node 1083 due to zero loss or negligible gradients.\n",
      "Skipping node 1434 due to zero loss or negligible gradients.\n",
      "Skipping node 1048 due to zero loss or negligible gradients.\n",
      "Skipping node 4 due to zero loss or negligible gradients.\n",
      "Skipping node 1262 due to zero loss or negligible gradients.\n",
      "Skipping node 712 due to zero loss or negligible gradients.\n",
      "Skipping node 1186 due to zero loss or negligible gradients.\n",
      "Skipping node 367 due to zero loss or negligible gradients.\n",
      "Skipping node 1051 due to zero loss or negligible gradients.\n",
      "Skipping node 58 due to zero loss or negligible gradients.\n",
      "Skipping node 1202 due to zero loss or negligible gradients.\n",
      "Skipping node 1087 due to zero loss or negligible gradients.\n",
      "Skipping node 87 due to zero loss or negligible gradients.\n",
      "Skipping node 395 due to zero loss or negligible gradients.\n",
      "Skipping node 31 due to zero loss or negligible gradients.\n",
      "Skipping node 930 due to zero loss or negligible gradients.\n",
      "Skipping node 974 due to zero loss or negligible gradients.\n",
      "Skipping node 277 due to zero loss or negligible gradients.\n",
      "Skipping node 71 due to zero loss or negligible gradients.\n",
      "Skipping node 344 due to zero loss or negligible gradients.\n",
      "Skipping node 55 due to zero loss or negligible gradients.\n",
      "Skipping node 378 due to zero loss or negligible gradients.\n",
      "Skipping node 417 due to zero loss or negligible gradients.\n",
      "Skipping node 221 due to zero loss or negligible gradients.\n",
      "Skipping node 1241 due to zero loss or negligible gradients.\n",
      "Skipping node 775 due to zero loss or negligible gradients.\n",
      "Skipping node 704 due to zero loss or negligible gradients.\n",
      "Skipping node 322 due to zero loss or negligible gradients.\n",
      "Skipping node 550 due to zero loss or negligible gradients.\n",
      "Skipping node 492 due to zero loss or negligible gradients.\n",
      "Skipping node 1009 due to zero loss or negligible gradients.\n",
      "Skipping node 292 due to zero loss or negligible gradients.\n",
      "Skipping node 129 due to zero loss or negligible gradients.\n",
      "Skipping node 1449 due to zero loss or negligible gradients.\n",
      "Skipping node 222 due to zero loss or negligible gradients.\n",
      "Skipping node 1263 due to zero loss or negligible gradients.\n",
      "Skipping node 363 due to zero loss or negligible gradients.\n",
      "Skipping node 1214 due to zero loss or negligible gradients.\n",
      "Skipping node 1161 due to zero loss or negligible gradients.\n",
      "Skipping node 1409 due to zero loss or negligible gradients.\n",
      "Skipping node 531 due to zero loss or negligible gradients.\n",
      "Skipping node 579 due to zero loss or negligible gradients.\n",
      "Skipping node 1460 due to zero loss or negligible gradients.\n",
      "Skipping node 1272 due to zero loss or negligible gradients.\n",
      "Skipping node 1320 due to zero loss or negligible gradients.\n",
      "Skipping node 291 due to zero loss or negligible gradients.\n",
      "Skipping node 981 due to zero loss or negligible gradients.\n",
      "Skipping node 419 due to zero loss or negligible gradients.\n",
      "Skipping node 331 due to zero loss or negligible gradients.\n",
      "Skipping node 1174 due to zero loss or negligible gradients.\n",
      "Skipping node 1453 due to zero loss or negligible gradients.\n",
      "Number of nodes with non-zero loss and non-zero gradients: 211\n",
      "Contents of max_nodes: []\n",
      "Contents of min_nodes: [(207, 0.0035292469763980594, 0.0002302858338225633, 15.325506), (379, 3.9149694293286643, 0.007614279631525278, 514.1615), (681, 739.944265991744, 0.10729599744081497, 6896.2896), (820, 1373.0481882882304, 0.14400380849838257, 9534.805)]\n",
      "Contents of moyen_nodes: []\n",
      "Contents of nodes_to_attack: [(207, 0.0035292469763980594, 0.0002302858338225633, 15.325506), (379, 3.9149694293286643, 0.007614279631525278, 514.1615), (681, 739.944265991744, 0.10729599744081497, 6896.2896), (820, 1373.0481882882304, 0.14400380849838257, 9534.805)]\n",
      "Nodes chosen for attack:\n",
      "Node: 207, Total Score: 0.0035, Loss: 0.0002, Gradient Sum: 15.3255\n",
      "Node: 379, Total Score: 3.9150, Loss: 0.0076, Gradient Sum: 514.1615\n",
      "Node: 681, Total Score: 739.9443, Loss: 0.1073, Gradient Sum: 6896.2896\n",
      "Node: 820, Total Score: 1373.0482, Loss: 0.1440, Gradient Sum: 9534.8047\n",
      "Nodes chosen for attack: [207, 379, 681, 820]\n",
      "Nodes that are clean: [1395, 35, 754, 1169, 362, 1183, 597, 255, 1036, 1008, 585, 1046, 1293, 146, 1486, 866, 229, 1057, 1403, 85, 1465, 333, 502, 192, 1050, 1026, 664, 1306, 1485, 309, 1452, 140, 365, 263, 1294, 723, 852, 425, 340, 1471, 397, 1458, 536, 1430, 881, 88, 1065, 6, 834, 271, 1451, 1393, 125, 194, 856, 1069, 1342, 351, 798, 1109, 806, 987, 697, 499, 1388, 864, 1152, 1443, 101, 127, 768, 1139, 1162, 118, 901, 67, 528, 1420, 595, 1334, 883, 382, 132, 742, 1412, 473, 1034, 25, 691, 1309, 1016, 747, 1251, 201, 657, 1359, 878, 614, 715, 258, 837, 21, 560, 693, 706, 1381, 1354, 1090, 1454, 1171, 49, 605, 721, 138, 1370, 418, 1353, 358, 763, 38, 746, 1144, 938, 276, 828, 1101, 854, 1150, 1212, 991, 318, 725, 843, 30, 1102, 394, 1167, 673, 133, 385, 613, 343, 275, 1080, 39, 524, 388, 916, 1017, 549, 727, 1157, 998, 1362, 890, 1321, 683, 484, 117, 1385, 274, 1428, 540, 231, 927, 692, 1044, 294, 480, 230, 247, 402, 874, 608, 1151, 1180, 415, 324, 1266, 210, 359, 338, 1047, 407, 581, 1055, 1112, 1287, 1329, 964, 640, 1390, 232, 1029, 1283, 665, 1252, 1200, 409, 1096, 336, 1027, 396, 1327, 233, 1441, 562, 188, 371, 444, 1138, 238, 1043, 537, 783, 995, 1175, 1119, 89, 410, 832, 7, 1466, 573, 967, 439, 969, 83, 863, 451, 1357, 370, 1188, 757, 801, 645, 357, 65, 1429, 924, 1302, 262, 647, 913, 264, 807, 1002, 628, 360, 993, 1204, 1117, 731, 157, 70, 648, 545, 1007, 239, 215, 412, 577, 364, 1129, 1143, 1391, 824, 541, 634, 777, 646, 816, 788, 554, 1437, 1164, 1289, 456, 559, 237, 106, 1389, 566, 955, 377, 776, 60, 170, 696, 876, 1235, 265, 1380, 74, 690, 126, 1386, 865, 216, 373, 944, 93, 95, 675, 574, 445, 14, 1059, 792, 1472, 1023, 266, 1347, 578, 1324, 679, 760, 884, 711, 936, 198, 301, 350, 543, 1108, 202, 53, 861, 79, 1106, 107, 1314, 1424, 252, 45, 1103, 470, 1125, 1216, 1286, 815, 1383, 770, 66, 272, 1397, 1269, 420, 1121, 36, 892, 907, 8, 1281, 196, 59, 896, 77, 284, 1315, 535, 195, 929, 1478, 954, 191, 867, 751, 1480, 1229, 329, 245, 1414, 686, 827, 1105, 781, 442, 465, 149, 703, 139, 466, 740, 199, 297, 1203, 1039, 180, 124, 225, 1113, 977, 1170, 1091, 1063, 555, 1470, 1318, 818, 176, 1273, 638, 649, 618, 156, 738, 173, 956, 1071, 1249, 1078, 1025, 1085, 1333, 260, 1062, 236, 1028, 506, 829, 682, 400, 1176, 810, 1012, 185, 452, 17, 1237, 848, 76, 482, 1095, 800, 463, 1093, 641, 204, 716, 976, 1461, 761, 423, 644, 479, 910, 698, 700, 179, 1349, 1032, 752, 663, 870, 656, 702, 522, 328, 306, 530, 152, 802, 243, 1257, 50, 1013, 253, 486, 940, 1411, 334, 515, 1483, 583, 1278, 1210, 1072, 1352, 962, 780, 717, 672, 171, 1335, 552, 24, 1142, 438, 755, 889, 143, 111, 1426, 941, 203, 572, 582, 1250, 875, 1344, 491, 1195, 1413, 983, 689, 1134, 489, 753, 208, 1110, 159, 669, 1168, 1331, 278, 462, 1199, 937, 897, 627, 1431, 1457, 850, 453, 1297, 226, 1037, 1098, 1064, 1218, 1319, 1351, 408, 1323, 1086, 688, 299, 426, 102, 758, 630, 227, 116, 561, 282, 1473, 1075, 1130, 766, 478, 1099, 175, 900, 467, 1003, 1018, 858, 488, 403, 389, 684, 654, 435, 765, 154, 52, 283, 268, 632, 569, 772, 23, 1446, 769, 1056, 404, 241, 136, 19, 853, 304, 308, 1243, 1439, 346, 320, 959, 398, 1192, 1394, 421, 1392, 259, 602, 734, 43, 951, 1285, 510, 366, 131, 174, 911, 1066, 323, 84, 1442, 869, 267, 235, 525, 1182, 1479, 1340, 1022, 250, 13, 517, 624, 982, 181, 1290, 844, 934, 113, 1332, 1448, 899, 148, 520, 668, 1425, 128, 1233, 300, 1079, 830, 1384, 1247, 1042, 791, 217, 34, 1360, 305, 1033, 1131, 1440, 178, 42, 513, 1153, 699, 619, 895, 1435, 1375, 464, 558, 163, 1338, 739, 1417, 1173, 912, 387, 41, 1468, 28, 643, 1444, 833, 1419, 599, 705, 381, 611, 187, 1280, 1477, 1404, 112, 814, 1253, 256, 942, 1221, 335, 1040, 1215, 965, 762, 526, 183, 269, 115, 1346, 1469, 1228, 1225, 1133, 920, 429, 588, 1361, 620, 877, 68, 958, 786, 823, 1166, 601, 1367, 1179, 922, 1270, 931, 1295, 1467, 1004, 1481, 119, 342, 11, 548, 1197, 1242, 223, 206, 391, 787, 1145, 81, 658, 512, 655, 871, 985, 1400, 748, 347, 978, 293, 1084, 1316, 82, 64, 1058, 542, 177, 553, 197, 908, 448, 455, 401, 72, 341, 1337, 633, 1123, 182, 1254, 687, 290, 1074, 1239, 737, 1366, 1376, 623, 1261, 62, 504, 551, 1308, 1193, 1326, 676, 273, 846, 1288, 211, 621, 244, 794, 1301, 437, 212, 383, 507, 494, 1445, 1292, 1399, 485, 879, 1398, 1219, 145, 169, 380, 950, 427, 1148, 330, 891, 424, 809, 1211, 1049, 719, 808, 728, 925, 1126, 764, 317, 141, 1156, 1011, 368, 567, 1310, 22, 1208, 167, 319, 56, 1116, 1433, 1355, 580, 612, 248, 33, 1264, 1226, 935, 1020, 390, 918, 1097, 821, 1298, 44, 393, 1038, 784, 99, 893, 1487, 1198, 997, 533, 708, 709, 1111, 1410, 1147, 29, 796, 16, 849, 949, 564, 1092, 1416, 1350, 785, 104, 1124, 150, 610, 295, 1312, 472, 1190, 756, 94, 1378, 1209, 120, 1083, 213, 1476, 1434, 1048, 4, 1262, 712, 1186, 367, 1051, 58, 1202, 1087, 87, 395, 31, 930, 974, 1146, 277, 71, 344, 55, 378, 417, 221, 1241, 775, 704, 322, 550, 492, 1009, 121, 292, 129, 996, 695, 475, 1449, 222, 1263, 363, 1214, 1161, 1409, 168, 531, 246, 579, 1460, 1272, 1320, 134, 291, 981, 419, 331, 1174, 1453]\n",
      "Nodes chosen for attack: [207, 379, 681, 820]\n",
      "Nodes that are clean: [1395, 35, 754, 1169, 362, 1183, 597, 255, 1036, 1008, 585, 1046, 1293, 146, 1486, 866, 229, 1057, 1403, 85, 1465, 333, 502, 192, 1050, 1026, 664, 1306, 1485, 309, 1452, 140, 365, 263, 1294, 723, 852, 425, 340, 1471, 397, 1458, 536, 1430, 881, 88, 1065, 6, 834, 271, 1451, 1393, 125, 194, 856, 1069, 1342, 351, 798, 1109, 806, 987, 697, 499, 1388, 864, 1152, 1443, 101, 127, 768, 1139, 1162, 118, 901, 67, 528, 1420, 595, 1334, 883, 382, 132, 742, 1412, 473, 1034, 25, 691, 1309, 1016, 747, 1251, 201, 657, 1359, 878, 614, 715, 258, 837, 21, 560, 693, 706, 1381, 1354, 1090, 1454, 1171, 49, 605, 721, 138, 1370, 418, 1353, 358, 763, 38, 746, 1144, 938, 276, 828, 1101, 854, 1150, 1212, 991, 318, 725, 843, 30, 1102, 394, 1167, 673, 133, 385, 613, 343, 275, 1080, 39, 524, 388, 916, 1017, 549, 727, 1157, 998, 1362, 890, 1321, 683, 484, 117, 1385, 274, 1428, 540, 231, 927, 692, 1044, 294, 480, 230, 247, 402, 874, 608, 1151, 1180, 415, 324, 1266, 210, 359, 338, 1047, 407, 581, 1055, 1112, 1287, 1329, 964, 640, 1390, 232, 1029, 1283, 665, 1252, 1200, 409, 1096, 336, 1027, 396, 1327, 233, 1441, 562, 188, 371, 444, 1138, 238, 1043, 537, 783, 995, 1175, 1119, 89, 410, 832, 7, 1466, 573, 967, 439, 969, 83, 863, 451, 1357, 370, 1188, 757, 801, 645, 357, 65, 1429, 924, 1302, 262, 647, 913, 264, 807, 1002, 628, 360, 993, 1204, 1117, 731, 157, 70, 648, 545, 1007, 239, 215, 412, 577, 364, 1129, 1143, 1391, 824, 541, 634, 777, 646, 816, 788, 554, 1437, 1164, 1289, 456, 559, 237, 106, 1389, 566, 955, 377, 776, 60, 170, 696, 876, 1235, 265, 1380, 74, 690, 126, 1386, 865, 216, 373, 944, 93, 95, 675, 574, 445, 14, 1059, 792, 1472, 1023, 266, 1347, 578, 1324, 679, 760, 884, 711, 936, 198, 301, 350, 543, 1108, 202, 53, 861, 79, 1106, 107, 1314, 1424, 252, 45, 1103, 470, 1125, 1216, 1286, 815, 1383, 770, 66, 272, 1397, 1269, 420, 1121, 36, 892, 907, 8, 1281, 196, 59, 896, 77, 284, 1315, 535, 195, 929, 1478, 954, 191, 867, 751, 1480, 1229, 329, 245, 1414, 686, 827, 1105, 781, 442, 465, 149, 703, 139, 466, 740, 199, 297, 1203, 1039, 180, 124, 225, 1113, 977, 1170, 1091, 1063, 555, 1470, 1318, 818, 176, 1273, 638, 649, 618, 156, 738, 173, 956, 1071, 1249, 1078, 1025, 1085, 1333, 260, 1062, 236, 1028, 506, 829, 682, 400, 1176, 810, 1012, 185, 452, 17, 1237, 848, 76, 482, 1095, 800, 463, 1093, 641, 204, 716, 976, 1461, 761, 423, 644, 479, 910, 698, 700, 179, 1349, 1032, 752, 663, 870, 656, 702, 522, 328, 306, 530, 152, 802, 243, 1257, 50, 1013, 253, 486, 940, 1411, 334, 515, 1483, 583, 1278, 1210, 1072, 1352, 962, 780, 717, 672, 171, 1335, 552, 24, 1142, 438, 755, 889, 143, 111, 1426, 941, 203, 572, 582, 1250, 875, 1344, 491, 1195, 1413, 983, 689, 1134, 489, 753, 208, 1110, 159, 669, 1168, 1331, 278, 462, 1199, 937, 897, 627, 1431, 1457, 850, 453, 1297, 226, 1037, 1098, 1064, 1218, 1319, 1351, 408, 1323, 1086, 688, 299, 426, 102, 758, 630, 227, 116, 561, 282, 1473, 1075, 1130, 766, 478, 1099, 175, 900, 467, 1003, 1018, 858, 488, 403, 389, 684, 654, 435, 765, 154, 52, 283, 268, 632, 569, 772, 23, 1446, 769, 1056, 404, 241, 136, 19, 853, 304, 308, 1243, 1439, 346, 320, 959, 398, 1192, 1394, 421, 1392, 259, 602, 734, 43, 951, 1285, 510, 366, 131, 174, 911, 1066, 323, 84, 1442, 869, 267, 235, 525, 1182, 1479, 1340, 1022, 250, 13, 517, 624, 982, 181, 1290, 844, 934, 113, 1332, 1448, 899, 148, 520, 668, 1425, 128, 1233, 300, 1079, 830, 1384, 1247, 1042, 791, 217, 34, 1360, 305, 1033, 1131, 1440, 178, 42, 513, 1153, 699, 619, 895, 1435, 1375, 464, 558, 163, 1338, 739, 1417, 1173, 912, 387, 41, 1468, 28, 643, 1444, 833, 1419, 599, 705, 381, 611, 187, 1280, 1477, 1404, 112, 814, 1253, 256, 942, 1221, 335, 1040, 1215, 965, 762, 526, 183, 269, 115, 1346, 1469, 1228, 1225, 1133, 920, 429, 588, 1361, 620, 877, 68, 958, 786, 823, 1166, 601, 1367, 1179, 922, 1270, 931, 1295, 1467, 1004, 1481, 119, 342, 11, 548, 1197, 1242, 223, 206, 391, 787, 1145, 81, 658, 512, 655, 871, 985, 1400, 748, 347, 978, 293, 1084, 1316, 82, 64, 1058, 542, 177, 553, 197, 908, 448, 455, 401, 72, 341, 1337, 633, 1123, 182, 1254, 687, 290, 1074, 1239, 737, 1366, 1376, 623, 1261, 62, 504, 551, 1308, 1193, 1326, 676, 273, 846, 1288, 211, 621, 244, 794, 1301, 437, 212, 383, 507, 494, 1445, 1292, 1399, 485, 879, 1398, 1219, 145, 169, 380, 950, 427, 1148, 330, 891, 424, 809, 1211, 1049, 719, 808, 728, 925, 1126, 764, 317, 141, 1156, 1011, 368, 567, 1310, 22, 1208, 167, 319, 56, 1116, 1433, 1355, 580, 612, 248, 33, 1264, 1226, 935, 1020, 390, 918, 1097, 821, 1298, 44, 393, 1038, 784, 99, 893, 1487, 1198, 997, 533, 708, 709, 1111, 1410, 1147, 29, 796, 16, 849, 949, 564, 1092, 1416, 1350, 785, 104, 1124, 150, 610, 295, 1312, 472, 1190, 756, 94, 1378, 1209, 120, 1083, 213, 1476, 1434, 1048, 4, 1262, 712, 1186, 367, 1051, 58, 1202, 1087, 87, 395, 31, 930, 974, 1146, 277, 71, 344, 55, 378, 417, 221, 1241, 775, 704, 322, 550, 492, 1009, 121, 292, 129, 996, 695, 475, 1449, 222, 1263, 363, 1214, 1161, 1409, 168, 531, 246, 579, 1460, 1272, 1320, 134, 291, 981, 419, 331, 1174, 1453]\n",
      "Test set results: loss= 0.3879 accuracy= 0.8274\n",
      "Test set results: loss= 0.7712 accuracy= 0.2500\n",
      "Test set results: loss= 0.3863 accuracy= 0.8298\n",
      "Test accuracy on attack set:  0.25\n",
      "Test accuracy on clean set:  0.8298097251585623\n",
      " number of nodes to attack: 4\n",
      "budget: 4 alpha(gradient's importance): 1.0 beta( commun neighbor's importance): 1.0\n",
      "Target node is: 207 with label: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahsa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\sparse\\_index.py:102: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added edge: (207, 854)\n",
      "Gradient: 302.06365966796875\n",
      "Number of common neighbors: 0\n",
      "Score: 302.06365966796875\n",
      "Added edge: (207, 1100)\n",
      "Gradient: 152.23431396484375\n",
      "Number of common neighbors: 1\n",
      "Score: 153.23431396484375\n",
      "Added edge: (207, 999)\n",
      "Gradient: 151.7749481201172\n",
      "Number of common neighbors: 1\n",
      "Score: 152.7749481201172\n",
      "Added edge: (207, 1152)\n",
      "Gradient: 150.7581787109375\n",
      "Number of common neighbors: 1\n",
      "Score: 151.7581787109375\n",
      "Target node is: 379 with label: 0\n",
      "Added edge: (379, 854)\n",
      "Gradient: 110.44276428222656\n",
      "Number of common neighbors: 0\n",
      "Score: 110.44276428222656\n",
      "Added edge: (379, 1100)\n",
      "Gradient: 55.66107177734375\n",
      "Number of common neighbors: 1\n",
      "Score: 56.66107177734375\n",
      "Added edge: (379, 999)\n",
      "Gradient: 55.49311065673828\n",
      "Number of common neighbors: 1\n",
      "Score: 56.49311065673828\n",
      "Added edge: (379, 1152)\n",
      "Gradient: 55.1213493347168\n",
      "Number of common neighbors: 1\n",
      "Score: 56.1213493347168\n",
      "Target node is: 681 with label: 0\n",
      "Added edge: (681, 854)\n",
      "Gradient: 36.15928649902344\n",
      "Number of common neighbors: 0\n",
      "Score: 36.15928649902344\n",
      "Added edge: (681, 1100)\n",
      "Gradient: 18.22359848022461\n",
      "Number of common neighbors: 1\n",
      "Score: 19.22359848022461\n",
      "Added edge: (681, 879)\n",
      "Gradient: 17.614458084106445\n",
      "Number of common neighbors: 2\n",
      "Score: 19.614458084106445\n",
      "Added edge: (681, 1152)\n",
      "Gradient: 18.046894073486328\n",
      "Number of common neighbors: 2\n",
      "Score: 20.046894073486328\n",
      "Target node is: 820 with label: 1\n",
      "Added edge: (820, 154)\n",
      "Gradient: 130.8237762451172\n",
      "Number of common neighbors: 0\n",
      "Score: 130.8237762451172\n",
      "Added edge: (820, 54)\n",
      "Gradient: 122.85295867919922\n",
      "Number of common neighbors: 1\n",
      "Score: 123.85295867919922\n",
      "Added edge: (820, 640)\n",
      "Gradient: 114.56117248535156\n",
      "Number of common neighbors: 2\n",
      "Score: 116.56117248535156\n",
      "Added edge: (820, 362)\n",
      "Gradient: 90.7513656616211\n",
      "Number of common neighbors: 3\n",
      "Score: 93.7513656616211\n",
      "Total number of edges added: 16\n",
      "Total number of edges removed: 0\n",
      "Test set results: loss= 1.1915 accuracy= 0.0000\n",
      "Test set results: loss= 0.4028 accuracy= 0.8309\n",
      "Test accuracy on attack set after attack:  0.0\n",
      "Test accuracy on clean set after attack:  0.8308668076109936\n",
      "*************** Crypto'Graph defense ***************\n",
      "Dropping dissimilar edges using metric :  neighbors  on links\n",
      "removed 1648 edges in polblogs 1\n",
      "removed 1664 edges in polblogs 2\n",
      "*** polblogs 1 ***\n",
      "Test set results: loss= 1.1456 accuracy= 0.0000\n",
      "*** polblogs 2 ***\n",
      "Test set results: loss= 0.7512 accuracy= 0.2500\n",
      "*** polblogs 1 ***\n",
      "Test set results: loss= 0.4954 accuracy= 0.7378\n",
      "*** polblogs 2 ***\n",
      "Test set results: loss= 0.4671 accuracy= 0.7304\n",
      "Test accuracy on attack set after Crypto'Graph:  (0.0, 0.25)\n",
      "Test accuracy on clean set after Crypto'Graph:  (0.7378435517970402, 0.7304439746300211)\n",
      "Total number of removed edges by CG: 2222\n",
      "Inserted edges removed by CG: [(207, 854), (207, 999), (207, 1152), (379, 854), (379, 999), (379, 1152), (681, 854), (681, 1152)]\n",
      "Number of inserted edges removed by CG: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########  scenario 4: surrogate: adj, ACC1:adj1, attaque:adj1, ACC2: adj1^,   Crypto'Graph: adj1^, adj2  , ACC3: adj1*\n",
    "# mahsa-V7 : improvements on V5.1 : changing to remove edges too. change: calculate_edge_scores_all_actions and perturb_edges_between_targets_all_actions\n",
    "# now it considers just ACC3 for adj1 after CG defense.\n",
    "# initial train on adj1  -DONE BEFORE ATTACK in the main execution\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from deeprobust.graph.defense import GCN\n",
    "from deeprobust.graph.utils import *\n",
    "from deeprobust.graph.data import Dataset\n",
    "from DistributedDefense import TwoPartyCNGCN\n",
    "from experiments import split_dataset\n",
    "from scipy.sparse import csr_matrix\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Train surrogate model\n",
    "def train_surrogate_model(features, adj, labels, idx_train, idx_val):\n",
    "    surrogate_model = GCN(nfeat=features.shape[1], nclass=labels.max().item() + 1, nhid=16, device=device, dropout=0.5)\n",
    "    surrogate_model = surrogate_model.to(device)\n",
    "    surrogate_model.fit(features, adj, labels, idx_train, idx_val=idx_val, patience=30, train_iters=200)\n",
    "    return surrogate_model\n",
    "\n",
    "# Compute gradients of loss w.r.t adjacency matrix\n",
    "def compute_gradients(surrogate_model, features, adj, labels, target_node):\n",
    "    surrogate_model.eval()\n",
    "    adj = torch.FloatTensor(adj.toarray()).to(device)\n",
    "    adj.requires_grad = True\n",
    "    features = torch.FloatTensor(features.toarray()).to(device)\n",
    "    labels = torch.LongTensor(labels).to(device)\n",
    "    output = surrogate_model(features, adj)\n",
    "    loss = F.nll_loss(output[[target_node]], labels[[target_node]])\n",
    "    loss.backward()\n",
    "    gradients = adj.grad.cpu().numpy()\n",
    "    return loss.item(), gradients\n",
    "\n",
    "\n",
    "def calc_nodes_to_attack(surrogate_model, features, adj, labels, idx_test, max_count=0, min_count=0, moyen_count=0, manual_nodes=None):\n",
    "    if manual_nodes is not None and len(manual_nodes) > 0:\n",
    "        print(\"Using manually selected nodes.\")\n",
    "        return [(node, 0, 0, 0) for node in manual_nodes]  # Dummy values for score, loss, gradient sum\n",
    "\n",
    "    non_zero_loss_and_gradients_nodes = []\n",
    "\n",
    "    for target_node in idx_test:\n",
    "        loss, gradients = compute_gradients(surrogate_model, features, adj, labels, target_node)\n",
    "\n",
    "        # Filter based on zero loss or negligible gradient values\n",
    "        gradient_sum = np.sum(np.abs(gradients))\n",
    "        if loss < 1e-5 or gradient_sum < 1e-12:  # Using a small threshold to account for numerical precision issues\n",
    "            print(f\"Skipping node {target_node} due to zero loss or negligible gradients.\")\n",
    "            continue\n",
    "\n",
    "        impact_score = loss * gradient_sum\n",
    "        non_zero_loss_and_gradients_nodes.append((target_node, impact_score, loss, gradient_sum))\n",
    "\n",
    "    print(f\"Number of nodes with non-zero loss and non-zero gradients: {len(non_zero_loss_and_gradients_nodes)}\")\n",
    "\n",
    "    # Sort nodes by impact score\n",
    "    # sorted_nodes = sorted(non_zero_loss_and_gradients_nodes, key=lambda x: x[1])\n",
    "    \n",
    "    sorted_nodes = sorted(non_zero_loss_and_gradients_nodes, key=lambda x: (x[1], x[2]))  # Sort by impact score, then by loss\n",
    "\n",
    "\n",
    "\n",
    "    # Selecting the top nodes with maximum loss and gradient\n",
    "    max_nodes = sorted_nodes[-max_count:] if max_count > 0 else []\n",
    "    # Selecting the bottom nodes with minimum loss and gradient\n",
    "    min_nodes = sorted_nodes[:min_count] if min_count > 0 else []\n",
    "    # Selecting nodes with median (moyen) loss and gradient\n",
    "    median_index = len(sorted_nodes) // 2\n",
    "    moyen_nodes = sorted_nodes[max(0, median_index - moyen_count//2): min(len(sorted_nodes), median_index + moyen_count//2)] if moyen_count > 0 else []\n",
    "\n",
    "    return max_nodes, min_nodes, moyen_nodes\n",
    "\n",
    "\n",
    "# Calculate common neighbors between two nodes\n",
    "def calculate_common_neighbors(adj, node1, node2):\n",
    "    neighbors1 = set(adj[node1].indices)\n",
    "    neighbors2 = set(adj[node2].indices)\n",
    "    common_neighbors = neighbors1 & neighbors2\n",
    "    return len(common_neighbors)\n",
    "\n",
    "\n",
    "### change to consider all actions (add/remove) for each target node\n",
    "def calculate_edge_scores_all_actions(adj, gradients, target_node, labels, alpha, beta):\n",
    "    scores = []\n",
    "    for i in range(adj.shape[0]):\n",
    "        if i == target_node:\n",
    "            continue\n",
    "        common_neighbors = calculate_common_neighbors(adj, target_node, i)\n",
    "        if adj[target_node, i] == 0:\n",
    "            # Edge does not exist, consider adding it\n",
    "            delta_loss = gradients[target_node, i]\n",
    "            score = delta_loss * alpha + common_neighbors * beta\n",
    "            action = 'add'\n",
    "        elif adj[target_node, i] == 1:\n",
    "            # Edge exists, consider removing it\n",
    "            delta_loss = -gradients[target_node, i]\n",
    "            score = delta_loss * alpha + common_neighbors * beta\n",
    "            action = 'remove'\n",
    "        else:\n",
    "            continue\n",
    "        scores.append((target_node, i, action, score, delta_loss, common_neighbors))\n",
    "    return scores\n",
    "\n",
    "def perturb_edges_between_targets(adj, surrogate_model, features, labels, idx_test_attack, budget, alpha, beta):\n",
    "    attacked_adj = adj.copy()\n",
    "    added_edges = []\n",
    "    removed_edges = []\n",
    "    edge_added_count = 0\n",
    "    edge_removed_count = 0\n",
    "\n",
    "    for target_node in idx_test_attack:\n",
    "        print(f\"Target node is: {target_node} with label: {labels[target_node]}\")\n",
    "        loss, gradients = compute_gradients(surrogate_model, features, attacked_adj, labels, target_node)\n",
    "        if not np.any(gradients):\n",
    "            continue\n",
    "        for _ in range(budget):\n",
    "            scores = calculate_edge_scores_all_actions(attacked_adj, gradients, target_node, labels, alpha, beta)\n",
    "            if not scores:\n",
    "                break\n",
    "            # Select the edge and action with the highest score\n",
    "            best_edge_info = max(scores, key=lambda x: x[3])  # x[3] is the score\n",
    "            edge = (best_edge_info[0], best_edge_info[1])\n",
    "            action = best_edge_info[2]\n",
    "            score = best_edge_info[3]\n",
    "            delta_loss = best_edge_info[4]\n",
    "            common_neighbors = best_edge_info[5]\n",
    "\n",
    "            if action == 'add':\n",
    "                # Perform add\n",
    "                attacked_adj[edge[0], edge[1]] = 1\n",
    "                attacked_adj[edge[1], edge[0]] = 1\n",
    "                added_edges.append(edge)\n",
    "                edge_added_count += 1\n",
    "                print(f\"Added edge: {edge}\")\n",
    "                print(f\"Gradient: {gradients[edge[0], edge[1]]}\")\n",
    "                print(f\"Number of common neighbors: {common_neighbors}\")\n",
    "                print(f\"Score: {score}\")\n",
    "            elif action == 'remove':\n",
    "                # Perform remove\n",
    "                attacked_adj[edge[0], edge[1]] = 0\n",
    "                attacked_adj[edge[1], edge[0]] = 0\n",
    "                removed_edges.append(edge)\n",
    "                edge_removed_count += 1\n",
    "                print(f\"Removed edge: {edge}\")\n",
    "                print(f\"Gradient: {gradients[edge[0], edge[1]]}\")\n",
    "                print(f\"Number of common neighbors: {common_neighbors}\")\n",
    "                print(f\"Score: {score}\")\n",
    "\n",
    "    print(f\"Total number of edges added: {edge_added_count}\") \n",
    "    print(f\"Total number of edges removed: {edge_removed_count}\") \n",
    "    return attacked_adj, added_edges, removed_edges\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main execution\n",
    "################################ Data loading #######################################\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load data\n",
    "dataset = \"polblogs\"\n",
    "data = Dataset(root='.', name=dataset, setting='gcn', seed=15)\n",
    "seed = 42\n",
    " \n",
    "set_seeds(seed)\n",
    "adj, features, labels = data.adj, data.features, data.labels\n",
    "idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
    "\n",
    "proportion_of_common_links = 0.5\n",
    "adj1, adj2 = split_dataset(adj, proportion_of_common_links) \n",
    "\n",
    "############  Train surrogate model and determine nodes to attack\n",
    "surrogate_model = train_surrogate_model(features, adj, labels, idx_train, idx_val)\n",
    "\n",
    "# k = 10 # budget in the meaning of Number of nodes to attack\n",
    "# nodes_to_attack = calc_nodes_to_attack(surrogate_model, features, adj, labels, idx_test)\n",
    "max_count = 0    # Number of nodes with maximum loss and gradients\n",
    "min_count = 4    # Number of nodes with minimum loss and gradients\n",
    "moyen_count = 0  # Number of nodes with median (moyen) loss and gradients\n",
    "manual_nodes = [275, 1130, 716, 1182, 213, 121]  # List of nodes to attack manually\n",
    "max_nodes, min_nodes, moyen_nodes = calc_nodes_to_attack(surrogate_model, features, adj, labels, idx_test, max_count, min_count, moyen_count)\n",
    "# max_nodes, min_nodes, moyen_nodes = calc_nodes_to_attack(surrogate_model, features, adj, labels, idx_test, manual_nodes=manual_nodes)\n",
    "\n",
    "print(\"Contents of max_nodes:\", max_nodes)\n",
    "print(\"Contents of min_nodes:\", min_nodes)\n",
    "print(\"Contents of moyen_nodes:\", moyen_nodes)\n",
    "\n",
    "# Combine into nodes_to_attack\n",
    "nodes_to_attack = max_nodes + min_nodes + moyen_nodes\n",
    "print(\"Contents of nodes_to_attack:\", nodes_to_attack)\n",
    "\n",
    "# Print out node information\n",
    "print(\"Nodes chosen for attack:\")\n",
    "for node_info in nodes_to_attack:\n",
    "    if isinstance(node_info, tuple):\n",
    "        node, score, loss, gradient_sum = node_info\n",
    "        print(f\"Node: {node}, Total Score: {score:.4f}, Loss: {loss:.4f}, Gradient Sum: {gradient_sum:.4f}\")\n",
    "    else:\n",
    "        print(f\"Unexpected format: {node_info} (type: {type(node_info)})\")\n",
    "\n",
    "\n",
    "# Extract only the node IDs from nodes_to_attack\n",
    "nodes_to_attack_ids = [node_info[0] for node_info in nodes_to_attack if isinstance(node_info, tuple)]\n",
    "idx_test_attack = nodes_to_attack_ids\n",
    "# Identify clean nodes\n",
    "idx_test_clean = [node for node in idx_test if node not in nodes_to_attack_ids]\n",
    "\n",
    "print(\"Nodes chosen for attack:\", nodes_to_attack_ids)\n",
    "print(\"Nodes that are clean:\", idx_test_clean)\n",
    "\n",
    "print(\"Nodes chosen for attack:\", idx_test_attack)\n",
    "print(\"Nodes that are clean:\", idx_test_clean)\n",
    "\n",
    "\n",
    "########### Train GCN model initially for evaluation before attack\n",
    "model = GCN(nfeat=features.shape[1], nclass=labels.max().item()+1,\n",
    "            nhid=16, device=device, dropout=0.5)\n",
    "model = model.to(device)\n",
    "model.fit(features, adj1, labels, idx_train, idx_val=idx_val, patience=30, train_iters=200)\n",
    "model.eval()\n",
    "output = model.test(idx_test)\n",
    "\n",
    "accuracy_test_attack_1 = model.test(idx_test_attack) \n",
    "accuracy_test_clean_1 = model.test(idx_test_clean)\n",
    "print(\"Test accuracy on attack set: \", accuracy_test_attack_1)\n",
    "print(\"Test accuracy on clean set: \", accuracy_test_clean_1)\n",
    "\n",
    "############# perform attack Perturb edges\n",
    "budget = 4 # Number of edges to add or remove for each target node\n",
    "alpha = 1.0\n",
    "beta = 1.0\n",
    "print(f\" number of nodes to attack: {len(idx_test_attack)}\")\n",
    "print(f\"budget: {budget}\", f\"alpha(gradient's importance): {alpha}\", F\"beta( commun neighbor's importance): {beta}\")\n",
    "\n",
    "attacked_adj_1, added_edges, removed_edges = perturb_edges_between_targets(adj1, surrogate_model, features, labels, idx_test_attack, budget, alpha, beta)\n",
    "\n",
    "# Model evaluation after attack\n",
    "model.fit(features, attacked_adj_1, labels, idx_train, idx_val=idx_val, patience=30, train_iters=200)\n",
    "model.eval()\n",
    "\n",
    "accuracy_test_attack_2 = model.test(idx_test_attack)\n",
    "accuracy_test_clean_2 = model.test(idx_test_clean)\n",
    "print(\"Test accuracy on attack set after attack: \", accuracy_test_attack_2)\n",
    "print(\"Test accuracy on clean set after attack: \", accuracy_test_clean_2)\n",
    "\n",
    "##################   Crypto'Graph defense\n",
    "print(\"*************** Crypto'Graph defense ***************\")\n",
    "\n",
    "threshold = 2\n",
    "metric = \"neighbors\"\n",
    "object = \"links\"\n",
    "\n",
    "model = TwoPartyCNGCN(dataset=dataset, nfeat=features.shape[1], nhid=16, nclass=labels.max().item() + 1, device=device)\n",
    "defense_duration, defense_duration, training_duration1, training_duration2, CG_defended_adj1, CG_defended_adj2 = model.fit(\n",
    "        attacked_adj_1.copy(), adj2.copy(), features, features, labels, idx_train, threshold, metric=metric, object=object,\n",
    "        train_iters=200, initialize=True, verbose=False, idx_val=idx_val)\n",
    "model.eval()\n",
    "\n",
    "accuracy_test_attack_3 = model.test(idx_test_attack)\n",
    "accuracy_test_clean_3 = model.test(idx_test_clean)\n",
    "print(\"Test accuracy on attack set after Crypto'Graph: \", accuracy_test_attack_3)\n",
    "print(\"Test accuracy on clean set after Crypto'Graph: \", accuracy_test_clean_3)\n",
    "\n",
    "# Use only the accuracy for the first part of the graph (adj1)\n",
    "accuracy_test_attack_3_adj1 = accuracy_test_attack_3[0]  # First part for attack set\n",
    "accuracy_test_clean_3_adj1 = accuracy_test_clean_3[0]  # First part for clean set\n",
    "\n",
    "\n",
    "# Function to find removed edges\n",
    "# Function to find removed edges  - should compare adj after attack and cg \n",
    "def find_removed_edges(attacked_adj_1, adj2, defended_adj1, defended_adj2):\n",
    "    removed_edges = []\n",
    "    attacked_adj_1 = attacked_adj_1.toarray()\n",
    "    adj2 = adj2.toarray()\n",
    "    defended_adj1 = defended_adj1.toarray()\n",
    "    defended_adj2 = defended_adj2.toarray()\n",
    "    combined_attacked_adj = np.maximum(attacked_adj_1, adj2)\n",
    "    combined_defended_adj = np.maximum(defended_adj1, defended_adj2)\n",
    "    for i in range(combined_attacked_adj.shape[0]):\n",
    "        for j in range(i + 1, combined_attacked_adj.shape[1]):\n",
    "            if combined_attacked_adj[i, j] == 1 and combined_defended_adj[i, j] == 0:\n",
    "                removed_edges.append((i, j))\n",
    "    return removed_edges\n",
    "\n",
    "# Find all removed edges\n",
    "removed_edges = find_removed_edges(attacked_adj_1, adj2, CG_defended_adj1, CG_defended_adj2)\n",
    "print(f\"Total number of removed edges by CG: {len(removed_edges)}\")\n",
    "\n",
    "\n",
    "# Check if any of the inserted edges during the attack were removed by CG\n",
    "removed_inserted_edges = [edge for edge in added_edges if edge in removed_edges]\n",
    "print(f\"Inserted edges removed by CG: {removed_inserted_edges}\")\n",
    "print(f\"Number of inserted edges removed by CG: {len(removed_inserted_edges)}\")\n",
    "\n",
    "################ Save and plot results\n",
    "accuracy_test_attack_1_std = np.std(accuracy_test_attack_1)\n",
    "accuracy_test_clean_1_std = np.std(accuracy_test_clean_1)\n",
    "accuracy_test_attack_2_std = np.std(accuracy_test_attack_2)\n",
    "accuracy_test_clean_2_std = np.std(accuracy_test_clean_2)\n",
    "accuracy_test_attack_3_std = np.std(accuracy_test_attack_3_adj1)\n",
    "accuracy_test_clean_3_std = np.std(accuracy_test_clean_3_adj1)\n",
    "\n",
    "with open('variables_std.pkl', 'wb') as f:\n",
    "    pickle.dump([accuracy_test_attack_1_std, accuracy_test_clean_1_std, \n",
    "                 accuracy_test_attack_2_std, accuracy_test_clean_2_std, \n",
    "                 accuracy_test_attack_3_std, accuracy_test_clean_3_std], f)\n",
    "\n",
    "accuracy_test_attack_1_avg = np.mean(accuracy_test_attack_1)\n",
    "accuracy_test_clean_1_avg = np.mean(accuracy_test_clean_1)\n",
    "accuracy_test_attack_2_avg = np.mean(accuracy_test_attack_2)\n",
    "accuracy_test_clean_2_avg = np.mean(accuracy_test_clean_2)\n",
    "accuracy_test_attack_3_avg = np.mean(accuracy_test_attack_3_adj1)\n",
    "accuracy_test_clean_3_avg = np.mean(accuracy_test_clean_3_adj1)\n",
    "\n",
    "with open('variables.pkl', 'wb') as f:\n",
    "    pickle.dump([accuracy_test_attack_1_avg, accuracy_test_clean_1_avg, \n",
    "                 accuracy_test_attack_2_avg, accuracy_test_clean_2_avg, \n",
    "                 accuracy_test_attack_3_avg, accuracy_test_clean_3_avg], f)\n",
    "\n",
    "labels = ['ACCs before attack', 'ACCs before attack', \n",
    "          'ACCs after attack', 'ACCs after attack', \n",
    "          'ACCs after CryptoGraph', 'ACCs after CryptoGraph']\n",
    "values = [accuracy_test_attack_1_avg, accuracy_test_clean_1_avg, \n",
    "          accuracy_test_attack_2_avg, accuracy_test_clean_2_avg, \n",
    "          accuracy_test_attack_3_avg, accuracy_test_clean_3_avg]\n",
    "std_devs = [accuracy_test_attack_1_std, accuracy_test_clean_1_std, \n",
    "            accuracy_test_attack_2_std, accuracy_test_clean_2_std, \n",
    "            accuracy_test_attack_3_std, accuracy_test_clean_3_std]\n",
    "x = np.arange(len(labels)//2)\n",
    "width = 0.35\n",
    "bars1 = plt.bar(x - width/2, values[::2], width, yerr=std_devs[::2], label='Attack set', color='red', capsize=5)\n",
    "bars2 = plt.bar(x + width/2, values[1::2], width, yerr=std_devs[1::2], label='Clean set', color='blue', capsize=5)\n",
    "plt.title('Average Accuracies')\n",
    "plt.xlabel('Levels of Attack and Defense')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.xticks(x, labels[::2])\n",
    "plt.legend()\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########  scenario 5: surrogate: adj1, ACC1:adj, attaque:adj, ACC2: adj^,   Crypto'Graph: adj^, adj2  , ACC3: adj*\n",
    "# mahsa-V7 : improvements on V5.1 : changing to remove edges too. change: calculate_edge_scores_all_actions and perturb_edges_between_targets_all_actions\n",
    "# now it considers just ACC3 for adj1 after CG defense.\n",
    "# initial train on adj1  -DONE BEFORE ATTACK in the main execution\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from deeprobust.graph.defense import GCN\n",
    "from deeprobust.graph.utils import *\n",
    "from deeprobust.graph.data import Dataset\n",
    "from DistributedDefense import TwoPartyCNGCN\n",
    "from experiments import split_dataset\n",
    "from scipy.sparse import csr_matrix\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Train surrogate model\n",
    "def train_surrogate_model(features, adj, labels, idx_train, idx_val):\n",
    "    surrogate_model = GCN(nfeat=features.shape[1], nclass=labels.max().item() + 1, nhid=16, device=device, dropout=0.5)\n",
    "    surrogate_model = surrogate_model.to(device)\n",
    "    surrogate_model.fit(features, adj, labels, idx_train, idx_val=idx_val, patience=30, train_iters=200)\n",
    "    return surrogate_model\n",
    "\n",
    "# Compute gradients of loss w.r.t adjacency matrix\n",
    "def compute_gradients(surrogate_model, features, adj, labels, target_node):\n",
    "    surrogate_model.eval()\n",
    "    adj = torch.FloatTensor(adj.toarray()).to(device)\n",
    "    adj.requires_grad = True\n",
    "    features = torch.FloatTensor(features.toarray()).to(device)\n",
    "    labels = torch.LongTensor(labels).to(device)\n",
    "    output = surrogate_model(features, adj)\n",
    "    loss = F.nll_loss(output[[target_node]], labels[[target_node]])\n",
    "    loss.backward()\n",
    "    gradients = adj.grad.cpu().numpy()\n",
    "    return loss.item(), gradients\n",
    "\n",
    "\n",
    "def calc_nodes_to_attack(surrogate_model, features, adj, labels, idx_test, max_count=0, min_count=0, moyen_count=0, manual_nodes=None):\n",
    "    if manual_nodes is not None and len(manual_nodes) > 0:\n",
    "        print(\"Using manually selected nodes.\")\n",
    "        return [(node, 0, 0, 0) for node in manual_nodes]  # Dummy values for score, loss, gradient sum\n",
    "\n",
    "    non_zero_loss_and_gradients_nodes = []\n",
    "\n",
    "    for target_node in idx_test:\n",
    "        loss, gradients = compute_gradients(surrogate_model, features, adj, labels, target_node)\n",
    "\n",
    "        # Filter based on zero loss or negligible gradient values\n",
    "        gradient_sum = np.sum(np.abs(gradients))\n",
    "        if loss < 1e-5 or gradient_sum < 1e-12:  # Using a small threshold to account for numerical precision issues\n",
    "            print(f\"Skipping node {target_node} due to zero loss or negligible gradients.\")\n",
    "            continue\n",
    "\n",
    "        impact_score = loss * gradient_sum\n",
    "        non_zero_loss_and_gradients_nodes.append((target_node, impact_score, loss, gradient_sum))\n",
    "\n",
    "    print(f\"Number of nodes with non-zero loss and non-zero gradients: {len(non_zero_loss_and_gradients_nodes)}\")\n",
    "\n",
    "    # Sort nodes by impact score\n",
    "    # sorted_nodes = sorted(non_zero_loss_and_gradients_nodes, key=lambda x: x[1])\n",
    "    \n",
    "    sorted_nodes = sorted(non_zero_loss_and_gradients_nodes, key=lambda x: (x[1], x[2]))  # Sort by impact score, then by loss\n",
    "\n",
    "\n",
    "\n",
    "    # Selecting the top nodes with maximum loss and gradient\n",
    "    max_nodes = sorted_nodes[-max_count:] if max_count > 0 else []\n",
    "    # Selecting the bottom nodes with minimum loss and gradient\n",
    "    min_nodes = sorted_nodes[:min_count] if min_count > 0 else []\n",
    "    # Selecting nodes with median (moyen) loss and gradient\n",
    "    median_index = len(sorted_nodes) // 2\n",
    "    moyen_nodes = sorted_nodes[max(0, median_index - moyen_count//2): min(len(sorted_nodes), median_index + moyen_count//2)] if moyen_count > 0 else []\n",
    "\n",
    "    return max_nodes, min_nodes, moyen_nodes\n",
    "\n",
    "\n",
    "# Calculate common neighbors between two nodes\n",
    "def calculate_common_neighbors(adj, node1, node2):\n",
    "    neighbors1 = set(adj[node1].indices)\n",
    "    neighbors2 = set(adj[node2].indices)\n",
    "    common_neighbors = neighbors1 & neighbors2\n",
    "    return len(common_neighbors)\n",
    "\n",
    "\n",
    "### change to consider all actions (add/remove) for each target node\n",
    "def calculate_edge_scores_all_actions(adj, gradients, target_node, labels, alpha, beta):\n",
    "    scores = []\n",
    "    for i in range(adj.shape[0]):\n",
    "        if i == target_node:\n",
    "            continue\n",
    "        common_neighbors = calculate_common_neighbors(adj, target_node, i)\n",
    "        if adj[target_node, i] == 0:\n",
    "            # Edge does not exist, consider adding it\n",
    "            delta_loss = gradients[target_node, i]\n",
    "            score = delta_loss * alpha + common_neighbors * beta\n",
    "            action = 'add'\n",
    "        elif adj[target_node, i] == 1:\n",
    "            # Edge exists, consider removing it\n",
    "            delta_loss = -gradients[target_node, i]\n",
    "            score = delta_loss * alpha + common_neighbors * beta\n",
    "            action = 'remove'\n",
    "        else:\n",
    "            continue\n",
    "        scores.append((target_node, i, action, score, delta_loss, common_neighbors))\n",
    "    return scores\n",
    "\n",
    "def perturb_edges_between_targets(adj, surrogate_model, features, labels, idx_test_attack, budget, alpha, beta):\n",
    "    attacked_adj = adj.copy()\n",
    "    added_edges = []\n",
    "    removed_edges = []\n",
    "    edge_added_count = 0\n",
    "    edge_removed_count = 0\n",
    "\n",
    "    for target_node in idx_test_attack:\n",
    "        print(f\"Target node is: {target_node} with label: {labels[target_node]}\")\n",
    "        loss, gradients = compute_gradients(surrogate_model, features, attacked_adj, labels, target_node)\n",
    "        if not np.any(gradients):\n",
    "            continue\n",
    "        for _ in range(budget):\n",
    "            scores = calculate_edge_scores_all_actions(attacked_adj, gradients, target_node, labels, alpha, beta)\n",
    "            if not scores:\n",
    "                break\n",
    "            # Select the edge and action with the highest score\n",
    "            best_edge_info = max(scores, key=lambda x: x[3])  # x[3] is the score\n",
    "            edge = (best_edge_info[0], best_edge_info[1])\n",
    "            action = best_edge_info[2]\n",
    "            score = best_edge_info[3]\n",
    "            delta_loss = best_edge_info[4]\n",
    "            common_neighbors = best_edge_info[5]\n",
    "\n",
    "            if action == 'add':\n",
    "                # Perform add\n",
    "                attacked_adj[edge[0], edge[1]] = 1\n",
    "                attacked_adj[edge[1], edge[0]] = 1\n",
    "                added_edges.append(edge)\n",
    "                edge_added_count += 1\n",
    "                print(f\"Added edge: {edge}\")\n",
    "                print(f\"Gradient: {gradients[edge[0], edge[1]]}\")\n",
    "                print(f\"Number of common neighbors: {common_neighbors}\")\n",
    "                print(f\"Score: {score}\")\n",
    "            elif action == 'remove':\n",
    "                # Perform remove\n",
    "                attacked_adj[edge[0], edge[1]] = 0\n",
    "                attacked_adj[edge[1], edge[0]] = 0\n",
    "                removed_edges.append(edge)\n",
    "                edge_removed_count += 1\n",
    "                print(f\"Removed edge: {edge}\")\n",
    "                print(f\"Gradient: {gradients[edge[0], edge[1]]}\")\n",
    "                print(f\"Number of common neighbors: {common_neighbors}\")\n",
    "                print(f\"Score: {score}\")\n",
    "\n",
    "    print(f\"Total number of edges added: {edge_added_count}\") \n",
    "    print(f\"Total number of edges removed: {edge_removed_count}\") \n",
    "    return attacked_adj, added_edges, removed_edges\n",
    "\n",
    "\n",
    "\n",
    "# Main execution\n",
    "################################ Data loading #######################################\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load data\n",
    "dataset = \"polblogs\"\n",
    "data = Dataset(root='.', name=dataset, setting='gcn', seed=15)\n",
    "seed = 42\n",
    " \n",
    "set_seeds(seed)\n",
    "adj, features, labels = data.adj, data.features, data.labels\n",
    "idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
    "\n",
    "proportion_of_common_links = 0.5\n",
    "adj1, adj2 = split_dataset(adj, proportion_of_common_links) \n",
    "\n",
    "############  Train surrogate model and determine nodes to attack\n",
    "surrogate_model = train_surrogate_model(features, adj1, labels, idx_train, idx_val)\n",
    "\n",
    "# k = 10 # budget in the meaning of Number of nodes to attack\n",
    "# nodes_to_attack = calc_nodes_to_attack(surrogate_model, features, adj, labels, idx_test)\n",
    "max_count = 0    # Number of nodes with maximum loss and gradients\n",
    "min_count = 1    # Number of nodes with minimum loss and gradients\n",
    "moyen_count = 0  # Number of nodes with median (moyen) loss and gradients\n",
    "manual_nodes = [275, 1130, 716, 1182, 213, 121]  # List of nodes to attack manually\n",
    "max_nodes, min_nodes, moyen_nodes = calc_nodes_to_attack(surrogate_model, features, adj, labels, idx_test, max_count, min_count, moyen_count)\n",
    "# max_nodes, min_nodes, moyen_nodes = calc_nodes_to_attack(surrogate_model, features, adj, labels, idx_test, manual_nodes=manual_nodes)\n",
    "\n",
    "print(\"Contents of max_nodes:\", max_nodes)\n",
    "print(\"Contents of min_nodes:\", min_nodes)\n",
    "print(\"Contents of moyen_nodes:\", moyen_nodes)\n",
    "\n",
    "# Combine into nodes_to_attack\n",
    "nodes_to_attack = max_nodes + min_nodes + moyen_nodes\n",
    "print(\"Contents of nodes_to_attack:\", nodes_to_attack)\n",
    "\n",
    "# Print out node information\n",
    "print(\"Nodes chosen for attack:\")\n",
    "for node_info in nodes_to_attack:\n",
    "    if isinstance(node_info, tuple):\n",
    "        node, score, loss, gradient_sum = node_info\n",
    "        print(f\"Node: {node}, Total Score: {score:.4f}, Loss: {loss:.4f}, Gradient Sum: {gradient_sum:.4f}\")\n",
    "    else:\n",
    "        print(f\"Unexpected format: {node_info} (type: {type(node_info)})\")\n",
    "\n",
    "\n",
    "# Extract only the node IDs from nodes_to_attack\n",
    "nodes_to_attack_ids = [node_info[0] for node_info in nodes_to_attack if isinstance(node_info, tuple)]\n",
    "idx_test_attack = nodes_to_attack_ids\n",
    "# Identify clean nodes\n",
    "idx_test_clean = [node for node in idx_test if node not in nodes_to_attack_ids]\n",
    "\n",
    "print(\"Nodes chosen for attack:\", nodes_to_attack_ids)\n",
    "print(\"Nodes that are clean:\", idx_test_clean)\n",
    "\n",
    "print(\"Nodes chosen for attack:\", idx_test_attack)\n",
    "print(\"Nodes that are clean:\", idx_test_clean)\n",
    "\n",
    "\n",
    "########### Train GCN model initially for evaluation before attack\n",
    "model = GCN(nfeat=features.shape[1], nclass=labels.max().item()+1,\n",
    "            nhid=16, device=device, dropout=0.5)\n",
    "model = model.to(device)\n",
    "model.fit(features, adj, labels, idx_train, idx_val=idx_val, patience=30, train_iters=200)\n",
    "model.eval()\n",
    "output = model.test(idx_test)\n",
    "\n",
    "accuracy_test_attack_1 = model.test(idx_test_attack) \n",
    "accuracy_test_clean_1 = model.test(idx_test_clean)\n",
    "print(\"Test accuracy on attack set: \", accuracy_test_attack_1)\n",
    "print(\"Test accuracy on clean set: \", accuracy_test_clean_1)\n",
    "\n",
    "############# perform attack Perturb edges\n",
    "budget = 2 # Number of edges to add or remove for each target node\n",
    "alpha = 1.0\n",
    "beta = 1.0\n",
    "print(f\" number of nodes to attack: {len(idx_test_attack)}\")\n",
    "print(f\"budget: {budget}\", f\"alpha(gradient's importance): {alpha}\", F\"beta( commun neighbor's importance): {beta}\")\n",
    "\n",
    "attacked_adj_1, added_edges, removed_edges = perturb_edges_between_targets(adj, surrogate_model, features, labels, idx_test_attack, budget, alpha, beta)\n",
    "\n",
    "# Model evaluation after attack\n",
    "model.fit(features, attacked_adj_1, labels, idx_train, idx_val=idx_val, patience=30, train_iters=200)\n",
    "model.eval()\n",
    "\n",
    "accuracy_test_attack_2 = model.test(idx_test_attack)\n",
    "accuracy_test_clean_2 = model.test(idx_test_clean)\n",
    "print(\"Test accuracy on attack set after attack: \", accuracy_test_attack_2)\n",
    "print(\"Test accuracy on clean set after attack: \", accuracy_test_clean_2)\n",
    "\n",
    "##################   Crypto'Graph defense\n",
    "print(\"*************** Crypto'Graph defense ***************\")\n",
    "\n",
    "threshold = 2\n",
    "metric = \"neighbors\"\n",
    "object = \"links\"\n",
    "\n",
    "model = TwoPartyCNGCN(dataset=dataset, nfeat=features.shape[1], nhid=16, nclass=labels.max().item() + 1, device=device)\n",
    "defense_duration, defense_duration, training_duration1, training_duration2, CG_defended_adj1, CG_defended_adj2 = model.fit(\n",
    "        attacked_adj_1.copy(), adj2.copy(), features, features, labels, idx_train, threshold, metric=metric, object=object,\n",
    "        train_iters=200, initialize=True, verbose=False, idx_val=idx_val)\n",
    "model.eval()\n",
    "\n",
    "accuracy_test_attack_3 = model.test(idx_test_attack)\n",
    "accuracy_test_clean_3 = model.test(idx_test_clean)\n",
    "print(\"Test accuracy on attack set after Crypto'Graph: \", accuracy_test_attack_3)\n",
    "print(\"Test accuracy on clean set after Crypto'Graph: \", accuracy_test_clean_3)\n",
    "\n",
    "# Use only the accuracy for the first part of the graph (adj1)\n",
    "accuracy_test_attack_3_adj1 = accuracy_test_attack_3[0]  # First part for attack set\n",
    "accuracy_test_clean_3_adj1 = accuracy_test_clean_3[0]  # First part for clean set\n",
    "\n",
    "\n",
    "# Function to find removed edges\n",
    "# Function to find removed edges  - should compare adj after attack and cg \n",
    "def find_removed_edges(attacked_adj_1, adj2, defended_adj1, defended_adj2):\n",
    "    removed_edges = []\n",
    "    attacked_adj_1 = attacked_adj_1.toarray()\n",
    "    adj2 = adj2.toarray()\n",
    "    defended_adj1 = defended_adj1.toarray()\n",
    "    defended_adj2 = defended_adj2.toarray()\n",
    "    combined_attacked_adj = np.maximum(attacked_adj_1, adj2)\n",
    "    combined_defended_adj = np.maximum(defended_adj1, defended_adj2)\n",
    "    for i in range(combined_attacked_adj.shape[0]):\n",
    "        for j in range(i + 1, combined_attacked_adj.shape[1]):\n",
    "            if combined_attacked_adj[i, j] == 1 and combined_defended_adj[i, j] == 0:\n",
    "                removed_edges.append((i, j))\n",
    "    return removed_edges\n",
    "\n",
    "# Find all removed edges\n",
    "removed_edges = find_removed_edges(attacked_adj_1, adj2, CG_defended_adj1, CG_defended_adj2)\n",
    "print(f\"Total number of removed edges by CG: {len(removed_edges)}\")\n",
    "\n",
    "\n",
    "# Check if any of the inserted edges during the attack were removed by CG\n",
    "removed_inserted_edges = [edge for edge in added_edges if edge in removed_edges]\n",
    "print(f\"Inserted edges removed by CG: {removed_inserted_edges}\")\n",
    "print(f\"Number of inserted edges removed by CG: {len(removed_inserted_edges)}\")\n",
    "\n",
    "################ Save and plot results\n",
    "accuracy_test_attack_1_std = np.std(accuracy_test_attack_1)\n",
    "accuracy_test_clean_1_std = np.std(accuracy_test_clean_1)\n",
    "accuracy_test_attack_2_std = np.std(accuracy_test_attack_2)\n",
    "accuracy_test_clean_2_std = np.std(accuracy_test_clean_2)\n",
    "accuracy_test_attack_3_std = np.std(accuracy_test_attack_3_adj1)\n",
    "accuracy_test_clean_3_std = np.std(accuracy_test_clean_3_adj1)\n",
    "\n",
    "with open('variables_std.pkl', 'wb') as f:\n",
    "    pickle.dump([accuracy_test_attack_1_std, accuracy_test_clean_1_std, \n",
    "                 accuracy_test_attack_2_std, accuracy_test_clean_2_std, \n",
    "                 accuracy_test_attack_3_std, accuracy_test_clean_3_std], f)\n",
    "\n",
    "accuracy_test_attack_1_avg = np.mean(accuracy_test_attack_1)\n",
    "accuracy_test_clean_1_avg = np.mean(accuracy_test_clean_1)\n",
    "accuracy_test_attack_2_avg = np.mean(accuracy_test_attack_2)\n",
    "accuracy_test_clean_2_avg = np.mean(accuracy_test_clean_2)\n",
    "accuracy_test_attack_3_avg = np.mean(accuracy_test_attack_3_adj1)\n",
    "accuracy_test_clean_3_avg = np.mean(accuracy_test_clean_3_adj1)\n",
    "\n",
    "with open('variables.pkl', 'wb') as f:\n",
    "    pickle.dump([accuracy_test_attack_1_avg, accuracy_test_clean_1_avg, \n",
    "                 accuracy_test_attack_2_avg, accuracy_test_clean_2_avg, \n",
    "                 accuracy_test_attack_3_avg, accuracy_test_clean_3_avg], f)\n",
    "\n",
    "labels = ['ACCs before attack', 'ACCs before attack', \n",
    "          'ACCs after attack', 'ACCs after attack', \n",
    "          'ACCs after CryptoGraph', 'ACCs after CryptoGraph']\n",
    "values = [accuracy_test_attack_1_avg, accuracy_test_clean_1_avg, \n",
    "          accuracy_test_attack_2_avg, accuracy_test_clean_2_avg, \n",
    "          accuracy_test_attack_3_avg, accuracy_test_clean_3_avg]\n",
    "std_devs = [accuracy_test_attack_1_std, accuracy_test_clean_1_std, \n",
    "            accuracy_test_attack_2_std, accuracy_test_clean_2_std, \n",
    "            accuracy_test_attack_3_std, accuracy_test_clean_3_std]\n",
    "x = np.arange(len(labels)//2)\n",
    "width = 0.35\n",
    "bars1 = plt.bar(x - width/2, values[::2], width, yerr=std_devs[::2], label='Attack set', color='red', capsize=5)\n",
    "bars2 = plt.bar(x + width/2, values[1::2], width, yerr=std_devs[1::2], label='Clean set', color='blue', capsize=5)\n",
    "plt.title('Average Accuracies')\n",
    "plt.xlabel('Levels of Attack and Defense')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.xticks(x, labels[::2])\n",
    "plt.legend()\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########  scenario 6: surrogate: adj1, ACC1:adj1, attaque:adj1, ACC2: adj1^,   Crypto'Graph: adj1^, adj2  , ACC3: adj1*\n",
    "# mahsa-V7 : improvements on V5.1 : changing to remove edges too. change: calculate_edge_scores_all_actions and perturb_edges_between_targets_all_actions\n",
    "# now it considers just ACC3 for adj1 after CG defense.\n",
    "# initial train on adj1  -DONE BEFORE ATTACK in the main execution\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from deeprobust.graph.defense import GCN\n",
    "from deeprobust.graph.utils import *\n",
    "from deeprobust.graph.data import Dataset\n",
    "from DistributedDefense import TwoPartyCNGCN\n",
    "from experiments import split_dataset\n",
    "from scipy.sparse import csr_matrix\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Train surrogate model\n",
    "def train_surrogate_model(features, adj, labels, idx_train, idx_val):\n",
    "    surrogate_model = GCN(nfeat=features.shape[1], nclass=labels.max().item() + 1, nhid=16, device=device, dropout=0.5)\n",
    "    surrogate_model = surrogate_model.to(device)\n",
    "    surrogate_model.fit(features, adj, labels, idx_train, idx_val=idx_val, patience=30, train_iters=200)\n",
    "    return surrogate_model\n",
    "\n",
    "# Compute gradients of loss w.r.t adjacency matrix\n",
    "def compute_gradients(surrogate_model, features, adj, labels, target_node):\n",
    "    surrogate_model.eval()\n",
    "    adj = torch.FloatTensor(adj.toarray()).to(device)\n",
    "    adj.requires_grad = True\n",
    "    features = torch.FloatTensor(features.toarray()).to(device)\n",
    "    labels = torch.LongTensor(labels).to(device)\n",
    "    output = surrogate_model(features, adj)\n",
    "    loss = F.nll_loss(output[[target_node]], labels[[target_node]])\n",
    "    loss.backward()\n",
    "    gradients = adj.grad.cpu().numpy()\n",
    "    return loss.item(), gradients\n",
    "\n",
    "\n",
    "def calc_nodes_to_attack(surrogate_model, features, adj, labels, idx_test, max_count=0, min_count=0, moyen_count=0, manual_nodes=None):\n",
    "    if manual_nodes is not None and len(manual_nodes) > 0:\n",
    "        print(\"Using manually selected nodes.\")\n",
    "        return [(node, 0, 0, 0) for node in manual_nodes]  # Dummy values for score, loss, gradient sum\n",
    "\n",
    "    non_zero_loss_and_gradients_nodes = []\n",
    "\n",
    "    for target_node in idx_test:\n",
    "        loss, gradients = compute_gradients(surrogate_model, features, adj, labels, target_node)\n",
    "\n",
    "        # Filter based on zero loss or negligible gradient values\n",
    "        gradient_sum = np.sum(np.abs(gradients))\n",
    "        if loss < 1e-5 or gradient_sum < 1e-12:  # Using a small threshold to account for numerical precision issues\n",
    "            print(f\"Skipping node {target_node} due to zero loss or negligible gradients.\")\n",
    "            continue\n",
    "\n",
    "        impact_score = loss * gradient_sum\n",
    "        non_zero_loss_and_gradients_nodes.append((target_node, impact_score, loss, gradient_sum))\n",
    "\n",
    "    print(f\"Number of nodes with non-zero loss and non-zero gradients: {len(non_zero_loss_and_gradients_nodes)}\")\n",
    "\n",
    "    # Sort nodes by impact score\n",
    "    # sorted_nodes = sorted(non_zero_loss_and_gradients_nodes, key=lambda x: x[1])\n",
    "    \n",
    "    sorted_nodes = sorted(non_zero_loss_and_gradients_nodes, key=lambda x: (x[1], x[2]))  # Sort by impact score, then by loss\n",
    "\n",
    "\n",
    "\n",
    "    # Selecting the top nodes with maximum loss and gradient\n",
    "    max_nodes = sorted_nodes[-max_count:] if max_count > 0 else []\n",
    "    # Selecting the bottom nodes with minimum loss and gradient\n",
    "    min_nodes = sorted_nodes[:min_count] if min_count > 0 else []\n",
    "    # Selecting nodes with median (moyen) loss and gradient\n",
    "    median_index = len(sorted_nodes) // 2\n",
    "    moyen_nodes = sorted_nodes[max(0, median_index - moyen_count//2): min(len(sorted_nodes), median_index + moyen_count//2)] if moyen_count > 0 else []\n",
    "\n",
    "    return max_nodes, min_nodes, moyen_nodes\n",
    "\n",
    "\n",
    "# Calculate common neighbors between two nodes\n",
    "def calculate_common_neighbors(adj, node1, node2):\n",
    "    neighbors1 = set(adj[node1].indices)\n",
    "    neighbors2 = set(adj[node2].indices)\n",
    "    common_neighbors = neighbors1 & neighbors2\n",
    "    return len(common_neighbors)\n",
    "\n",
    "\n",
    "### change to consider all actions (add/remove) for each target node\n",
    "def calculate_edge_scores_all_actions(adj, gradients, target_node, labels, alpha, beta):\n",
    "    scores = []\n",
    "    for i in range(adj.shape[0]):\n",
    "        if i == target_node:\n",
    "            continue\n",
    "        common_neighbors = calculate_common_neighbors(adj, target_node, i)\n",
    "        if adj[target_node, i] == 0:\n",
    "            # Edge does not exist, consider adding it\n",
    "            delta_loss = gradients[target_node, i]\n",
    "            score = delta_loss * alpha + common_neighbors * beta\n",
    "            action = 'add'\n",
    "        elif adj[target_node, i] == 1:\n",
    "            # Edge exists, consider removing it\n",
    "            delta_loss = -gradients[target_node, i]\n",
    "            score = delta_loss * alpha + common_neighbors * beta\n",
    "            action = 'remove'\n",
    "        else:\n",
    "            continue\n",
    "        scores.append((target_node, i, action, score, delta_loss, common_neighbors))\n",
    "    return scores\n",
    "\n",
    "def perturb_edges_between_targets(adj, surrogate_model, features, labels, idx_test_attack, budget, alpha, beta):\n",
    "    attacked_adj = adj.copy()\n",
    "    added_edges = []\n",
    "    removed_edges = []\n",
    "    edge_added_count = 0\n",
    "    edge_removed_count = 0\n",
    "\n",
    "    for target_node in idx_test_attack:\n",
    "        print(f\"Target node is: {target_node} with label: {labels[target_node]}\")\n",
    "        loss, gradients = compute_gradients(surrogate_model, features, attacked_adj, labels, target_node)\n",
    "        if not np.any(gradients):\n",
    "            continue\n",
    "        for _ in range(budget):\n",
    "            scores = calculate_edge_scores_all_actions(attacked_adj, gradients, target_node, labels, alpha, beta)\n",
    "            if not scores:\n",
    "                break\n",
    "            # Select the edge and action with the highest score\n",
    "            best_edge_info = max(scores, key=lambda x: x[3])  # x[3] is the score\n",
    "            edge = (best_edge_info[0], best_edge_info[1])\n",
    "            action = best_edge_info[2]\n",
    "            score = best_edge_info[3]\n",
    "            delta_loss = best_edge_info[4]\n",
    "            common_neighbors = best_edge_info[5]\n",
    "\n",
    "            if action == 'add':\n",
    "                # Perform add\n",
    "                attacked_adj[edge[0], edge[1]] = 1\n",
    "                attacked_adj[edge[1], edge[0]] = 1\n",
    "                added_edges.append(edge)\n",
    "                edge_added_count += 1\n",
    "                print(f\"Added edge: {edge}\")\n",
    "                print(f\"Gradient: {gradients[edge[0], edge[1]]}\")\n",
    "                print(f\"Number of common neighbors: {common_neighbors}\")\n",
    "                print(f\"Score: {score}\")\n",
    "            elif action == 'remove':\n",
    "                # Perform remove\n",
    "                attacked_adj[edge[0], edge[1]] = 0\n",
    "                attacked_adj[edge[1], edge[0]] = 0\n",
    "                removed_edges.append(edge)\n",
    "                edge_removed_count += 1\n",
    "                print(f\"Removed edge: {edge}\")\n",
    "                print(f\"Gradient: {gradients[edge[0], edge[1]]}\")\n",
    "                print(f\"Number of common neighbors: {common_neighbors}\")\n",
    "                print(f\"Score: {score}\")\n",
    "\n",
    "    print(f\"Total number of edges added: {edge_added_count}\") \n",
    "    print(f\"Total number of edges removed: {edge_removed_count}\") \n",
    "    return attacked_adj, added_edges, removed_edges\n",
    "\n",
    "\n",
    "\n",
    "# Main execution\n",
    "################################ Data loading #######################################\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load data\n",
    "dataset = \"polblogs\"\n",
    "data = Dataset(root='.', name=dataset, setting='gcn', seed=15)\n",
    "seed = 42\n",
    " \n",
    "set_seeds(seed)\n",
    "adj, features, labels = data.adj, data.features, data.labels\n",
    "idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
    "\n",
    "proportion_of_common_links = 0.5\n",
    "adj1, adj2 = split_dataset(adj, proportion_of_common_links) \n",
    "\n",
    "############  Train surrogate model and determine nodes to attack\n",
    "surrogate_model = train_surrogate_model(features, adj1, labels, idx_train, idx_val)\n",
    "\n",
    "# k = 10 # budget in the meaning of Number of nodes to attack\n",
    "# nodes_to_attack = calc_nodes_to_attack(surrogate_model, features, adj, labels, idx_test)\n",
    "max_count = 10    # Number of nodes with maximum loss and gradients\n",
    "min_count = 0    # Number of nodes with minimum loss and gradients\n",
    "moyen_count = 0  # Number of nodes with median (moyen) loss and gradients\n",
    "manual_nodes = [275, 1130, 716, 1182, 213, 121]  # List of nodes to attack manually\n",
    "max_nodes, min_nodes, moyen_nodes = calc_nodes_to_attack(surrogate_model, features, adj, labels, idx_test, max_count, min_count, moyen_count)\n",
    "# max_nodes, min_nodes, moyen_nodes = calc_nodes_to_attack(surrogate_model, features, adj, labels, idx_test, manual_nodes=manual_nodes)\n",
    "\n",
    "print(\"Contents of max_nodes:\", max_nodes)\n",
    "print(\"Contents of min_nodes:\", min_nodes)\n",
    "print(\"Contents of moyen_nodes:\", moyen_nodes)\n",
    "\n",
    "# Combine into nodes_to_attack\n",
    "nodes_to_attack = max_nodes + min_nodes + moyen_nodes\n",
    "print(\"Contents of nodes_to_attack:\", nodes_to_attack)\n",
    "\n",
    "# Print out node information\n",
    "print(\"Nodes chosen for attack:\")\n",
    "for node_info in nodes_to_attack:\n",
    "    if isinstance(node_info, tuple):\n",
    "        node, score, loss, gradient_sum = node_info\n",
    "        print(f\"Node: {node}, Total Score: {score:.4f}, Loss: {loss:.4f}, Gradient Sum: {gradient_sum:.4f}\")\n",
    "    else:\n",
    "        print(f\"Unexpected format: {node_info} (type: {type(node_info)})\")\n",
    "\n",
    "\n",
    "# Extract only the node IDs from nodes_to_attack\n",
    "nodes_to_attack_ids = [node_info[0] for node_info in nodes_to_attack if isinstance(node_info, tuple)]\n",
    "idx_test_attack = nodes_to_attack_ids\n",
    "# Identify clean nodes\n",
    "idx_test_clean = [node for node in idx_test if node not in nodes_to_attack_ids]\n",
    "\n",
    "print(\"Nodes chosen for attack:\", nodes_to_attack_ids)\n",
    "print(\"Nodes that are clean:\", idx_test_clean)\n",
    "\n",
    "print(\"Nodes chosen for attack:\", idx_test_attack)\n",
    "print(\"Nodes that are clean:\", idx_test_clean)\n",
    "\n",
    "\n",
    "########### Train GCN model initially for evaluation before attack\n",
    "model = GCN(nfeat=features.shape[1], nclass=labels.max().item()+1,\n",
    "            nhid=16, device=device, dropout=0.5)\n",
    "model = model.to(device)\n",
    "model.fit(features, adj1, labels, idx_train, idx_val=idx_val, patience=30, train_iters=200)\n",
    "model.eval()\n",
    "output = model.test(idx_test)\n",
    "\n",
    "accuracy_test_attack_1 = model.test(idx_test_attack) \n",
    "accuracy_test_clean_1 = model.test(idx_test_clean)\n",
    "print(\"Test accuracy on attack set: \", accuracy_test_attack_1)\n",
    "print(\"Test accuracy on clean set: \", accuracy_test_clean_1)\n",
    "\n",
    "############# perform attack Perturb edges\n",
    "budget = 1 # Number of edges to add or remove for each target node\n",
    "alpha = 1.0\n",
    "beta = 1.0\n",
    "print(f\" number of nodes to attack: {len(idx_test_attack)}\")\n",
    "print(f\"budget: {budget}\", f\"alpha(gradient's importance): {alpha}\", F\"beta( commun neighbor's importance): {beta}\")\n",
    "\n",
    "attacked_adj_1, added_edges, removed_edges = perturb_edges_between_targets(adj1, surrogate_model, features, labels, idx_test_attack, budget, alpha, beta)\n",
    "\n",
    "# Model evaluation after attack\n",
    "model.fit(features, attacked_adj_1, labels, idx_train, idx_val=idx_val, patience=30, train_iters=200)\n",
    "model.eval()\n",
    "\n",
    "accuracy_test_attack_2 = model.test(idx_test_attack)\n",
    "accuracy_test_clean_2 = model.test(idx_test_clean)\n",
    "print(\"Test accuracy on attack set after attack: \", accuracy_test_attack_2)\n",
    "print(\"Test accuracy on clean set after attack: \", accuracy_test_clean_2)\n",
    "\n",
    "##################   Crypto'Graph defense\n",
    "print(\"*************** Crypto'Graph defense ***************\")\n",
    "\n",
    "threshold = 2\n",
    "metric = \"neighbors\"\n",
    "object = \"links\"\n",
    "\n",
    "model = TwoPartyCNGCN(dataset=dataset, nfeat=features.shape[1], nhid=16, nclass=labels.max().item() + 1, device=device)\n",
    "defense_duration, defense_duration, training_duration1, training_duration2, CG_defended_adj1, CG_defended_adj2 = model.fit(\n",
    "        attacked_adj_1.copy(), adj2.copy(), features, features, labels, idx_train, threshold, metric=metric, object=object,\n",
    "        train_iters=200, initialize=True, verbose=False, idx_val=idx_val)\n",
    "model.eval()\n",
    "\n",
    "accuracy_test_attack_3 = model.test(idx_test_attack)\n",
    "accuracy_test_clean_3 = model.test(idx_test_clean)\n",
    "print(\"Test accuracy on attack set after Crypto'Graph: \", accuracy_test_attack_3)\n",
    "print(\"Test accuracy on clean set after Crypto'Graph: \", accuracy_test_clean_3)\n",
    "\n",
    "# Use only the accuracy for the first part of the graph (adj1)\n",
    "accuracy_test_attack_3_adj1 = accuracy_test_attack_3[0]  # First part for attack set\n",
    "accuracy_test_clean_3_adj1 = accuracy_test_clean_3[0]  # First part for clean set\n",
    "\n",
    "\n",
    "# Function to find removed edges\n",
    "# Function to find removed edges  - should compare adj after attack and cg \n",
    "def find_removed_edges(attacked_adj_1, adj2, defended_adj1, defended_adj2):\n",
    "    removed_edges = []\n",
    "    attacked_adj_1 = attacked_adj_1.toarray()\n",
    "    adj2 = adj2.toarray()\n",
    "    defended_adj1 = defended_adj1.toarray()\n",
    "    defended_adj2 = defended_adj2.toarray()\n",
    "    combined_attacked_adj = np.maximum(attacked_adj_1, adj2)\n",
    "    combined_defended_adj = np.maximum(defended_adj1, defended_adj2)\n",
    "    for i in range(combined_attacked_adj.shape[0]):\n",
    "        for j in range(i + 1, combined_attacked_adj.shape[1]):\n",
    "            if combined_attacked_adj[i, j] == 1 and combined_defended_adj[i, j] == 0:\n",
    "                removed_edges.append((i, j))\n",
    "    return removed_edges\n",
    "\n",
    "# Find all removed edges\n",
    "removed_edges = find_removed_edges(attacked_adj_1, adj2, CG_defended_adj1, CG_defended_adj2)\n",
    "print(f\"Total number of removed edges by CG: {len(removed_edges)}\")\n",
    "\n",
    "\n",
    "# Check if any of the inserted edges during the attack were removed by CG\n",
    "removed_inserted_edges = [edge for edge in added_edges if edge in removed_edges]\n",
    "print(f\"Inserted edges removed by CG: {removed_inserted_edges}\")\n",
    "print(f\"Number of inserted edges removed by CG: {len(removed_inserted_edges)}\")\n",
    "\n",
    "################ Save and plot results\n",
    "accuracy_test_attack_1_std = np.std(accuracy_test_attack_1)\n",
    "accuracy_test_clean_1_std = np.std(accuracy_test_clean_1)\n",
    "accuracy_test_attack_2_std = np.std(accuracy_test_attack_2)\n",
    "accuracy_test_clean_2_std = np.std(accuracy_test_clean_2)\n",
    "accuracy_test_attack_3_std = np.std(accuracy_test_attack_3_adj1)\n",
    "accuracy_test_clean_3_std = np.std(accuracy_test_clean_3_adj1)\n",
    "\n",
    "with open('variables_std.pkl', 'wb') as f:\n",
    "    pickle.dump([accuracy_test_attack_1_std, accuracy_test_clean_1_std, \n",
    "                 accuracy_test_attack_2_std, accuracy_test_clean_2_std, \n",
    "                 accuracy_test_attack_3_std, accuracy_test_clean_3_std], f)\n",
    "\n",
    "accuracy_test_attack_1_avg = np.mean(accuracy_test_attack_1)\n",
    "accuracy_test_clean_1_avg = np.mean(accuracy_test_clean_1)\n",
    "accuracy_test_attack_2_avg = np.mean(accuracy_test_attack_2)\n",
    "accuracy_test_clean_2_avg = np.mean(accuracy_test_clean_2)\n",
    "accuracy_test_attack_3_avg = np.mean(accuracy_test_attack_3_adj1)\n",
    "accuracy_test_clean_3_avg = np.mean(accuracy_test_clean_3_adj1)\n",
    "\n",
    "with open('variables.pkl', 'wb') as f:\n",
    "    pickle.dump([accuracy_test_attack_1_avg, accuracy_test_clean_1_avg, \n",
    "                 accuracy_test_attack_2_avg, accuracy_test_clean_2_avg, \n",
    "                 accuracy_test_attack_3_avg, accuracy_test_clean_3_avg], f)\n",
    "\n",
    "labels = ['ACCs before attack', 'ACCs before attack', \n",
    "          'ACCs after attack', 'ACCs after attack', \n",
    "          'ACCs after CryptoGraph', 'ACCs after CryptoGraph']\n",
    "values = [accuracy_test_attack_1_avg, accuracy_test_clean_1_avg, \n",
    "          accuracy_test_attack_2_avg, accuracy_test_clean_2_avg, \n",
    "          accuracy_test_attack_3_avg, accuracy_test_clean_3_avg]\n",
    "std_devs = [accuracy_test_attack_1_std, accuracy_test_clean_1_std, \n",
    "            accuracy_test_attack_2_std, accuracy_test_clean_2_std, \n",
    "            accuracy_test_attack_3_std, accuracy_test_clean_3_std]\n",
    "x = np.arange(len(labels)//2)\n",
    "width = 0.35\n",
    "bars1 = plt.bar(x - width/2, values[::2], width, yerr=std_devs[::2], label='Attack set', color='red', capsize=5)\n",
    "bars2 = plt.bar(x + width/2, values[1::2], width, yerr=std_devs[1::2], label='Clean set', color='blue', capsize=5)\n",
    "plt.title('Average Accuracies')\n",
    "plt.xlabel('Levels of Attack and Defense')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.xticks(x, labels[::2])\n",
    "plt.legend()\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
